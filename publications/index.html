<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">  
  <title>Publications | Georgia Tech Visualization Lab</title>
  <link rel=icon href="/assets/images/favicon.png">
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Bai+Jamjuree:wght@600;700&display=swap">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;700&display=swap">

  <link rel="stylesheet" type="text/css" href="/node_modules/font-awesome/css/font-awesome.css">
  <link rel="stylesheet" type="text/css" href="/node_modules/bootstrap/dist/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/node_modules/slick-carousel/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="/node_modules/jquery-ui-dist/jquery-ui.min.css"/>
  <link rel="stylesheet" type="text/css" href="/node_modules/select2/dist/css/select2.min.css"/>
  <link rel="stylesheet" type="text/css" href="/assets/css/styles.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/page.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/nav.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/cards.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/home.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/faculty.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/sponsors.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/students.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/courses.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/groups.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/publications.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/news.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/gallery.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/sc-links.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/footer.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/margins.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/paddings.css">
  <link rel="stylesheet" type="text/css" href="/assets/css/spacings.css">
  
  <script type="text/javascript" src="/node_modules/jquery/dist/jquery.min.js"></script>
  <script type="text/javascript" src="/node_modules/popper.js/dist/umd/popper.min.js"></script>
  <script type="text/javascript" src="/node_modules/bootstrap/dist/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="/node_modules/slick-carousel/slick/slick.min.js"></script>
  <script type="text/javascript" src="/node_modules/jquery-ui-dist/jquery-ui.min.js"></script>
  <script type="text/javascript" src="/node_modules/jquery-ui-touch-punch/jquery.ui.touch-punch.min.js"></script>
  <script type="text/javascript" src="/node_modules/select2/dist/js/select2.min.js"></script>

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Our Publications | Georgia Tech Visualization Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Our Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Our mission is to empower everyone to analyze and communicate data with interactive systems. The team is made up of four groups that lead visualization research in Information Visualization, Human-Computer Interaction, Visual Analytics, and Geographic Information Systems." />
<meta property="og:description" content="Our mission is to empower everyone to analyze and communicate data with interactive systems. The team is made up of four groups that lead visualization research in Information Visualization, Human-Computer Interaction, Visual Analytics, and Geographic Information Systems." />
<link rel="canonical" href="https://vis.gatech.edu/publications/" />
<meta property="og:url" content="https://vis.gatech.edu/publications/" />
<meta property="og:site_name" content="Georgia Tech Visualization Lab" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Our Publications" />
<meta name="twitter:site" content="@GT_Vis" />
<meta name="twitter:creator" content="@GT_Vis" />
<script type="application/ld+json">
{"description":"Our mission is to empower everyone to analyze and communicate data with interactive systems. The team is made up of four groups that lead visualization research in Information Visualization, Human-Computer Interaction, Visual Analytics, and Geographic Information Systems.","url":"https://vis.gatech.edu/publications/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://vis.gatech.edu/assets/images/logo_bluegold_aspect_2_1.png"}},"@type":"WebPage","headline":"Our Publications","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-6V9MFRB4SQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-6V9MFRB4SQ');
  </script>
  
</head>
<body>
  <div id="nav">
  <div class="mob-hide">
    <a href="/" >Home</a>
    <a href="/news/" >News</a>
    <a href="/faculty/" >Faculty</a>
    <a href="/students/" >Students</a>
    <a href="/groups/" >Groups</a>
    <a href="/courses/" >Courses</a>
    <a href="/showcase/" >Showcase</a>
    <a href="/publications/"  class="active"
      >Publications</a>
    <a href="/sponsors/" >Sponsors</a>
  </div>

  <!--Navbar-->
  <nav class="navbar back-white" id="ham-menu">


    <!-- Collapse button -->
    <button class="navbar-toggler toggler-example" id="hamburger-button" type="button" data-toggle="collapse"
      data-target="#navbarSupportedContent1" aria-controls="navbarSupportedContent1" aria-expanded="false"
      aria-label="Toggle navigation"><span class="dark-blue-text"><i class="fa fa-bars"></i></span></button>

    <h1 id="ham-logo" class="hide-ham">GT Visualization Lab</h1>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent1">

      <!-- Links -->
      <ul class="navbar-nav mr-auto">
        <div class="container">
          <div class="row">
            <li class="nav-item col-12 col-md-4">
              <a href="/" >Home</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/news/" >News</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/faculty/" >Faculty</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/students/" >Students</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/groups/" >Groups</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/courses/" >Courses</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/showcase/" >Showcase</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/publications/"  class="active"
                >Publications</a>
            </li>
            <li class="nav-item col-12 col-md-4">
              <a href="/sponsors/" >Sponsors</a>
            </li>
          </div>
        </div>
      </ul>
      <!-- Links -->
    </div>
    <!-- Collapsible content -->
  </nav>
  <!--/.Navbar-->
</div>

<script>
  let prev = 0;


  $(window).on('scroll', function () {
    // Current Scroll Position
    let scrollTop = $(window).scrollTop();


    // Initial Navbar Condition
    $('#ham-logo').toggleClass('hide-ham', scrollTop < 300);
    $('#ham-menu').toggleClass('bottom-border', scrollTop > 300);

    // Nav-bar behaviour 
    if (scrollTop > 300) {
      $('#ham-menu').toggleClass('hidden-nav', scrollTop > prev);
      prev = scrollTop;
    } 
  });

  $('#hamburger-button').on('click', function() {
    $('#ham-menu').toggleClass('expanded-menu ');
  });

</script>
  <div class="flex-1"><div class="vspace-lg"></div>
<section id="page">
  <div class="container">
    <div>
      <div class="row">
        <div class="col-lg-2 col-md-2 col-sm-3"></div>
        <div class="col-lg-8 col-md-8 col-sm-6">
          
        </div>
        <div class="col-lg-2 col-md-2 col-sm-3"></div>
      </div>
      <div class="row">
        <div class="col-lg-2"></div>
        <div class="col-lg-8">
          <h1>
            Our Publications
          </h1>
          <hr />
          <p class="intro">
            We actively publish at premier conferences and journals in Visualization, Human Computer Interaction, Geographic Information Systems, Machine Learning, and Data Mining.
          </p>
        </div>
        <div class="col-lg-2"></div>
      </div>
    </div>
    <div class="vspace-lg"></div>
    <form class="form">
    <div class="row">
        <div class="col-lg-2 col-md-6 col-sm-6 mb-2">
            <label for="sTitle">Title</label>
            <select class="form-control mr-sm-2 mb-2" type="text" id="sTitle" name="sTitle" placeholder="Title" multiple=""></select>
        </div>
        <div class="col-lg-2 col-md-6 col-sm-6 mb-2">
            <label for="sAuthor">Author</label>
            <select class="form-control mr-sm-2 mb-2" id="sAuthor" name="sAuthor" placeholder="Author" multiple=""></select>
        </div>
        <div class="col-lg-4 col-md-12 col-sm-12 mb-2">
            <label for="sYear">Year</label>
            <div class="mr-sm-2 mb-2" id="sYear" name="sYear">
                <div id="custom-handle1" class="ui-slider-handle"></div>
                <div id="custom-handle2" class="ui-slider-handle"></div>
            </div>
        </div>
        <div class="col-lg-2 col-md-6 col-sm-6 mb-2">
            <label for="sVenue">Venue</label>
            <select class="form-control mr-sm-2 mb-2" id="sVenue" name="sVenue" placeholder="Venue" multiple=""></select>
        </div>
        <div class="col-lg-2 col-md-6 col-sm-6 mb-2">
            <label for="sTag">Tag</label>
            <select class="form-control mr-sm-2 mb-2" type="text" id="sTag" name="sTag" placeholder="Tag" multiple=""></select>
        </div>
    </div>
    <div class="row">
        <div class="col">
            <label class="sr-only" for="clearBtn"></label>
            <a id="clearBtn" href="javascript:clear();">
                clear all filters
            </a>
        </div>
    </div>
</form>

<p><br /><br /></p>

<div id="publications" class="row">
    
    
    <div class="publication col-lg-12" data-pub-id="0">
        <h5 class="caps">SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.</h5>
        <div class="authors">
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Anindya S. Paul</span><span>,</span>
            
                <span class="caps">Pruthvi Perumalla</span><span>,</span>
            
                <span class="caps">Zhiyan Zhou</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SkeletonVis%3A+Interactive+Visualization+for+Understanding+Adversarial+Attacks+on+Human+Action+Recognition+Models." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="1">
        <h5 class="caps">SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.</h5>
        <div class="authors">
            
                <span class="caps">Arpit Narechania</span><span>,</span>
            
                <span class="caps">Ahsan Qamar</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SafetyLens%3A+Visual+Data+Analysis+of+Functional+Safety+of+Vehicles." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="2">
        <h5 class="caps">RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Austin P. Wright</span><span>,</span>
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Will Epperson</span><span>,</span>
            
                <span class="caps">Muhammed Ahmed</span><span>,</span>
            
                <span class="caps">Stephane Pinel</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Diyi Yang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=RECAST%3A+Enabling+User+Recourse+and+Interpretability+of+Toxicity+Detection+Models+with+Interactive+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="3">
        <h5 class="caps">NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.</h5>
        <div class="authors">
            
                <span class="caps">Arpit Narechania</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=NL4DV%3A+A+Toolkit+for+Generating+Analytic+Specifications+for+Data+Visualization+from+Natural+Language+Queries." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="4">
        <h5 class="caps">MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.</h5>
        <div class="authors">
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Rahul Duggal</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=MalNet%3A+A+Large-Scale+Cybersecurity+Image+Database+of+Malicious+Software." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="5">
        <h5 class="caps">EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.</h5>
        <div class="authors">
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Jon Saad-Falcon</span><span>,</span>
            
                <span class="caps">Austin P. Wright</span><span>,</span>
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Omar Isaac Asensio</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=EnergyVis%3A+Interactively+Tracking+and+Exploring+Energy+Consumption+for+ML+Models." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="6">
        <h5 class="caps">Dodrio: Exploring Transformer Models with Interactive Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Robert Turko</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Dodrio%3A+Exploring+Transformer+Models+with+Interactive+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="7">
        <h5 class="caps">Data Animator: Authoring Expressive Animated Data Graphics.</h5>
        <div class="authors">
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Data+Animator%3A+Authoring+Expressive+Animated+Data+Graphics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="8">
        <h5 class="caps">CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Dylan Cashman</span><span>,</span>
            
                <span class="caps">Shenyu Xu</span><span>,</span>
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Florian Heimerl</span><span>,</span>
            
                <span class="caps">Cong Liu</span><span>,</span>
            
                <span class="caps">Shah Rukh Humayoun</span><span>,</span>
            
                <span class="caps">Michael Gleicher</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Remco Chang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=CAVA%3A+A+Visual+Analytics+System+for+Exploratory+Columnar+Data+Augmentation+Using+Knowledge+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="9">
        <h5 class="caps">CACTUS: Detecting and Resolving Conflicts in Objective Functions.</h5>
        <div class="authors">
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2021</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=CACTUS%3A+Detecting+and+Resolving+Conflicts+in+Objective+Functions." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="10">
        <h5 class="caps">Mapping Researchers with PeopleMap.</h5>
        <div class="authors">
            
                <span class="caps">Jon Saad-Falcon</span><span>,</span>
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Austin P. Wright</span><span>,</span>
            
                <span class="caps">Sasha Richardson</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mapping+Researchers+with+PeopleMap." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="11">
        <h5 class="caps">Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.</h5>
        <div class="authors">
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Wilmot Li</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Empirical Studies In Visualization </span>
            
                <span class="badge badge-secondary caps"> Humancentered Computing </span>
            
                <span class="badge badge-secondary caps"> Visualization Theory </span>
            
                <span class="badge badge-secondary caps"> Concepts And Paradigms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Understanding+the+Design+Space+and+Authoring+Paradigms+for+Animated+Data+Graphics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="12">
        <h5 class="caps">Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Adam Coscia</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Toward+a+Bias-Aware+Future+for+Mixed-Initiative+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="13">
        <h5 class="caps">Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Ayshwarya Saktheeswaran</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Touch%3F+Speech%3F+or+Touch+and+Speech%3F+Investigating+Multimodal+Interaction+for+Visual+Network+Exploration+and+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="14">
        <h5 class="caps">Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.</h5>
        <div class="authors">
            
                <span class="caps">Joshua Baker</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Daniel DellaPosta</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Spatial+Social+Network+%28SSN%29+Hot+Spot+Detection%3A+Scan+Methods+for+Non-Planar+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="15">
        <h5 class="caps">Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.</h5>
        <div class="authors">
            
                <span class="caps">Fabian Sperrle</span><span>,</span>
            
                <span class="caps">Mennatallah El-Assady</span><span>,</span>
            
                <span class="caps">Grace Guo</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Daniel A. Keim</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Should+We+Trust+%28X%29AI%3F+Design+Dimensions+for+Structured+Experimental+Evaluations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="16">
        <h5 class="caps">REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.</h5>
        <div class="authors">
            
                <span class="caps">Rahul Duggal</span><span>,</span>
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Cao Xiao</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Jimeng Sun</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WWW 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=REST%3A+Robust+and+Efficient+Neural+Networks+for+Sleep+Monitoring+in+the+Wild." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="17">
        <h5 class="caps">Reaching Broader Audiences With Data Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">Eun Kyoung Choe</span><span>,</span>
            
                <span class="caps">Petra Isenberg</span><span>,</span>
            
                <span class="caps">Kim Marriott</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Theresa-Marie Rhyne</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Software Tools </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Smart Phones </span>
            
                <span class="badge badge-secondary caps"> Public Infrastructure </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Reaching+Broader+Audiences+With+Data+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="18">
        <h5 class="caps">QUESTO: Interactive Construction of Objective Functions for Classification Tasks.</h5>
        <div class="authors">
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Shenyu Xu</span><span>,</span>
            
                <span class="caps">Michael Gleicher</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Mathematics Of Computing </span>
            
                <span class="badge badge-secondary caps"> Interactive Objective Functions </span>
            
                <span class="badge badge-secondary caps"> Machine Learning Task </span>
            
                <span class="badge badge-secondary caps"> Model Construction And Selection </span>
            
                <span class="badge badge-secondary caps"> Classification </span>
            
                <span class="badge badge-secondary caps"> Humancentered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=QUESTO%3A+Interactive+Construction+of+Objective+Functions+for+Classification+Tasks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="19">
        <h5 class="caps">Putting the "I" in Interaction: Interactive Interfaces Personalized to Individuals.</h5>
        <div class="authors">
            
                <span class="caps">John E. Wenskovitch</span><span>,</span>
            
                <span class="caps">Michelle X. Zhou</span><span>,</span>
            
                <span class="caps">Christopher Collins</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Michelle Dowling</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Kai Xu</span><span>,</span>
            
                <span class="caps">Theresa-Marie Rhyne</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Adaptive Systems </span>
            
                <span class="badge badge-secondary caps"> Memory Management </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Putting+the+%22I%22+in+Interaction%3A+Interactive+Interfaces+Personalized+to+Individuals." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="20">
        <h5 class="caps">Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.</h5>
        <div class="authors">
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Robert Firstman</span><span>,</span>
            
                <span class="caps">Emily Rogers</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Adversarial Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Graph Analytics </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/massif" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Massif%3A+Interactive+Interpretation+of+Adversarial+Attacks+on+Deep+Learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="21">
        <h5 class="caps">What are Data Insights to Professional Visualization Users?</h5>
        <div class="authors">
            
                <span class="caps">Po-Ming Law</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE VIS (Short Papers) 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=What+are+Data+Insights+to+Professional+Visualization+Users%3F" target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="22">
        <h5 class="caps">Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Samuel Huron</span><span>,</span>
            
                <span class="caps">Charles Perin</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Instruments </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Encoding </span>
            
                <span class="badge badge-secondary caps"> Image Color Analysis </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Investigating+Direct+Manipulation+of+Graphical+Encodings+as+a+Method+for+User+Interaction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="23">
        <h5 class="caps">Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interweaving+Multimodal+Interaction+with+Flexible+Unit+Visualizations+for+Data+Exploration." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="24">
        <h5 class="caps">Interpreting Deep Neural Networks through Prototype Factorization.</h5>
        <div class="authors">
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Panpan Xu</span><span>,</span>
            
                <span class="caps">Zeng Dai</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Liu Ren</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICDM (Workshops) 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interpreting+Deep+Neural+Networks+through+Prototype+Factorization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="25">
        <h5 class="caps">Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).</h5>
        <div class="authors">
            
                <span class="caps">Daniela Oelke</span><span>,</span>
            
                <span class="caps">Daniel A. Keim</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Dagstuhl Reports 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactive+Visualization+for+Fostering+Trust+in+AI+%28Dagstuhl+Seminar+20382%29." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="26">
        <h5 class="caps">How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Daniel F. Keefe</span><span>,</span>
            
                <span class="caps">Melanie Tory</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Measurement </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Training </span>
            
                <span class="badge badge-secondary caps"> Natural Languages </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=How+to+Ask+What+to+Say%3F%3A+Strategies+for+Evaluating+Natural+Language+Interfaces+for+Data+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="27">
        <h5 class="caps">GOGGLES: Automatic Image Labeling with Affinity Coding.</h5>
        <div class="authors">
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Sanya Chaba</span><span>,</span>
            
                <span class="caps">Renzhi Wu</span><span>,</span>
            
                <span class="caps">Sakshi Gandhi</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Xu Chu</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Learning Settings </span>
            
                <span class="badge badge-secondary caps"> Learning Paradigms </span>
            
                <span class="badge badge-secondary caps"> Cluster Analysis </span>
            
                <span class="badge badge-secondary caps"> Computer Vision </span>
            
                <span class="badge badge-secondary caps"> Mathematics Of Computing </span>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Computer Vision Representations </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Unsupervised Learning </span>
            
                <span class="badge badge-secondary caps"> Probability And Statistics </span>
            
                <span class="badge badge-secondary caps"> Artificial Intelligence </span>
            
                <span class="badge badge-secondary caps"> Probabilistic Inference Problems </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>SIGMOD Conference 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GOGGLES%3A+Automatic+Image+Labeling+with+Affinity+Coding." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="28">
        <h5 class="caps">Gaggle: Visual Analytics for Model Space Navigation.</h5>
        <div class="authors">
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Dylan Cashman</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Gaggle%3A+Visual+Analytics+for+Model+Space+Navigation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="29">
        <h5 class="caps">Examining the Ordering of Rhetorical Strategies in Persuasive Requests.</h5>
        <div class="authors">
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Jiaao Chen</span><span>,</span>
            
                <span class="caps">Jon Saad-Falcon</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Diyi Yang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Examining+the+Ordering+of+Rhetorical+Strategies+in+Persuasive+Requests." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="30">
        <h5 class="caps">Evaluating Graph Vulnerability and Robustness using TIGER.</h5>
        <div class="authors">
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluating+Graph+Vulnerability+and+Robustness+using+TIGER." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="31">
        <h5 class="caps">EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.</h5>
        <div class="authors">
            
                <span class="caps">Haipeng Zeng</span><span>,</span>
            
                <span class="caps">Xingbo Wang</span><span>,</span>
            
                <span class="caps">Aoyu Wu</span><span>,</span>
            
                <span class="caps">Yong Wang 0021</span><span>,</span>
            
                <span class="caps">Quan Li</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Huamin Qu</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Videos </span>
            
                <span class="badge badge-secondary caps"> Feature Extraction </span>
            
                <span class="badge badge-secondary caps"> Emotion Recognition </span>
            
                <span class="badge badge-secondary caps"> Coherence </span>
            
                <span class="badge badge-secondary caps"> Face </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=EmoCo%3A+Visual+Analysis+of+Emotion+Coherence+in+Presentation+Videos." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="32">
        <h5 class="caps">ELF: An Early-Exiting Framework for Long-Tailed Classification.</h5>
        <div class="authors">
            
                <span class="caps">Rahul Duggal</span><span>,</span>
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Sunny Dhamnani</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Jimeng Sun</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=ELF%3A+An+Early-Exiting+Framework+for+Long-Tailed+Classification." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="33">
        <h5 class="caps">D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.</h5>
        <div class="authors">
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Andrew Wicker</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Joshua Neil</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=D2M%3A+Dynamic+Defense+and+Modeling+of+Adversarial+Movement+in+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="34">
        <h5 class="caps">Critical Reflections on Visualization Authoring Systems.</h5>
        <div class="authors">
            
                <span class="caps">Arvind Satyanarayan</span><span>,</span>
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">Donghao Ren</span><span>,</span>
            
                <span class="caps">Jeffrey Heer</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Matthew Brehmer</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Interactive Systems </span>
            
                <span class="badge badge-secondary caps"> Libraries </span>
            
                <span class="badge badge-secondary caps"> Programming </span>
            
                <span class="badge badge-secondary caps"> Authoring Systems </span>
            
                <span class="badge badge-secondary caps"> Grammar </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Critical+Reflections+on+Visualization+Authoring+Systems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="35">
        <h5 class="caps">CNN 101: Interactive Visual Learning for Convolutional Neural Networks.</h5>
        <div class="authors">
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Robert Turko</span><span>,</span>
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Education </span>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Convolutional Neural Networks </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://zijie.wang/papers/cnn-101/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=CNN+101%3A+Interactive+Visual+Learning+for+Convolutional+Neural+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="36">
        <h5 class="caps">CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Robert Turko</span><span>,</span>
            
                <span class="caps">Omar Shaikh</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Education </span>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Convolutional Neural Networks </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://poloclub.github.io/cnn-explainer/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=CNN+Explainer%3A+Learning+Convolutional+Neural+Networks+with+Interactive+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="37">
        <h5 class="caps">Characterizing Automated Data Insights.</h5>
        <div class="authors">
            
                <span class="caps">Po-Ming Law</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE VIS (Short Papers) 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Characterizing+Automated+Data+Insights." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="38">
        <h5 class="caps">Causal Perception in Question-Answering Systems.</h5>
        <div class="authors">
            
                <span class="caps">Po-Ming Law</span><span>,</span>
            
                <span class="caps">Leo Yu-Ho Lo</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Huamin Qu</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Causal+Perception+in+Question-Answering+Systems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="39">
        <h5 class="caps">Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.</h5>
        <div class="authors">
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Robert Firstman</span><span>,</span>
            
                <span class="caps">Emily Rogers</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Adversarial Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Graph Analytics </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VIS 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/bluff" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Bluff%3A+Interactively+Deciphering+Adversarial+Attacks+on+Deep+Neural+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="40">
        <h5 class="caps">AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.</h5>
        <div class="authors">
            
                <span class="caps">Dong-Kyu Chae</span><span>,</span>
            
                <span class="caps">Jihoo Kim</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Sang-Wook Kim</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SIGIR 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=AR-CF%3A+Augmenting+Virtual+Users+and+Items+in+Collaborative+Filtering+for+Addressing+Cold-Start+Problems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="41">
        <h5 class="caps">A Large-Scale Database for Graph Representation Learning.</h5>
        <div class="authors">
            
                <span class="caps">Scott Freitas</span><span>,</span>
            
                <span class="caps">Yuxiao Dong</span><span>,</span>
            
                <span class="caps">Joshua Neil</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Large-Scale+Database+for+Graph+Representation+Learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="42">
        <h5 class="caps">A Comparative Analysis of Industry Human-AI Interaction Guidelines.</h5>
        <div class="authors">
            
                <span class="caps">Austin P. Wright</span><span>,</span>
            
                <span class="caps">Zijie J. Wang</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Grace Guo</span><span>,</span>
            
                <span class="caps">Fabian Sperrle</span><span>,</span>
            
                <span class="caps">Mennatallah El-Assady</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Daniel A. Keim</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Comparative+Analysis+of+Industry+Human-AI+Interaction+Guidelines." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="43">
        <h5 class="caps">Communicating with Interactive Articles.</h5>
        <div class="authors">
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Matthew Conlen</span><span>,</span>
            
                <span class="caps">Jeffrey Heer</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Interactive Articles </span>
            
                <span class="badge badge-secondary caps"> Data Storytelling </span>
            
                <span class="badge badge-secondary caps"> Explorable Explanations </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Distill 2020</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://distill.pub/2020/communicating-with-interactive-articles/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Communicating+with+Interactive+Articles." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="44">
        <h5 class="caps">A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Leslie M. Blaha</span><span>,</span>
            
                <span class="caps">Celeste Lyn Paul</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Cognitive Bias </span>
            
                <span class="badge badge-secondary caps"> Anchoring Bias </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>INTERACT (2) 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Formative+Study+of+Interactive+Bias+Metrics+in+Visual+Analytics+Using+Anchoring+Bias." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="45">
        <h5 class="caps">Toward a Design Space for Mitigating Cognitive Bias in Vis.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognitive Biases </span>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Cognitive Processes </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Bias Mitigation Strategies </span>
            
                <span class="badge badge-secondary caps"> Design Space </span>
            
                <span class="badge badge-secondary caps"> Behavioural Sciences Computing </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Visual Analytic Tools </span>
            
                <span class="badge badge-secondary caps"> Decision Making </span>
            
                <span class="badge badge-secondary caps"> Cognitive Heuristics </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Training </span>
            
                <span class="badge badge-secondary caps"> Data Visualisation </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VIS (Short Papers) 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Toward+a+Design+Space+for+Mitigating+Cognitive+Bias+in+Vis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="46">
        <h5 class="caps">TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.</h5>
        <div class="authors">
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Dongjin Choi</span><span>,</span>
            
                <span class="caps">Barry L. Drake</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Haesun Park</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Retrieved Subset </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Simple Keyword Matching Search </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Large-scale Document Retrieval </span>
            
                <span class="badge badge-secondary caps"> Document Collections </span>
            
                <span class="badge badge-secondary caps"> Topicsifter </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Document Handling </span>
            
                <span class="badge badge-secondary caps"> Missed Relevant Documents </span>
            
                <span class="badge badge-secondary caps"> Query Processing </span>
            
                <span class="badge badge-secondary caps"> Irrelevant Documents </span>
            
                <span class="badge badge-secondary caps"> Keyword Queries </span>
            
                <span class="badge badge-secondary caps"> Interactive Search Space Reduction </span>
            
                <span class="badge badge-secondary caps"> Data Visualisation </span>
            
                <span class="badge badge-secondary caps"> Negative Feedback </span>
            
                <span class="badge badge-secondary caps"> Relevance Feedback </span>
            
                <span class="badge badge-secondary caps"> Targeted Topic Modeling </span>
            
                <span class="badge badge-secondary caps"> Buildings </span>
            
                <span class="badge badge-secondary caps"> Matrix Decomposition </span>
            
                <span class="badge badge-secondary caps"> Search Problems </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VAST 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TopicSifter%3A+Interactive+Search+Space+Reduction+through+Targeted+Topic+Modeling." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="47">
        <h5 class="caps">The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Brynne Godfrey</span><span>,</span>
            
                <span class="caps">Carleen F. Maitland</span><span>,</span>
            
                <span class="caps">Matthew McGee</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>GeoHumanities@SIGSPATIAL 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+built+environment+and+syrian+refugee+integration+in+Turkey%3A+an+analysis+of+mobile+phone+data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="48">
        <h5 class="caps">TexTonic: Interactive visualization for exploration and discovery of very large text collections.</h5>
        <div class="authors">
            
                <span class="caps">Celeste Lyn Paul</span><span>,</span>
            
                <span class="caps">Jessica Chang</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Nick Cramer</span><span>,</span>
            
                <span class="caps">David Gillen</span><span>,</span>
            
                <span class="caps">Shawn D. Hampton</span><span>,</span>
            
                <span class="caps">Russ Burtner</span><span>,</span>
            
                <span class="caps">Ralph Perko</span><span>,</span>
            
                <span class="caps">Kristin A. Cook</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TexTonic%3A+Interactive+visualization+for+exploration+and+discovery+of+very+large+text+collections." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="49">
        <h5 class="caps">Task-Based Effectiveness of Basic Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Correlation </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Automobiles </span>
            
                <span class="badge badge-secondary caps"> Motion Pictures </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Task-Based+Effectiveness+of+Basic+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="50">
        <h5 class="caps">Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.</h5>
        <div class="authors">
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Caleb Robinson</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Interpretability </span>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Activation Summarization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Summit%3A+Scaling+Deep+Learning+Interpretability+by+Visualizing+Activation+and+Attribution+Summarizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="51">
        <h5 class="caps">Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.</h5>
        <div class="authors">
            
                <span class="caps">Xi Liu</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Sohrab Rahimi</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Comput. Environ. Urban Syst. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Place+niche+and+its+regional+variability%3A+Measuring+spatial+context+patterns+for+points+of+interest+with+representation+learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="52">
        <h5 class="caps">NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.</h5>
        <div class="authors">
            
                <span class="caps">Haekyu Park</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Interpretability </span>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Activation Summarization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>PacificVis 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="http://haekyu.com/neural-divergence/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=NeuralDivergence%3A+Exploring+and+Understanding+Neural+Networks+by+Comparing+Activation+Distributions." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="53">
        <h5 class="caps">Mixed Reality for Learning Programming.</h5>
        <div class="authors">
            
                <span class="caps">Joonyoung Kim</span><span>,</span>
            
                <span class="caps">Kristina Marotta</span><span>,</span>
            
                <span class="caps">Jonathan Leo</span><span>,</span>
            
                <span class="caps">Sudeep Agarwal</span><span>,</span>
            
                <span class="caps">Siwei Li</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IDC 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mixed+Reality+for+Learning+Programming." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="54">
        <h5 class="caps">Metrics for characterizing network structure and node importance in Spatial Social Networks.</h5>
        <div class="authors">
            
                <span class="caps">Dipto Sarkar</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Colin A. Chapman</span><span>,</span>
            
                <span class="caps">Raja R. Sengupta</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Int. J. Geogr. Inf. Sci. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Metrics+for+characterizing+network+structure+and+node+importance+in+Spatial+Social+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="55">
        <h5 class="caps">Liger: Combining Interaction Paradigms for Visual Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Lei Jiang</span><span>,</span>
            
                <span class="caps">Charles Perin</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Liger%3A+Combining+Interaction+Paradigms+for+Visual+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="56">
        <h5 class="caps">Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Humancentered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Investigating+the+Manual+View+Specification+and+Visualization+by+Demonstration+Paradigms+for+Visualization+Construction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="57">
        <h5 class="caps">Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Jean-Daniel Fekete</span><span>,</span>
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">Shixia Liu</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>NII Shonan Meet. Rep. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactive+Visualization+for+Interpretable+Machine+Learning+~+Beyond+Visualization+and+Steering+of+the+Parametric+Space+%28NII+Shonan+Meeting+161%29." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="58">
        <h5 class="caps">Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.</h5>
        <div class="authors">
            
                <span class="caps">Xi Liu</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Zixuan Huang</span><span>,</span>
            
                <span class="caps">Sohrab Rahimi</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EPJ Data Sci. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Inside+50%2C+000+living+rooms%3A+an+assessment+of+global+residential+ornamentation+using+transfer+learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="59">
        <h5 class="caps">Geospatial Privacy and Security.</h5>
        <div class="authors">
            
                <span class="caps">Grant McKenzie</span><span>,</span>
            
                <span class="caps">Carsten Ke</span><span>,</span>
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>J. Spatial Inf. Sci. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Geospatial+Privacy+and+Security." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="60">
        <h5 class="caps">Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Bum Chul Kwon</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Geono-Cluster%3A+Interactive+Visual+Cluster+Analysis+for+Biologists." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="61">
        <h5 class="caps">FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.</h5>
        <div class="authors">
            
                <span class="caps">Alex Cabrera</span><span>,</span>
            
                <span class="caps">Will Epperson</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Jamie Morgenstern</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Machine Learning Fairness </span>
            
                <span class="badge badge-secondary caps"> Subgroup Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VAST 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://cabreraalex.com/#/paper/fairvis" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=FairVis%3A+Visual+Analytics+for+Discovering+Intersectional+Bias+in+Machine+Learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="62">
        <h5 class="caps">Embedded Merge  Split: Visual Adjustment of Data Grouping.</h5>
        <div class="authors">
            
                <span class="caps">Ali Sarvghad</span><span>,</span>
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Nadir Weibel</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Synthetic Aperture Sonar </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Histograms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Embedded+Merge++Split%3A+Visual+Adjustment+of+Data+Grouping." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="63">
        <h5 class="caps">ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.</h5>
        <div class="authors">
            
                <span class="caps">Xiangyun Lei</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Andrew J. Medford</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> 3D Visualization </span>
            
                <span class="badge badge-secondary caps"> Visualization Application </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VIS 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/electrolens" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=ElectroLens%3A+Understanding+Atomistic+Simulations+Through+Spatially-resolved+Visualization+of+High-dimensional+Features." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="64">
        <h5 class="caps">Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.</h5>
        <div class="authors">
            
                <span class="caps">Alex Cabrera</span><span>,</span>
            
                <span class="caps">Will Epperson</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Jamie Morgenstern</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Machine Learning Fairness </span>
            
                <span class="badge badge-secondary caps"> Subgroup Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>DebugML @ ICML 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://cabreraalex.com/#/paper/subgroup-gen" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Discovery+of+Intersectional+Bias+in+Machine+Learning+Using+Automatic+Subgroup+Generation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="65">
        <h5 class="caps">Demonstrational Interaction for Data Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Theresa-Marie Rhyne</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Demonstrational+Interaction+for+Data+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="66">
        <h5 class="caps">Broadening Intellectual Diversity in Visualization Research Papers.</h5>
        <div class="authors">
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">Kate Isaacs</span><span>,</span>
            
                <span class="caps">Danielle Albers Szafir</span><span>,</span>
            
                <span class="caps">G. Elisabeta Marai</span><span>,</span>
            
                <span class="caps">Cagatay Turkay</span><span>,</span>
            
                <span class="caps">Melanie Tory</span><span>,</span>
            
                <span class="caps">Sheelagh Carpendale</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Theresa-Marie Rhyne</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Lenses </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Broadening+Intellectual+Diversity+in+Visualization+Research+Papers." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="67">
        <h5 class="caps">BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.</h5>
        <div class="authors">
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Dylan Cashman</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Inspection </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=BEAMES%3A+Interactive+Multimodel+Steering%2C+Selection%2C+and+Inspection+for+Regression+Tasks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="68">
        <h5 class="caps">Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">Steven Mark Drucker</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Augmenting+Visualizations+with+Interactive+Data+Facts+to+Facilitate+Interpretation+and+Communication." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="69">
        <h5 class="caps">Atlas: Local Graph Exploration in a Global Context.</h5>
        <div class="authors">
            
                <span class="caps">James Abello</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Varun Bezzam</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Graph Analytics </span>
            
                <span class="badge badge-secondary caps"> Graph Algorithms </span>
            
                <span class="badge badge-secondary caps"> Interactive Graph Decomposition </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IUI 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/atlas" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Atlas%3A+Local+Graph+Exploration+in+a+Global+Context." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="70">
        <h5 class="caps">A User-based Visual Analytics Workflow for Exploratory Model Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Dylan Cashman</span><span>,</span>
            
                <span class="caps">Shah Rukh Humayoun</span><span>,</span>
            
                <span class="caps">Florian Heimerl</span><span>,</span>
            
                <span class="caps">Kendall Park</span><span>,</span>
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Abigail Mosca</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Michael Gleicher</span><span>,</span>
            
                <span class="caps">Remco Chang</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Model Development And Analysis </span>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Mathematics Of Computing </span>
            
                <span class="badge badge-secondary caps"> Humancentered Computing </span>
            
                <span class="badge badge-secondary caps"> Exploratory Data Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+User-based+Visual+Analytics+Workflow+for+Exploratory+Model+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="71">
        <h5 class="caps">A Provenance Task Abstraction Framework.</h5>
        <div class="authors">
            
                <span class="caps">Christian Bors</span><span>,</span>
            
                <span class="caps">John E. Wenskovitch</span><span>,</span>
            
                <span class="caps">Michelle Dowling</span><span>,</span>
            
                <span class="caps">Simon Attfield</span><span>,</span>
            
                <span class="caps">Leilani Battle</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Olga Kulyk</span><span>,</span>
            
                <span class="caps">Robert S. Laramee</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> History </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Provenance+Task+Abstraction+Framework." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="72">
        <h5 class="caps">A Heuristic Approach to Value-Driven Evaluation of Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Meeshu Agnihotri</span><span>,</span>
            
                <span class="caps">Laura E. Matzen</span><span>,</span>
            
                <span class="caps">Kristin Divis</span><span>,</span>
            
                <span class="caps">Michael Haass</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Usability </span>
            
                <span class="badge badge-secondary caps"> Guidelines </span>
            
                <span class="badge badge-secondary caps"> Task Analysis </span>
            
                <span class="badge badge-secondary caps"> Benchmark Testing </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Heuristic+Approach+to+Value-Driven+Evaluation+of+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="73">
        <h5 class="caps">Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.</h5>
        <div class="authors">
            
                <span class="caps">Julia Deeb-Swihart</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Amy S. Bruckman</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Understanding+Law+Enforcement+Strategies+and+Needs+for+Combating+Human+Trafficking." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="74">
        <h5 class="caps">VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.</h5>
        <div class="authors">
            
                <span class="caps">Mallika Agarwal</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Tiles </span>
            
                <span class="badge badge-secondary caps"> Strips </span>
            
                <span class="badge badge-secondary caps"> Motion Pictures </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Histograms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VIS (Short Papers) 2019</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VisWall%3A+Visual+Data+Exploration+Using+Direct+Combination+on+Large+Touch+Displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="75">
        <h5 class="caps">Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Alan Wilson</span><span>,</span>
            
                <span class="caps">Mira Dontcheva</span><span>,</span>
            
                <span class="caps">James Delorey</span><span>,</span>
            
                <span class="caps">Sam Grigg</span><span>,</span>
            
                <span class="caps">Bernard Kerr</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Data+Illustrator%3A+Augmenting+Vector+Design+Tools+with+Lazy+Data+Binding+for+Expressive+Visualization+Authoring." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="76">
        <h5 class="caps">VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.</h5>
        <div class="authors">
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Edward Clarkson</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Changhyun Lee</span><span>,</span>
            
                <span class="caps">Fuxin Li</span><span>,</span>
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Ramakrishnan Kannan</span><span>,</span>
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Haesun Park</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ACM Trans. Knowl. Discov. Data 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VisIRR%3A+A+Visual+Analytics+System+for+Information+Retrieval+and+Recommendation+for+Large-Scale+Document+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="77">
        <h5 class="caps">VIGOR: Interactive Visual Exploration of Graph Query Results.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Kevin A. Roundy</span><span>,</span>
            
                <span class="caps">Christopher S. Gates</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Interactive Graph Querying </span>
            
                <span class="badge badge-secondary caps"> Graph Query Summarization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/vigor" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VIGOR%3A+Interactive+Visual+Exploration+of+Graph+Query+Results." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="78">
        <h5 class="caps">Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.</h5>
        <div class="authors">
            
                <span class="caps">Mengxue Yue</span><span>,</span>
            
                <span class="caps">Chaogui Kang</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Kun Qin</span><span>,</span>
            
                <span class="caps">Yu Liu</span><span>,</span>
            
                <span class="caps">Qingxiang Meng</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Trans. GIS 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Understanding+the+interplay+between+bus%2C+metro%2C+and+cab+ridership+dynamics+in+Shenzhen%2C+China." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="79">
        <h5 class="caps">Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">Meeshu Agnihotri</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Touching+Data%3A+A+Discoverability-based+Evaluation+of+a+Visualization+Interface+for+Tablet+Computers." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="80">
        <h5 class="caps">Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.</h5>
        <div class="authors">
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Interaction Techniques </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Visualization Techniques </span>
            
                <span class="badge badge-secondary caps"> Graph Drawings </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Gestural Input </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Tangraphe%3A+interactive+exploration+of+network+visualizations+using+single+hand%2C+multi-touch+gestures." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="81">
        <h5 class="caps">State of the Art of Sports Data Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Charles Perin</span><span>,</span>
            
                <span class="caps">Romain Vuillemot</span><span>,</span>
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Jo Wood</span><span>,</span>
            
                <span class="caps">Sheelagh Carpendale</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=State+of+the+Art+of+Sports+Data+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="82">
        <h5 class="caps">SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.</h5>
        <div class="authors">
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Madhuri Shanbhogue</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Siwei Li</span><span>,</span>
            
                <span class="caps">Li Chen</span><span>,</span>
            
                <span class="caps">Michael E. Kounavis</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Adversarial Machine Learning </span>
            
                <span class="badge badge-secondary caps"> JPEG Compression </span>
            
                <span class="badge badge-secondary caps"> Deep Learning </span>
            
                <span class="badge badge-secondary caps"> Computer Vision </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/shield" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SHIELD%3A+Fast%2C+Practical+Defense+and+Vaccination+for+Deep+Learning+using+JPEG+Compression." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="83">
        <h5 class="caps">Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.</h5>
        <div class="authors">
            
                <span class="caps">Alok Tripathy</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Oded Green</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Graph Analytics </span>
            
                <span class="badge badge-secondary caps"> Graph Decomposition </span>
            
                <span class="badge badge-secondary caps"> Scalable Graph Algorithms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/kcore" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Scalable+K-Core+Decomposition+for+Static+Graphs+Using+a+Dynamic+Graph+Data+Structure." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="84">
        <h5 class="caps">Robust Physical Adversarial Attack on Faster R-CNN Object Detector.</h5>
        <div class="authors">
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Cory Cornelius</span><span>,</span>
            
                <span class="caps">Jason Martin</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Robust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="85">
        <h5 class="caps">Podium: Ranking Data Using Mixed-Initiative Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">Ravish Chawla</span><span>,</span>
            
                <span class="caps">Bharath Kalidindi</span><span>,</span>
            
                <span class="caps">Eli T. Brown</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Support Vector Machines </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Prototypes </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Podium%3A+Ranking+Data+Using+Mixed-Initiative+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="86">
        <h5 class="caps">Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Speech </span>
            
                <span class="badge badge-secondary caps"> Taxonomy </span>
            
                <span class="badge badge-secondary caps"> Prototypes </span>
            
                <span class="badge badge-secondary caps"> Natural Languages </span>
            
                <span class="badge badge-secondary caps"> Mice </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Orko%3A+Facilitating+Multimodal+Interaction+for+Visual+Exploration+and+Analysis+of+Networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="87">
        <h5 class="caps">Multimodal interaction for data visualization.</h5>
        <div class="authors">
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Melanie Tory</span><span>,</span>
            
                <span class="caps">Vidya Setlur</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Paradigms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Multimodal+interaction+for+data+visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="88">
        <h5 class="caps">Interactive Classification for Deep Learning Interpretation.</h5>
        <div class="authors">
            
                <span class="caps">Alex Cabrera</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Jason Lin</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Interpretability </span>
            
                <span class="badge badge-secondary caps"> Computer Vision </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CVPR, Demo 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://cabreraalex.com/#/paper/interactive-classification" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactive+Classification+for+Deep+Learning+Interpretation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="89">
        <h5 class="caps">Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">Hyunwoo Park</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Rahul C. Basole</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Prototypes </span>
            
                <span class="badge badge-secondary caps"> Joining Processes </span>
            
                <span class="badge badge-secondary caps"> Image Color Analysis </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Graphiti%3A+Interactive+Specification+of+Attribute-Based+Edges+for+Network+Modeling+and+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="90">
        <h5 class="caps">GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.</h5>
        <div class="authors">
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Nikhil Thorat</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Fernanda B. Vi</span><span>,</span>
            
                <span class="caps">Martin Wattenberg</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Training </span>
            
                <span class="badge badge-secondary caps"> Gallium Nitride </span>
            
                <span class="badge badge-secondary caps"> Generative Adversarial Networks </span>
            
                <span class="badge badge-secondary caps"> Generators </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>arXiv 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GAN+Lab%3A+Understanding+Complex+Deep+Generative+Models+using+Interactive+Visual+Experimentation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="91">
        <h5 class="caps">Fraud Detection Using Social Network Analysis: A Case Study.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Encyclopedia of Social Network Analysis and Mining. 2nd Ed. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Fraud+Detection+Using+Social+Network+Analysis%3A+A+Case+Study." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="92">
        <h5 class="caps">Four Perspectives on Human Bias in Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Leslie M. Blaha</span><span>,</span>
            
                <span class="caps">Celeste Lyn Paul</span><span>,</span>
            
                <span class="caps">Kristin A. Cook</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Cognitive Biases in Visualizations 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Four+Perspectives+on+Human+Bias+in+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="93">
        <h5 class="caps">Evaluation of Visualization by Demonstration and Manual View Specification.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluation+of+Visualization+by+Demonstration+and+Manual+View+Specification." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="94">
        <h5 class="caps">Evaluating Interactive Graphical Encodings for Data Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">Eric D. Ragan</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Encoding </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Estimation </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluating+Interactive+Graphical+Encodings+for+Data+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="95">
        <h5 class="caps">Visual Analytics for Automated Model Discovery.</h5>
        <div class="authors">
            
                <span class="caps">Dylan Cashman</span><span>,</span>
            
                <span class="caps">Shah Rukh Humayoun</span><span>,</span>
            
                <span class="caps">Florian Heimerl</span><span>,</span>
            
                <span class="caps">Kendall Park</span><span>,</span>
            
                <span class="caps">Subhajit Das</span><span>,</span>
            
                <span class="caps">John Thompson</span><span>,</span>
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Abigail Mosca</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Michael Gleicher</span><span>,</span>
            
                <span class="caps">Remco Chang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analytics+for+Automated+Model+Discovery." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="96">
        <h5 class="caps">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.</h5>
        <div class="authors">
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Deep Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Neural Network Interpretability </span>
            
                <span class="badge badge-secondary caps"> Survey Paper </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/visual-analytics-in-deep-learning/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analytics+in+Deep+Learning%3A+An+Interrogative+Survey+for+the+Next+Frontiers." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="97">
        <h5 class="caps">Challenges for social flows.</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Xi Liu</span><span>,</span>
            
                <span class="caps">Joseph Ferreira</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Comput. Environ. Urban Syst. 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Challenges+for+social+flows." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="98">
        <h5 class="caps">Approximate Query Matching for Graph-Based Holistic Image Retrieval.</h5>
        <div class="authors">
            
                <span class="caps">Abhijit Suprem</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Calton Pu</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>BigData Congress 2018</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Approximate+Query+Matching+for+Graph-Based+Holistic+Image+Retrieval." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="99">
        <h5 class="caps">ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.</h5>
        <div class="authors">
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Nathan O. Hodas</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Machine Learning Visualization </span>
            
                <span class="badge badge-secondary caps"> Model Exploration </span>
            
                <span class="badge badge-secondary caps"> Interactive Machine Learning </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/shapeshop" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=ShapeShop%3A+Towards+Understanding+Deep+Learning+Representations+via+Interactive+Experimentation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="100">
        <h5 class="caps">Visualizing Social Media Content with SentenTree.</h5>
        <div class="authors">
            
                <span class="caps">Mengdie Hu</span><span>,</span>
            
                <span class="caps">Krist Wongsuphasawat</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Layout </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Tag Clouds </span>
            
                <span class="badge badge-secondary caps"> Media </span>
            
                <span class="badge badge-secondary caps"> Context </span>
            
                <span class="badge badge-secondary caps"> Games </span>
            
                <span class="badge badge-secondary caps"> Twitter </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualizing+Social+Media+Content+with+SentenTree." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="101">
        <h5 class="caps">Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Eli T. Brown</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Spatial Databases </span>
            
                <span class="badge badge-secondary caps"> Encoding </span>
            
                <span class="badge badge-secondary caps"> Automobiles </span>
            
                <span class="badge badge-secondary caps"> Image Color Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualization+by+Demonstration%3A+An+Interaction+Paradigm+for+Visual+Data+Exploration." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="102">
        <h5 class="caps">Visual Graph Query Construction and Refinement.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Graph Querying </span>
            
                <span class="badge badge-secondary caps"> Interactive Querying </span>
            
                <span class="badge badge-secondary caps"> Query Construction </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>SIGMOD Conference 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/visage" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Graph+Query+Construction+and+Refinement." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="103">
        <h5 class="caps">Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.</h5>
        <div class="authors">
            
                <span class="caps">Petra Isenberg</span><span>,</span>
            
                <span class="caps">Florian Heimerl</span><span>,</span>
            
                <span class="caps">Steffen Koch</span><span>,</span>
            
                <span class="caps">Tobias Isenberg</span><span>,</span>
            
                <span class="caps">Panpan Xu</span><span>,</span>
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">Michael Sedlmair</span><span>,</span>
            
                <span class="caps">Jian Chen</span><span>,</span>
            
                <span class="caps">Torsten M</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Metadata </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Portable Document Format </span>
            
                <span class="badge badge-secondary caps"> Libraries </span>
            
                <span class="badge badge-secondary caps"> History </span>
            
                <span class="badge badge-secondary caps"> Conferences </span>
            
                <span class="badge badge-secondary caps"> Terminology </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Vispubdata.org%3A+A+Metadata+Collection+About+IEEE+Visualization+%28VIS%29+Publications." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="104">
        <h5 class="caps">VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.</h5>
        <div class="authors">
            
                <span class="caps">Taeheon Kim</span><span>,</span>
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Blair MacIntyre</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VisAR%3A+Bringing+Interactivity+to+Static+Data+Visualizations+through+Augmented+Reality." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="105">
        <h5 class="caps">Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.</h5>
        <div class="authors">
            
                <span class="caps">Sohrab Rahimi</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Xi Liu</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>UrbanGIS@SIGSPATIAL 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Using+Yelp+to+Find+Romance+in+the+City%3A+A+Case+of+Restaurants+in+Four+Cities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="106">
        <h5 class="caps">TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Alex Godwin</span><span>,</span>
            
                <span class="caps">Yongxin Wang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EuroVis (Short Papers) 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TypoTweet+Maps%3A+Characterizing+Urban+Areas+through+Typographic+Social+Media+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="107">
        <h5 class="caps">Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.</h5>
        <div class="authors">
            
                <span class="caps">R. Jordan Crouser</span><span>,</span>
            
                <span class="caps">Lyndsey Franklin</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Kristin A. Cook</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Animals </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Education </span>
            
                <span class="badge badge-secondary caps"> Biological Cells </span>
            
                <span class="badge badge-secondary caps"> Genomics </span>
            
                <span class="badge badge-secondary caps"> Vegetation </span>
            
                <span class="badge badge-secondary caps"> Bioinformatics </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Toward+Theoretical+Techniques+for+Measuring+the+Use+of+Human+Effort+in+Visual+Analytic+Systems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="108">
        <h5 class="caps">The State of the Art in Integrating Machine Learning into Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">W. Ribarsky</span><span>,</span>
            
                <span class="caps">Cagatay Turkay</span><span>,</span>
            
                <span class="caps">B. L. William Wong</span><span>,</span>
            
                <span class="caps">Ian T. Nabney</span><span>,</span>
            
                <span class="caps">Ignacio D</span><span>,</span>
            
                <span class="caps">Fabrice Rossi</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Humancentred Computing </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Information Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+State+of+the+Art+in+Integrating+Machine+Learning+into+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="109">
        <h5 class="caps">The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Nick Cramer</span><span>,</span>
            
                <span class="caps">Grant Nakamura</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>HCI (14) 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+Impact+of+Streaming+Data+on+Sensemaking+with+Mixed-Initiative+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="110">
        <h5 class="caps">Supporting Team-First Visual Analytics through Group Activity Representations.</h5>
        <div class="authors">
            
                <span class="caps">Sriram Karthik Badam</span><span>,</span>
            
                <span class="caps">Zehua Zeng</span><span>,</span>
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Niklas Elmqvist</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Supporting+Team-First+Visual+Analytics+through+Group+Activity+Representations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="111">
        <h5 class="caps">Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Leslie M. Blaha</span><span>,</span>
            
                <span class="caps">Lyndsey Franklin</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> User Interactions </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Measurement </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Visual Analytic Tools </span>
            
                <span class="badge badge-secondary caps"> Interactive Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Interaxis </span>
            
                <span class="badge badge-secondary caps"> Human-in-the-loop Systems </span>
            
                <span class="badge badge-secondary caps"> Bias Assessment </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Domain Expertise </span>
            
                <span class="badge badge-secondary caps"> Bias Measurement </span>
            
                <span class="badge badge-secondary caps"> Cognitive Bias </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Data Visualisation </span>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Sensemaking Capabilities </span>
            
                <span class="badge badge-secondary caps"> Human Biases </span>
            
                <span class="badge badge-secondary caps"> Decision Making </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VAST 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Warning%2C+Bias+May+Occur%3A+A+Proposed+Approach+to+Detecting+Cognitive+Bias+in+Interactive+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="112">
        <h5 class="caps">Search Rank Fraud and Malware Detection in Google Play.</h5>
        <div class="authors">
            
                <span class="caps">Mahmudur Rahman</span><span>,</span>
            
                <span class="caps">Mizanur Rahman</span><span>,</span>
            
                <span class="caps">Bogdan Carbunar</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Knowl. Data Eng. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Search+Rank+Fraud+and+Malware+Detection+in+Google+Play." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="113">
        <h5 class="caps">Predicting Cyber Threats with Virtual Security Products.</h5>
        <div class="authors">
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Yufei Han</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christopher S. Gates</span><span>,</span>
            
                <span class="caps">Michael Hart</span><span>,</span>
            
                <span class="caps">Kevin A. Roundy</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ACSAC 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Predicting+Cyber+Threats+with+Virtual+Security+Products." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="114">
        <h5 class="caps">Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.</h5>
        <div class="authors">
            
                <span class="caps">Alex Godwin</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EuroVis (Short Papers) 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Nodes%2C+Paths%2C+and+Edges%3A+Using+Mental+Maps+to+Augment+Crime+Data+Analysis+in+Urban+Spaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="115">
        <h5 class="caps">Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.</h5>
        <div class="authors">
            
                <span class="caps">Arjun Srinivasan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EuroVis (Short Papers) 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Natural+Language+Interfaces+for+Data+Analysis+with+Visualization%3A+Considering+What+Has+and+Could+Be+Asked." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="116">
        <h5 class="caps">mHealth Visual Discovery Dashboard.</h5>
        <div class="authors">
            
                <span class="caps">Dezhi Fang</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Peter J. Polack Jr.</span><span>,</span>
            
                <span class="caps">Hillol Sarker</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Moushumi Sharmin</span><span>,</span>
            
                <span class="caps">Mustafa al'Absi</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Health Informatics </span>
            
                <span class="badge badge-secondary caps"> Time Series Data </span>
            
                <span class="badge badge-secondary caps"> Motif Discovery </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Ubicomp, Demo 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/dashboard" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=mHealth+Visual+Discovery+Dashboard." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="117">
        <h5 class="caps">Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.</h5>
        <div class="authors">
            
                <span class="caps">Nilaksh Das</span><span>,</span>
            
                <span class="caps">Madhuri Shanbhogue</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Li Chen</span><span>,</span>
            
                <span class="caps">Michael E. Kounavis</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Adversarial Machine Learning </span>
            
                <span class="badge badge-secondary caps"> JPEG Compression </span>
            
                <span class="badge badge-secondary caps"> Deep Learning </span>
            
                <span class="badge badge-secondary caps"> Computer Vision </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>arXiv 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/papers/defense" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Keeping+the+Bad+Guys+Out%3A+Protecting+and+Vaccinating+Deep+Learning+with+JPEG+Compression." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="118">
        <h5 class="caps">HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.</h5>
        <div class="authors">
            
                <span class="caps">Alex Godwin</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>HICSS 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=HotSketch%3A+Drawing+Police+Patrol+Routes+among+Spatiotemporal+Crime+Hotspots." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="119">
        <h5 class="caps">FACETS: Adaptive Local Exploration of Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Zhiyuan Lin</span><span>,</span>
            
                <span class="caps">Jilles Vreeken</span><span>,</span>
            
                <span class="caps">Partha P. Talukdar</span><span>,</span>
            
                <span class="caps">James Abello</span><span>,</span>
            
                <span class="caps">Ganesh Parameswaran</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SDM 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=FACETS%3A+Adaptive+Local+Exploration+of+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="120">
        <h5 class="caps">Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.</h5>
        <div class="authors">
            
                <span class="caps">Peter J. Polack Jr.</span><span>,</span>
            
                <span class="caps">Moushumi Sharmin</span><span>,</span>
            
                <span class="caps">Kaya de Barbaro</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Mobile Health - Sensors, Analytic Methods, and Applications 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Exploratory+Visual+Analytics+of+Mobile+Health+Data%3A+Sensemaking+Challenges+and+Opportunities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="121">
        <h5 class="caps">Data and Task Based Effectiveness of Basic Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Data+and+Task+Based+Effectiveness+of+Basic+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="122">
        <h5 class="caps">AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.</h5>
        <div class="authors">
            
                <span class="caps">Bum Chul Kwon</span><span>,</span>
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Emily Wall</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Automobiles </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
                <span class="badge badge-secondary caps"> Manifolds </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=AxiSketcher%3A+Interactive+Nonlinear+Axis+Mapping+of+Visualizations+through+User+Drawings." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="123">
        <h5 class="caps">A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.</h5>
        <div class="authors">
            
                <span class="caps">Fred Hohman</span><span>,</span>
            
                <span class="caps">Sandeep Soni</span><span>,</span>
            
                <span class="caps">Ian Stewart</span><span>,</span>
            
                <span class="caps">John Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Video Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Storytelling </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VIS4DH @ VIS 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!-- 
                <a href="https://fredhohman.com/a-viz-of-ice-and-fire/" target="_blank"><i class="fa fa-external-link"></i></a>
             -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Viz+of+Ice+and+Fire%3A+Exploring+Entertainment+Video+Using+Color+and+Dialogue." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="124">
        <h5 class="caps">ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.</h5>
        <div class="authors">
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Pierre Y. Andrews</span><span>,</span>
            
                <span class="caps">Aditya Kalro</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=ActiVis%3A+Visual+Exploration+of+Industry-Scale+Deep+Neural+Network+Models." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="125">
        <h5 class="caps">Analysis of Smoking and Drinking Relapse in an Online Community.</h5>
        <div class="authors">
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Munmun De Choudhury</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>DH 2017</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Analysis+of+Smoking+and+Drinking+Relapse+in+an+Online+Community." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="126">
        <h5 class="caps">What's Hot in Intelligent User Interfaces.</h5>
        <div class="authors">
            
                <span class="caps">Shimei Pan</span><span>,</span>
            
                <span class="caps">Oliver Brdiczka</span><span>,</span>
            
                <span class="caps">Giuseppe Carenini</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Per Ola Kristensson</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>AAAI 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=What%27s+Hot+in+Intelligent+User+Interfaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="127">
        <h5 class="caps">VISAGE: Interactive Visual Graph Querying.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>AVI 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VISAGE%3A+Interactive+Visual+Graph+Querying." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="128">
        <h5 class="caps">Redefining a Contribution for Immersive Visualization Research.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">Vidya Setlur</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ISS Companion 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Redefining+a+Contribution+for+Immersive+Visualization+Research." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="129">
        <h5 class="caps">Node Immunization on Large Graphs: Theory and Algorithms.</h5>
        <div class="authors">
            
                <span class="caps">Chen Chen 0022</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">B. Aditya Prakash</span><span>,</span>
            
                <span class="caps">Charalampos E. Tsourakakis</span><span>,</span>
            
                <span class="caps">Tina Eliassi-Rad</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Knowl. Data Eng. 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Node+Immunization+on+Large+Graphs%3A+Theory+and+Algorithms." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="130">
        <h5 class="caps">M3: Scaling Up Machine Learning via Memory Mapping.</h5>
        <div class="authors">
            
                <span class="caps">Dezhi Fang</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Operating Systems </span>
            
                <span class="badge badge-secondary caps"> Contextual Software Domains </span>
            
                <span class="badge badge-secondary caps"> Memory Management </span>
            
                <span class="badge badge-secondary caps"> Software And Its Engineering </span>
            
                <span class="badge badge-secondary caps"> Software Organization And Properties </span>
            
                <span class="badge badge-secondary caps"> Virtual Memory </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>SIGMOD Conference 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=M3%3A+Scaling+Up+Machine+Learning+via+Memory+Mapping." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="131">
        <h5 class="caps">InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.</h5>
        <div class="authors">
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Scalability </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
                <span class="badge badge-secondary caps"> Principal Component Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=InterAxis%3A+Steering+Scatterplot+Axes+via+Observation-Level+Interaction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="132">
        <h5 class="caps">Interactive visual co-cluster analysis of bipartite graphs.</h5>
        <div class="authors">
            
                <span class="caps">Panpan Xu</span><span>,</span>
            
                <span class="caps">Nan Cao</span><span>,</span>
            
                <span class="caps">Huamin Qu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Correlation </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Algorithm Design And Analysis </span>
            
                <span class="badge badge-secondary caps"> Bipartite Graph </span>
            
                <span class="badge badge-secondary caps"> Measurement </span>
            
                <span class="badge badge-secondary caps"> Clustering Algorithms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>PacificVis 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactive+visual+co-cluster+analysis+of+bipartite+graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="133">
        <h5 class="caps">Integrating social network data into GISystems.</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Int. J. Geogr. Inf. Sci. 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Integrating+social+network+data+into+GISystems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="134">
        <h5 class="caps">Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.</h5>
        <div class="authors">
            
                <span class="caps">Sohrab Rahimi</span><span>,</span>
            
                <span class="caps">Xi Liu</span><span>,</span>
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>UrbanGIS@SIGSPATIAL 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Hidden+style+in+the+city%3A+an+analysis+of+geolocated+airbnb+rental+images+in+ten+major+cities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="135">
        <h5 class="caps">Generating Graph Snapshots from Streaming Edge Data.</h5>
        <div class="authors">
            
                <span class="caps">Sucheta Soundarajan</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Elias B. Khalil</span><span>,</span>
            
                <span class="caps">Tina Eliassi-Rad</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Brian Gallagher</span><span>,</span>
            
                <span class="caps">Kevin A. Roundy</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WWW (Companion Volume) 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Generating+Graph+Snapshots+from+Streaming+Edge+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="136">
        <h5 class="caps">Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.</h5>
        <div class="authors">
            
                <span class="caps">Michael A. Madaio</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Oliver L. Haimson</span><span>,</span>
            
                <span class="caps">Wenwen Zhang</span><span>,</span>
            
                <span class="caps">Xiang Cheng</span><span>,</span>
            
                <span class="caps">Matthew Hinds-Aldrich</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Bistra Dilkina</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Computing / Technology Policy </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Geographic Information Systems </span>
            
                <span class="badge badge-secondary caps"> Commerce Policy </span>
            
                <span class="badge badge-secondary caps"> Supervised Learning By Classification </span>
            
                <span class="badge badge-secondary caps"> Data Cleaning </span>
            
                <span class="badge badge-secondary caps"> Governmental Regulations </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Supervised Learning </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Learning Paradigms </span>
            
                <span class="badge badge-secondary caps"> Spatial-temporal Systems </span>
            
                <span class="badge badge-secondary caps"> Visualization Application Domains </span>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Social And Professional Topics </span>
            
                <span class="badge badge-secondary caps"> Information Systems Applications </span>
            
                <span class="badge badge-secondary caps"> Geographic Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Firebird%3A+Predicting+Fire+Risk+and+Prioritizing+Fire+Inspections+in+Atlanta." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="137">
        <h5 class="caps">FairPlay: Fraud and Malware Detection in Google Play.</h5>
        <div class="authors">
            
                <span class="caps">Mahmudur Rahman</span><span>,</span>
            
                <span class="caps">Mizanur Rahman</span><span>,</span>
            
                <span class="caps">Bogdan Carbunar</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SDM 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=FairPlay%3A+Fraud+and+Malware+Detection+in+Google+Play." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="138">
        <h5 class="caps">Expanding Selection for Information Visualization Systems on Tablet Devices.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ISS 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Expanding+Selection+for+Information+Visualization+Systems+on+Tablet+Devices." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="139">
        <h5 class="caps">Designing Multiple Coordinated Visualizations for Tablets.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Computer Graphics </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Designing+Multiple+Coordinated+Visualizations+for+Tablets." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="140">
        <h5 class="caps">Constraint based temporal event sequence mining for Glioblastoma survival prediction.</h5>
        <div class="authors">
            
                <span class="caps">Kunal Malhotra</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Costas Hadjipanayis</span><span>,</span>
            
                <span class="caps">Jimeng Sun</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>J. Biomed. Informatics 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Constraint+based+temporal+event+sequence+mining+for+Glioblastoma+survival+prediction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="141">
        <h5 class="caps">Communication Efficient Distributed Agnostic Boosting.</h5>
        <div class="authors">
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Maria-Florina Balcan</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>AISTATS 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Communication+Efficient+Distributed+Agnostic+Boosting." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="142">
        <h5 class="caps">Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.</h5>
        <div class="authors">
            
                <span class="caps">Eric D. Ragan</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Jibonananda Sanyal</span><span>,</span>
            
                <span class="caps">Jian Chen</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Characterizing+Provenance+in+Visualization+and+Data+Analysis%3A+An+Organizational+Framework+of+Provenance+Types+and+Purposes." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="143">
        <h5 class="caps">Building a research data science platform from industrial machines.</h5>
        <div class="authors">
            
                <span class="caps">Fang (Cherry) Liu</span><span>,</span>
            
                <span class="caps">Fu Shen</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Neil Bright</span><span>,</span>
            
                <span class="caps">Mehmet Belgin</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Building+a+research+data+science+platform+from+industrial+machines." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="144">
        <h5 class="caps">Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Bahador Saket</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Hci Design And Evaluation Methods </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>BELIV 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Beyond+Usability+and+Performance%3A+A+Review+of+User+Experience-focused+Evaluations+in+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="145">
        <h5 class="caps">Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.</h5>
        <div class="authors">
            
                <span class="caps">Nathan Oken Hodas</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Adding+Semantic+Information+into+Data+Models+by+Learning+Domain+Expertise+from+User+Interaction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="146">
        <h5 class="caps">A Computational Model for Dyadic Relationships (Invited Paper).</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IRI 2016</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Computational+Model+for+Dyadic+Relationships+%28Invited+Paper%29." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="147">
        <h5 class="caps">LBSN Data and the Social Butterfly Effect (Vision Paper).</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>LBSN@SIGSPATIAL/GIS 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=LBSN+Data+and+the+Social+Butterfly+Effect+%28Vision+Paper%29." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="148">
        <h5 class="caps">Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.</h5>
        <div class="authors">
            
                <span class="caps">Sheriff Jolaoso</span><span>,</span>
            
                <span class="caps">Russ Burtner</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Sensemaking </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Signature Discovery </span>
            
                <span class="badge badge-secondary caps"> Analytic Process </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>INTERACT (2) 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Toward+a+Deeper+Understanding+of+Data+Analysis%2C+Sensemaking%2C+and+Signature+Discovery." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="149">
        <h5 class="caps">TimeStitch: Interactive multi-focus cohort discovery and comparison.</h5>
        <div class="authors">
            
                <span class="caps">Peter J. Polack Jr.</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Moushumi Sharmin</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Mobile Computing </span>
            
                <span class="badge badge-secondary caps"> Abstinent Smoker </span>
            
                <span class="badge badge-secondary caps"> Event-based Timeline </span>
            
                <span class="badge badge-secondary caps"> Medical Computing </span>
            
                <span class="badge badge-secondary caps"> Congestive Heart Failure </span>
            
                <span class="badge badge-secondary caps"> Medical Services </span>
            
                <span class="badge badge-secondary caps"> Mobile Communication </span>
            
                <span class="badge badge-secondary caps"> Interactive Systems </span>
            
                <span class="badge badge-secondary caps"> Interactive Multifocus Cohort Discovery </span>
            
                <span class="badge badge-secondary caps"> Visualization Technique </span>
            
                <span class="badge badge-secondary caps"> Timestitch </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
                <span class="badge badge-secondary caps"> Health Care </span>
            
                <span class="badge badge-secondary caps"> Healthcare </span>
            
                <span class="badge badge-secondary caps"> Cloning </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Data Visualisation </span>
            
                <span class="badge badge-secondary caps"> Mobile Health Sensor Data </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Mortality Risk </span>
            
                <span class="badge badge-secondary caps"> Heart </span>
            
                <span class="badge badge-secondary caps"> Interactive Technique </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VAST 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TimeStitch%3A+Interactive+multi-focus+cohort+discovery+and+comparison." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="150">
        <h5 class="caps">Spotting Suspicious Reviews via (Quasi-)clique Extraction.</h5>
        <div class="authors">
            
                <span class="caps">Paras Jain</span><span>,</span>
            
                <span class="caps">Shang-Tse Chen</span><span>,</span>
            
                <span class="caps">Mozhgan Azimpourkivi</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Bogdan Carbunar</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Spotting+Suspicious+Reviews+via+%28Quasi-%29clique+Extraction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="151">
        <h5 class="caps">Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Michelle X. Zhou</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Semantic+Interaction%3A+Coupling+Cognition+and+Computation+through+Usable+Interactive+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="152">
        <h5 class="caps">Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Zhiyuan Lin</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Jilles Vreeken</span><span>,</span>
            
                <span class="caps">Partha P. Talukdar</span><span>,</span>
            
                <span class="caps">James Abello</span><span>,</span>
            
                <span class="caps">Ganesh Parameswaran</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Seeing+the+Forest+through+the+Trees%3A+Adaptive+Local+Exploration+of+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="153">
        <h5 class="caps">Scalable graph exploration and visualization: Sensemaking challenges and opportunities.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">James Abello</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>BigComp 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Scalable+graph+exploration+and+visualization%3A+Sensemaking+challenges+and+opportunities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="154">
        <h5 class="caps">Mixed-initiative visual analytics using task-driven recommendations.</h5>
        <div class="authors">
            
                <span class="caps">Kristin A. Cook</span><span>,</span>
            
                <span class="caps">Nick Cramer</span><span>,</span>
            
                <span class="caps">David J. Israel</span><span>,</span>
            
                <span class="caps">Michael Wolverton</span><span>,</span>
            
                <span class="caps">Joe Bruce</span><span>,</span>
            
                <span class="caps">Russ Burtner</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Semantic Interaction </span>
            
                <span class="badge badge-secondary caps"> Analytic Discourse </span>
            
                <span class="badge badge-secondary caps"> User Interactions </span>
            
                <span class="badge badge-secondary caps"> Mixed-initiative Systems </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Myriad Analytic Models </span>
            
                <span class="badge badge-secondary caps"> Active Data Environment </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Visual Analytic Tools </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Mixed-initiative Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Discovery Tasks </span>
            
                <span class="badge badge-secondary caps"> Analytic Process </span>
            
                <span class="badge badge-secondary caps"> Design Guidelines </span>
            
                <span class="badge badge-secondary caps"> Automated Activities </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Sensemaking Tasks </span>
            
                <span class="badge badge-secondary caps"> Task-driven Recommendations </span>
            
                <span class="badge badge-secondary caps"> Iterative Sensemaking </span>
            
                <span class="badge badge-secondary caps"> Data Visualisation </span>
            
                <span class="badge badge-secondary caps"> Spatial Workspace </span>
            
                <span class="badge badge-secondary caps"> Recommender Systems </span>
            
                <span class="badge badge-secondary caps"> Interactive Visual Interfaces </span>
            
                <span class="badge badge-secondary caps"> Task Recommendations </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Ade Prototype </span>
            
                <span class="badge badge-secondary caps"> Visual Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Usability </span>
            
                <span class="badge badge-secondary caps"> Task Model </span>
            
                <span class="badge badge-secondary caps"> Analytic Sensemaking </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
                <span class="badge badge-secondary caps"> Human Activities </span>
            
                <span class="badge badge-secondary caps"> Cognitive Actions </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>VAST 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mixed-initiative+visual+analytics+using+task-driven+recommendations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="155">
        <h5 class="caps">M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.</h5>
        <div class="authors">
            
                <span class="caps">Hugo Gualdron</span><span>,</span>
            
                <span class="caps">Robson L. F. Cordeiro</span><span>,</span>
            
                <span class="caps">Jos</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">U Kang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=M-Flash%3A+Fast+Billion-scale+Graph+Computation+Using+Block+Partition+Model." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="156">
        <h5 class="caps">Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.</h5>
        <div class="authors">
            
                <span class="caps">Yi Han</span><span>,</span>
            
                <span class="caps">Agata Rozga</span><span>,</span>
            
                <span class="caps">Nevena Dimitrova</span><span>,</span>
            
                <span class="caps">Gregory D. Abowd</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> H.5.m. </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analysis+of+Proximal+Temporal+Relationships+of+Social+and+Communicative+Behaviors." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="157">
        <h5 class="caps">IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Christopher Collins</span><span>,</span>
            
                <span class="caps">Wenwen Dou</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IUI 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=IUI-TextVis+2015%3A+Fourth+Workshop+on+Interactive+Visual+Text+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="158">
        <h5 class="caps">Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IUI Companion 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactive+Querying+over+Large+Network+Data%3A+Scalability%2C+Visualization%2C+and+Interaction+Design." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="159">
        <h5 class="caps">Identifying Successful Investors in the Startup Ecosystem.</h5>
        <div class="authors">
            
                <span class="caps">Srishti Gupta</span><span>,</span>
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Rahul C. Basole</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WWW (Companion Volume) 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Identifying+Successful+Investors+in+the+Startup+Ecosystem." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="160">
        <h5 class="caps">Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Zoe Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>MDM (2) 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Exploring+Institution-Based+Mobility%3A+Which+Universities+Attract+Athletes+from+Distant+and+Diverse+Locales%3F" target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="161">
        <h5 class="caps">explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.</h5>
        <div class="authors">
            
                <span class="caps">Robert Chen</span><span>,</span>
            
                <span class="caps">Vikas Kumar</span><span>,</span>
            
                <span class="caps">Natalie Fitch</span><span>,</span>
            
                <span class="caps">Jitesh Jagadish</span><span>,</span>
            
                <span class="caps">Lifan Zhang</span><span>,</span>
            
                <span class="caps">William Dunn</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EMBC 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=explICU%3A+A+web-based+visualization+and+predictive+modeling+toolkit+for+mortality+in+intensive+care+patients." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="162">
        <h5 class="caps">Characterizing Smoking and Drinking Abstinence from Social Media.</h5>
        <div class="authors">
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Munmun De Choudhury</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>HT 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Characterizing+Smoking+and+Drinking+Abstinence+from+Social+Media." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="163">
        <h5 class="caps">Center of excellence for mobile sensor data-to-knowledge (MD2K).</h5>
        <div class="authors">
            
                <span class="caps">Santosh Kumar</span><span>,</span>
            
                <span class="caps">Gregory D. Abowd</span><span>,</span>
            
                <span class="caps">William T. Abraham</span><span>,</span>
            
                <span class="caps">Mustafa al'Absi</span><span>,</span>
            
                <span class="caps">J. Gayle Beck</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Tyson Condie</span><span>,</span>
            
                <span class="caps">David E. Conroy</span><span>,</span>
            
                <span class="caps">Emre Ertin</span><span>,</span>
            
                <span class="caps">Deborah Estrin</span><span>,</span>
            
                <span class="caps">Deepak Ganesan</span><span>,</span>
            
                <span class="caps">Cho Lam</span><span>,</span>
            
                <span class="caps">Benjamin M. Marlin</span><span>,</span>
            
                <span class="caps">Clay B. Marsh</span><span>,</span>
            
                <span class="caps">Susan A. Murphy</span><span>,</span>
            
                <span class="caps">Inbal Nahum-Shani</span><span>,</span>
            
                <span class="caps">Kevin Patrick</span><span>,</span>
            
                <span class="caps">James M. Rehg</span><span>,</span>
            
                <span class="caps">Moushumi Sharmin</span><span>,</span>
            
                <span class="caps">Vivek Shetty</span><span>,</span>
            
                <span class="caps">Ida Sim</span><span>,</span>
            
                <span class="caps">Bonnie Spring</span><span>,</span>
            
                <span class="caps">Mani B. Srivastava</span><span>,</span>
            
                <span class="caps">David W. Wetter</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>J. Am. Medical Informatics Assoc. 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Center+of+excellence+for+mobile+sensor+data-to-knowledge+%28MD2K%29." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="164">
        <h5 class="caps">A visual analytics approach to understanding care process variation and conformance.</h5>
        <div class="authors">
            
                <span class="caps">Rahul C. Basole</span><span>,</span>
            
                <span class="caps">Hyunwoo Park</span><span>,</span>
            
                <span class="caps">Mayank Gupta</span><span>,</span>
            
                <span class="caps">Mark L. Braunstein</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Michael Thompson</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VAHC 2015</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+visual+analytics+approach+to+understanding+care+process+variation+and+conformance." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="165">
        <h5 class="caps">Value-driven evaluation of visualizations.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Hci Design And Evaluation Methods </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>BELIV 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Value-driven+evaluation+of+visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="166">
        <h5 class="caps">VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.</h5>
        <div class="authors">
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Changhyun Lee</span><span>,</span>
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Ramakrishnan Kannan</span><span>,</span>
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Barry L. Drake</span><span>,</span>
            
                <span class="caps">Haesun Park</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Alzheimer's Disease </span>
            
                <span class="badge badge-secondary caps"> Support Vector Machines </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Information Retrieval </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VisIRR%3A+Visual+analytics+for+information+retrieval+and+recommendation+with+large-scale+document+data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="167">
        <h5 class="caps">User-centered design guidelines for collaborative software for intelligence analysis.</h5>
        <div class="authors">
            
                <span class="caps">Jean Scholtz</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CTS 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=User-centered+design+guidelines+for+collaborative+software+for+intelligence+analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="168">
        <h5 class="caps">Towards scalable graph computation on mobile devices.</h5>
        <div class="authors">
            
                <span class="caps">Yiqi Chen</span><span>,</span>
            
                <span class="caps">Zhiyuan Lin</span><span>,</span>
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Towards+scalable+graph+computation+on+mobile+devices." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="169">
        <h5 class="caps">The human is the loop: new directions for visual analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Mahmud Shahriar Hossain</span><span>,</span>
            
                <span class="caps">Naren Ramakrishnan</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Patrick Fiaux</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>J. Intell. Inf. Syst. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+human+is+the+loop%3A+new+directions+for+visual+analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="170">
        <h5 class="caps">SharkFin: Spatio-temporal mining of software adoption and penetration.</h5>
        <div class="authors">
            
                <span class="caps">Evangelos E. Papalexakis</span><span>,</span>
            
                <span class="caps">Tudor Dumitras</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">B. Aditya Prakash</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Soc. Netw. Anal. Min. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SharkFin%3A+Spatio-temporal+mining+of+software+adoption+and+penetration." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="171">
        <h5 class="caps">Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Semantic+Interaction+for+Visual+Analytics%3A+Toward+Coupling+Cognition+and+Computation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="172">
        <h5 class="caps">Reflections on the evolution of the Jigsaw visual analytics system.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Reflections+on+the+evolution+of+the+Jigsaw+visual+analytics+system." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="173">
        <h5 class="caps">Ploceus: Modeling, visualizing, and analyzing tabular data as networks.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Ploceus%3A+Modeling%2C+visualizing%2C+and+analyzing+tabular+data+as+networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="174">
        <h5 class="caps">OnSet: A Visualization Technique for Large-scale Binary Set Data.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">Timothy Major</span><span>,</span>
            
                <span class="caps">Alistair D. M. Dove</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Large-scale Systems </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Complexity Theory </span>
            
                <span class="badge badge-secondary caps"> Image Color Analysis </span>
            
                <span class="badge badge-secondary caps"> Nonhomogeneous Media </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=OnSet%3A+A+Visualization+Technique+for+Large-scale+Binary+Set+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="175">
        <h5 class="caps">MAGE: Matching approximate patterns in richly-attributed graphs.</h5>
        <div class="authors">
            
                <span class="caps">Robert Pienta</span><span>,</span>
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=MAGE%3A+Matching+approximate+patterns+in+richly-attributed+graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="176">
        <h5 class="caps">Large-scale insider trading analysis: patterns and discoveries.</h5>
        <div class="authors">
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Elias B. Khalil</span><span>,</span>
            
                <span class="caps">Bo Xie</span><span>,</span>
            
                <span class="caps">Stephen L. Lenkey</span><span>,</span>
            
                <span class="caps">Bryan R. Routledge</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Soc. Netw. Anal. Min. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Large-scale+insider+trading+analysis%3A+patterns+and+discoveries." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="177">
        <h5 class="caps">Guilt by association: large scale malware detection by mining file-relation graphs.</h5>
        <div class="authors">
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Kevin A. Roundy</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Security And Privacy </span>
            
                <span class="badge badge-secondary caps"> Information Systems Applications </span>
            
                <span class="badge badge-secondary caps"> Systems Security </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Operating Systems Security </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Guilt+by+association%3A+large+scale+malware+detection+by+mining+file-relation+graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="178">
        <h5 class="caps">GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.</h5>
        <div class="authors">
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Zhiyuan Lin</span><span>,</span>
            
                <span class="caps">Florian Foerster</span><span>,</span>
            
                <span class="caps">Aakash Goel</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Interactive Systems </span>
            
                <span class="badge badge-secondary caps"> Aggregates </span>
            
                <span class="badge badge-secondary caps"> Graph Theory </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GLO-STIX%3A+Graph-Level+Operations+for+Specifying+Techniques+and+Interactive+eXploration." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="179">
        <h5 class="caps">GLOs: graph-level operations for exploratory network visualization.</h5>
        <div class="authors">
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">Florian Foerster</span><span>,</span>
            
                <span class="caps">Minsuk Kahng</span><span>,</span>
            
                <span class="caps">Zhiyuan Lin</span><span>,</span>
            
                <span class="caps">Aakash Goel</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GLOs%3A+graph-level+operations+for+exploratory+network+visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="180">
        <h5 class="caps">Future directions of humans in Big Data Research: Summary of the 1</h5>
        <div class="authors">
            
                <span class="caps">Celeste Lyn Paul</span><span>,</span>
            
                <span class="caps">Chris Argenta</span><span>,</span>
            
                <span class="caps">William C. Elm</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Future+directions+of+humans+in+Big+Data+Research%3A+Summary+of+the+1" target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="181">
        <h5 class="caps">Finding Waldo: Learning about Users from their Interactions.</h5>
        <div class="authors">
            
                <span class="caps">Eli T. Brown</span><span>,</span>
            
                <span class="caps">Alvitta Ottley</span><span>,</span>
            
                <span class="caps">Helen Zhao</span><span>,</span>
            
                <span class="caps">Quan Lin</span><span>,</span>
            
                <span class="caps">Richard Souvenir</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Remco Chang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Finding+Waldo%3A+Learning+about+Users+from+their+Interactions." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="182">
        <h5 class="caps">Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.</h5>
        <div class="authors">
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Yi Han</span><span>,</span>
            
                <span class="caps">Mengdie Hu</span><span>,</span>
            
                <span class="caps">Hannah Kim</span><span>,</span>
            
                <span class="caps">James Nugent</span><span>,</span>
            
                <span class="caps">Francesco Poggi</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Matlab </span>
            
                <span class="badge badge-secondary caps"> Organizations </span>
            
                <span class="badge badge-secondary caps"> Global Positioning System </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Exploring+anomalies+in+GAStech%3A+VAST+2014+Mini+Challenge+1+and+2." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="183">
        <h5 class="caps">Designing and implementing an interactive scatterplot visualization for a tablet computer.</h5>
        <div class="authors">
            
                <span class="caps">Ramik Sadana</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Designing+and+implementing+an+interactive+scatterplot+visualization+for+a+tablet+computer." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="184">
        <h5 class="caps">Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Characterizing+the+intelligence+analysis+process+through+a+longitudinal+field+study%3A+Implications+for+visual+analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="185">
        <h5 class="caps">Big graph mining for the web and social media: algorithms, anomaly detection, and applications.</h5>
        <div class="authors">
            
                <span class="caps">U Kang</span><span>,</span>
            
                <span class="caps">Leman Akoglu</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WSDM 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Big+graph+mining+for+the+web+and+social+media%3A+algorithms%2C+anomaly+detection%2C+and+applications." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="186">
        <h5 class="caps">7 key challenges for visualization in cyber network defense.</h5>
        <div class="authors">
            
                <span class="caps">Daniel M. Best</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Daniel Kidwell</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VizSEC 2014</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=7+key+challenges+for+visualization+in+cyber+network+defense." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="187">
        <h5 class="caps">Mining Connection Pathways for Marked Nodes in Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Leman Akoglu</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Nikolaj Tatti</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Jilles Vreeken</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SDM 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mining+Connection+Pathways+for+Marked+Nodes+in+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="188">
        <h5 class="caps">Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Rahul C. Basole</span><span>,</span>
            
                <span class="caps">Trustin Clear</span><span>,</span>
            
                <span class="caps">Mengdie Hu</span><span>,</span>
            
                <span class="caps">Harshit Mehrotra</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Understanding+Interfirm+Relationships+in+Business+Ecosystems+with+Interactive+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="189">
        <h5 class="caps">Typograph: Multiscale spatial exploration of text documents.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Russ Burtner</span><span>,</span>
            
                <span class="caps">Nick Cramer</span><span>,</span>
            
                <span class="caps">Ralph Perko</span><span>,</span>
            
                <span class="caps">Shawn D. Hampton</span><span>,</span>
            
                <span class="caps">Kristin A. Cook</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE BigData 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Typograph%3A+Multiscale+spatial+exploration+of+text+documents." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="190">
        <h5 class="caps">Tasks for Multivariate Network Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Johannes Pretorius</span><span>,</span>
            
                <span class="caps">Helen C. Purchase</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Multivariate Network Visualization 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Tasks+for+Multivariate+Network+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="191">
        <h5 class="caps">Support Vector Machine for Spatial Variation.</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">David Cowen</span><span>,</span>
            
                <span class="caps">Jason D. Wittenbach</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Trans. GIS 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Support+Vector+Machine+for+Spatial+Variation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="192">
        <h5 class="caps">POWERWALL: int. workshop on interactive, ultra-high-resolution displays.</h5>
        <div class="authors">
            
                <span class="caps">Chris Rooney</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Jean-Daniel Fekete</span><span>,</span>
            
                <span class="caps">Kasper Hornb</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=POWERWALL%3A+int.+workshop+on+interactive%2C+ultra-high-resolution+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="193">
        <h5 class="caps">MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.</h5>
        <div class="authors">
            
                <span class="caps">Michael J. Henry</span><span>,</span>
            
                <span class="caps">Shawn D. Hampton</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Ian Roberts</span><span>,</span>
            
                <span class="caps">Deborah Payne</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ISM 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=MultiFacet%3A+A+Faceted+Interface+for+Browsing+Large+Multimedia+Collections." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="194">
        <h5 class="caps">Visual Analytics Support for Intelligence Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Computer 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analytics+Support+for+Intelligence+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="195">
        <h5 class="caps">Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.</h5>
        <div class="authors">
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Kristen Koch</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Int. J. Hum. Comput. Stud. 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Large+high+resolution+displays+for+co-located+collaborative+sensemaking%3A+Display+usage+and+territoriality." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="196">
        <h5 class="caps">Inside insider trading: patterns  discoveries from a large scale exploratory analysis.</h5>
        <div class="authors">
            
                <span class="caps">Acar Tamersoy</span><span>,</span>
            
                <span class="caps">Bo Xie</span><span>,</span>
            
                <span class="caps">Stephen L. Lenkey</span><span>,</span>
            
                <span class="caps">Bryan R. Routledge</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ASONAM 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Inside+insider+trading%3A+patterns++discoveries+from+a+large+scale+exploratory+analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="197">
        <h5 class="caps">How analysts cognitively "connect the dots".</h5>
        <div class="authors">
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Jessica Zeitz Self</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Mahmud Shahriar Hossain</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Naren Ramakrishnan</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ISI 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=How+analysts+cognitively+%22connect+the+dots%22." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="198">
        <h5 class="caps">Detecting insider threats in a real corporate database of computer usage activity.</h5>
        <div class="authors">
            
                <span class="caps">Ted E. Senator</span><span>,</span>
            
                <span class="caps">Henry G. Goldberg</span><span>,</span>
            
                <span class="caps">Alex Memory</span><span>,</span>
            
                <span class="caps">William T. Young</span><span>,</span>
            
                <span class="caps">Brad Rees</span><span>,</span>
            
                <span class="caps">Robert Pierce</span><span>,</span>
            
                <span class="caps">Daniel Huang</span><span>,</span>
            
                <span class="caps">Matthew Reardon</span><span>,</span>
            
                <span class="caps">David A. Bader</span><span>,</span>
            
                <span class="caps">Edmond Chow</span><span>,</span>
            
                <span class="caps">Irfan A. Essa</span><span>,</span>
            
                <span class="caps">Joshua Jones</span><span>,</span>
            
                <span class="caps">Vinay Bettadapura</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Oded Green</span><span>,</span>
            
                <span class="caps">Oguz Kaya</span><span>,</span>
            
                <span class="caps">Anita Zakrzewska</span><span>,</span>
            
                <span class="caps">Erica Briscoe</span><span>,</span>
            
                <span class="caps">Rudolph L. Mappus IV</span><span>,</span>
            
                <span class="caps">Robert McColl</span><span>,</span>
            
                <span class="caps">Lora Weiss</span><span>,</span>
            
                <span class="caps">Thomas G. Dietterich</span><span>,</span>
            
                <span class="caps">Alan Fern</span><span>,</span>
            
                <span class="caps">Weng-Keen Wong</span><span>,</span>
            
                <span class="caps">Shubhomoy Das</span><span>,</span>
            
                <span class="caps">Andrew Emmott</span><span>,</span>
            
                <span class="caps">Jed Irvine</span><span>,</span>
            
                <span class="caps">Jay Yoon Lee</span><span>,</span>
            
                <span class="caps">Danai Koutra</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Daniel D. Corkill</span><span>,</span>
            
                <span class="caps">Lisa Friedland</span><span>,</span>
            
                <span class="caps">Amanda Gentzel</span><span>,</span>
            
                <span class="caps">David D. Jensen</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Information Systems Applications </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Detecting+insider+threats+in+a+real+corporate+database+of+computer+usage+activity." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="199">
        <h5 class="caps">Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Jaeyeon Kihm</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Algorithm Design And Analysis </span>
            
                <span class="badge badge-secondary caps"> Tag Clouds </span>
            
                <span class="badge badge-secondary caps"> Text Analysis </span>
            
                <span class="badge badge-secondary caps"> Measurement </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Combining+Computational+Analyses+and+Interactive+Visualization+for+Document+Exploration+and+Sensemaking+in+Jigsaw." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="200">
        <h5 class="caps">Bixplorer: Visual Analytics with Biclusters.</h5>
        <div class="authors">
            
                <span class="caps">Patrick Fiaux</span><span>,</span>
            
                <span class="caps">Maoyuan Sun</span><span>,</span>
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Naren Ramakrishnan</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Computer 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Bixplorer%3A+Visual+Analytics+with+Biclusters." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="201">
        <h5 class="caps">Beyond Control Panels: Direct Manipulation for Visual Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Mathematical Model </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Beyond+Control+Panels%3A+Direct+Manipulation+for+Visual+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="202">
        <h5 class="caps">An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.</h5>
        <div class="authors">
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Haesun Park</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Visualization and Data Analysis 2013</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=An+interactive+visual+testbed+system+for+dimension+reduction+and+clustering+of+large-scale+high-dimensional+data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="203">
        <h5 class="caps">TopicViz: interactive topic exploration in document collections.</h5>
        <div class="authors">
            
                <span class="caps">Jacob Eisenstein</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Aniket Kittur</span><span>,</span>
            
                <span class="caps">Eric P. Xing</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TopicViz%3A+interactive+topic+exploration+in+document+collections." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="204">
        <h5 class="caps">TourViz: interactive visualization of connection pathways in large graphs.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Leman Akoglu</span><span>,</span>
            
                <span class="caps">Jilles Vreeken</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Information Retrieval </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=TourViz%3A+interactive+visualization+of+connection+pathways+in+large+graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="205">
        <h5 class="caps">Visual Analytics for Converging-Business-Ecosystem Intelligence.</h5>
        <div class="authors">
            
                <span class="caps">Rahul C. Basole</span><span>,</span>
            
                <span class="caps">Mengdie Hu</span><span>,</span>
            
                <span class="caps">Pritesh Patel</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Ecosystems </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Interactive Systems </span>
            
                <span class="badge badge-secondary caps"> Computational Intelligence </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Business </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analytics+for+Converging-Business-Ecosystem+Intelligence." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="206">
        <h5 class="caps">The semantics of clustering: analysis of user-generated spatializations of text documents.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Seth Fox</span><span>,</span>
            
                <span class="caps">Dipayan Maiti</span><span>,</span>
            
                <span class="caps">Scotland Leman</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+semantics+of+clustering%3A+analysis+of+user-generated+spatializations+of+text+documents." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="207">
        <h5 class="caps">The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.</h5>
        <div class="authors">
            
                <span class="caps">Sharon Lynn Chu Yew Yee</span><span>,</span>
            
                <span class="caps">Francis K. H. Quek</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Haeyong Chung</span><span>,</span>
            
                <span class="caps">Blake Sawyer</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICALT 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+Physicality+of+Technological+Devices+in+Education%3A+Building+a+Digital+Experience+for+Learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="208">
        <h5 class="caps">The Parallel Coordinates Matrix.</h5>
        <div class="authors">
            
                <span class="caps">Julian Heinrich</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Daniel Weiskopf</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EuroVis (Short Papers) 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+Parallel+Coordinates+Matrix." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="209">
        <h5 class="caps">Supporting asynchronous collaboration in visual analytics systems.</h5>
        <div class="authors">
            
                <span class="caps">Nathalie Henry Riche</span><span>,</span>
            
                <span class="caps">Kori Inkpen</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Tom Gross</span><span>,</span>
            
                <span class="caps">Mary Czerwinski</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Collaborative And Social Computing </span>
            
                <span class="badge badge-secondary caps"> Collaborative And Social Computing Systems And Tools </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Asynchronous Editors </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Supporting+asynchronous+collaboration+in+visual+analytics+systems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="210">
        <h5 class="caps">SnapShot: Visualization to Propel Ice Hockey Analytics.</h5>
        <div class="authors">
            
                <span class="caps">Hannah Pileggi</span><span>,</span>
            
                <span class="caps">Charles D. Stolper</span><span>,</span>
            
                <span class="caps">J. Michael Boyle</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Sports Equipment </span>
            
                <span class="badge badge-secondary caps"> Games </span>
            
                <span class="badge badge-secondary caps"> Knowledge Discovery </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SnapShot%3A+Visualization+to+Propel+Ice+Hockey+Analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="211">
        <h5 class="caps">Semantic interaction for visual text analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Patrick Fiaux</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Semantic+interaction+for+visual+text+analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="212">
        <h5 class="caps">Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Patrick Fiaux</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Mathematical Model </span>
            
                <span class="badge badge-secondary caps"> User Interfaces </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Semantic+Interaction+for+Sensemaking%3A+Inferring+Analytical+Reasoning+for+Model+Steering." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="213">
        <h5 class="caps">Pegasus: Mining billion-scale graphs in the cloud.</h5>
        <div class="authors">
            
                <span class="caps">U Kang</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICASSP 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Pegasus%3A+Mining+billion-scale+graphs+in+the+cloud." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="214">
        <h5 class="caps">OPAvion: mining and visualization in large graphs.</h5>
        <div class="authors">
            
                <span class="caps">Leman Akoglu</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">U Kang</span><span>,</span>
            
                <span class="caps">Danai Koutra</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Information Systems Applications </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>SIGMOD Conference 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=OPAvion%3A+mining+and+visualization+in+large+graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="215">
        <h5 class="caps">iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.</h5>
        <div class="authors">
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Jaeyeon Kihm</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Haesun Park</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> H.1.2 </span>
            
                <span class="badge badge-secondary caps"> H.2.8 </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Comput. Graph. Forum 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=iVisClustering%3A+An+Interactive+Visual+Document+Clustering+via+Topic+Modeling." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="216">
        <h5 class="caps">Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Leman Akoglu</span><span>,</span>
            
                <span class="caps">Jilles Vreeken</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ASONAM 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interactively+and+Visually+Exploring+Tours+of+Marked+Nodes+in+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="217">
        <h5 class="caps">Interaction junk: user interaction-based evaluation of visual analytic systems.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>BELIV 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Interaction+junk%3A+user+interaction-based+evaluation+of+visual+analytic+systems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="218">
        <h5 class="caps">How spatial layout, interactivity, and persistent visibility affect learning with large displays.</h5>
        <div class="authors">
            
                <span class="caps">Eric D. Ragan</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Doug A. Bowman</span><span>,</span>
            
                <span class="caps">Francis K. H. Quek</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Graphical User Interfaces </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Paradigms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=How+spatial+layout%2C+interactivity%2C+and+persistent+visibility+affect+learning+with+large+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="219">
        <h5 class="caps">Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Market Research </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Qualitative Analysis </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Examining+the+Use+of+a+Visual+Analytics+System+for+Sensemaking+Tasks%3A+Case+Studies+with+Domain+Experts." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="220">
        <h5 class="caps">Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.</h5>
        <div class="authors">
            
                <span class="caps">Yong Cao</span><span>,</span>
            
                <span class="caps">Reese Moore</span><span>,</span>
            
                <span class="caps">Peng Mi</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Randy C. Marchany</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Dynamic+analysis+of+large+datasets+with+animated+and+correlated+views%3A+VAST+2012+Mini+Challenge+%23+award%3A+Honorable+mention+for+good+use+of+coordinated+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="221">
        <h5 class="caps">Discovering Spatial Patterns in Origin-Destination Mobility Data.</h5>
        <div class="authors">
            
                <span class="caps">Diansheng Guo</span><span>,</span>
            
                <span class="caps">Xi Zhu</span><span>,</span>
            
                <span class="caps">Hai Jin</span><span>,</span>
            
                <span class="caps">Peng Gao</span><span>,</span>
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Trans. GIS 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Discovering+Spatial+Patterns+in+Origin-Destination+Mobility+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="222">
        <h5 class="caps">Designing large high-resolution display workspaces.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Jessica Zeitz</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>AVI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Designing+large+high-resolution+display+workspaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="223">
        <h5 class="caps">Breaking news on twitter.</h5>
        <div class="authors">
            
                <span class="caps">Mengdie Hu</span><span>,</span>
            
                <span class="caps">Shixia Liu</span><span>,</span>
            
                <span class="caps">Furu Wei</span><span>,</span>
            
                <span class="caps">Yingcai Wu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Kwan-Liu Ma</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2012</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Breaking+news+on+twitter." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="224">
        <h5 class="caps">Observation-level interaction with statistical models for visual analytics.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Chao Han</span><span>,</span>
            
                <span class="caps">Dipayan Maiti</span><span>,</span>
            
                <span class="caps">Leanna House</span><span>,</span>
            
                <span class="caps">Scotland Leman</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Layout </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Data Models </span>
            
                <span class="badge badge-secondary caps"> Principal Component Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Observation-level+interaction+with+statistical+models+for+visual+analytics." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="225">
        <h5 class="caps">Visual encodings that support physical navigation on large displays.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Yueh-Hua Lee</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+encodings+that+support+physical+navigation+on+large+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="226">
        <h5 class="caps">Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.</h5>
        <div class="authors">
            
                <span class="caps">Danai Koutra</span><span>,</span>
            
                <span class="caps">Tai-You Ke</span><span>,</span>
            
                <span class="caps">U Kang</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Hsing-Kuo Kenneth Pao</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ECML/PKDD (2) 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Unifying+Guilt-by-Association+Approaches%3A+Theorems+and+Fast+Algorithms." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="227">
        <h5 class="caps">The effects of spatial layout and view control on cognitive processing.</h5>
        <div class="authors">
            
                <span class="caps">Eric D. Ragan</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Doug A. Bowman</span><span>,</span>
            
                <span class="caps">Francis K. H. Quek</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+effects+of+spatial+layout+and+view+control+on+cognitive+processing." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="228">
        <h5 class="caps">Supporting the cyber analytic process using visual history on large displays.</h5>
        <div class="authors">
            
                <span class="caps">Ankit Singh</span><span>,</span>
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Robert Kincaid</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VizSEC 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Supporting+the+cyber+analytic+process+using+visual+history+on+large+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="229">
        <h5 class="caps">Predicting migration system dynamics with conditional and posterior probabilities.</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Samuel Halverson</span><span>,</span>
            
                <span class="caps">Frank Hardisty</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICSDM 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Predicting+migration+system+dynamics+with+conditional+and+posterior+probabilities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="230">
        <h5 class="caps">Weighted Radial Variation for Node Feature Classification</h5>
        <div class="authors">
            
                <span class="caps">Clio Andris</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>arXiv 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Weighted+Radial+Variation+for+Node+Feature+Classification" target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="231">
        <h5 class="caps">Network-based visual analysis of tabular data.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Shamkant B. Navathe</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Semantics </span>
            
                <span class="badge badge-secondary caps"> Organizations </span>
            
                <span class="badge badge-secondary caps"> Relational Databases </span>
            
                <span class="badge badge-secondary caps"> Aggregates </span>
            
                <span class="badge badge-secondary caps"> Cities And Towns </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Network-based+visual+analysis+of+tabular+data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="232">
        <h5 class="caps">Mining large graphs: Algorithms, inference, and discoveries.</h5>
        <div class="authors">
            
                <span class="caps">U Kang</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICDE 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mining+large+graphs%3A+Algorithms%2C+inference%2C+and+discoveries." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="233">
        <h5 class="caps">Large Scale Graph Mining and Inference for Malware Detection.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Carey Nachenberg</span><span>,</span>
            
                <span class="caps">Jeffrey Wilhelm</span><span>,</span>
            
                <span class="caps">Adam Wright</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SDM 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Large+Scale+Graph+Mining+and+Inference+for+Malware+Detection." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="234">
        <h5 class="caps">Jigsaw to save vastopolis.</h5>
        <div class="authors">
            
                <span class="caps">Elizabeth Braunstein</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Text Analysis </span>
            
                <span class="badge badge-secondary caps"> Organizations </span>
            
                <span class="badge badge-secondary caps"> Manuals </span>
            
                <span class="badge badge-secondary caps"> Information Retrieval </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Jigsaw+to+save+vastopolis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="235">
        <h5 class="caps">Information visualization: State of the field and new research directions.</h5>
        <div class="authors">
            
                <span class="caps">Andreas Kerren</span><span>,</span>
            
                <span class="caps">Catherine Plaisant</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Information+visualization%3A+State+of+the+field+and+new+research+directions." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="236">
        <h5 class="caps">Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Beth Yost</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Information+visualization+on+large%2C+high-resolution+displays%3A+Issues%2C+challenges%2C+and+opportunities." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="237">
        <h5 class="caps">How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Control Systems </span>
            
                <span class="badge badge-secondary caps"> Computational Intelligence </span>
            
                <span class="badge badge-secondary caps"> Availability </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Performance Analysis </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=How+Can+Visual+Analytics+Assist+Investigative+Analysis%3F+Design+Implications+from+an+Evaluation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="238">
        <h5 class="caps">Graph Analytics-Lessons Learned and Challenges Ahead.</h5>
        <div class="authors">
            
                <span class="caps">Pak Chung Wong</span><span>,</span>
            
                <span class="caps">Chaomei Chen</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Ben Shneiderman</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Jim Thomas</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Power Grids </span>
            
                <span class="badge badge-secondary caps"> Green Products </span>
            
                <span class="badge badge-secondary caps"> Tools </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Graph+Analytics-Lessons+Learned+and+Challenges+Ahead." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="239">
        <h5 class="caps">Exploring Complex Mobile Life through Lightweight Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Tanyoung Kim</span><span>,</span>
            
                <span class="caps">Jan Blom</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>EuroVA@EuroVis 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Exploring+Complex+Mobile+Life+through+Lightweight+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="240">
        <h5 class="caps">Evaluating video visualizations of human behavior.</h5>
        <div class="authors">
            
                <span class="caps">Mario Romero</span><span>,</span>
            
                <span class="caps">Alice Vialard</span><span>,</span>
            
                <span class="caps">John Peponis</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Gregory D. Abowd</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluating+video+visualizations+of+human+behavior." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="241">
        <h5 class="caps">Analytic provenance: process+interaction+insight.</h5>
        <div class="authors">
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Wenwen Dou</span><span>,</span>
            
                <span class="caps">Richard May</span><span>,</span>
            
                <span class="caps">Bill Pike</span><span>,</span>
            
                <span class="caps">Glenn A. Fink</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Analytic+provenance%3A+process%2Binteraction%2Binsight." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="242">
        <h5 class="caps">Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Information Services </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Electronic Publishing </span>
            
                <span class="badge badge-secondary caps"> Collaboration </span>
            
                <span class="badge badge-secondary caps"> Analytical Models </span>
            
                <span class="badge badge-secondary caps"> Internet </span>
            
                <span class="badge badge-secondary caps"> Artificial Intelligence </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Characterizing+the+intelligence+analysis+process%3A+Informing+visual+analytics+design+through+a+longitudinal+field+study." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="243">
        <h5 class="caps">ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Patrick Fiaux</span><span>,</span>
            
                <span class="caps">Haeyong Chung</span><span>,</span>
            
                <span class="caps">Michael Stewart</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=ChairMouse%3A+leveraging+natural+chair+rotation+for+cursor+navigation+on+large%2C+high-resolution+displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="244">
        <h5 class="caps">Apolo: making sense of large network data by combining rich user interaction and machine learning.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Aniket Kittur</span><span>,</span>
            
                <span class="caps">Jason I. Hong</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Network Services </span>
            
                <span class="badge badge-secondary caps"> Network Monitoring </span>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Machine Learning </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Networks </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Apolo%3A+making+sense+of+large+network+data+by+combining+rich+user+interaction+and+machine+learning." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="245">
        <h5 class="caps">Apolo: interactive large graph sensemaking by combining machine learning and visualization.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Aniket Kittur</span><span>,</span>
            
                <span class="caps">Jason I. Hong</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Evaluation Of Retrieval Results </span>
            
                <span class="badge badge-secondary caps"> Relevance Assessment </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Information Retrieval </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>KDD 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Apolo%3A+interactive+large+graph+sensemaking+by+combining+machine+learning+and+visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="246">
        <h5 class="caps">Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.</h5>
        <div class="authors">
            
                <span class="caps">Katherine Vogt</span><span>,</span>
            
                <span class="caps">Lauren Bradel</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Duke Hutchings</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Sensemaking </span>
            
                <span class="badge badge-secondary caps"> Co-located </span>
            
                <span class="badge badge-secondary caps"> Large High-resolution Display </span>
            
                <span class="badge badge-secondary caps"> Cscw </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>INTERACT (2) 2011</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Co-located+Collaborative+Sensemaking+on+a+Large+High-Resolution+Display+with+Multiple+Input+Devices." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="247">
        <h5 class="caps">Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Hannah J. Tipney</span><span>,</span>
            
                <span class="caps">Karin Verspoor</span><span>,</span>
            
                <span class="caps">William A. Baumgartner Jr.</span><span>,</span>
            
                <span class="caps">K. Bretonnel Cohen</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Lawrence Hunter</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>KES (4) 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualization+and+Language+Processing+for+Supporting+Analysis+across+the+Biomedical+Literature." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="248">
        <h5 class="caps">Uncovering cabdrivers' behavior patterns from their digital traces.</h5>
        <div class="authors">
            
                <span class="caps">Liang Liu</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Carlo Ratti</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Comput. Environ. Urban Syst. 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Uncovering+cabdrivers%27+behavior+patterns+from+their+digital+traces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="249">
        <h5 class="caps">Towards efficient collaboration in cyber security.</h5>
        <div class="authors">
            
                <span class="caps">Peter Hui</span><span>,</span>
            
                <span class="caps">Joe Bruce</span><span>,</span>
            
                <span class="caps">Glenn A. Fink</span><span>,</span>
            
                <span class="caps">Michelle L. Gregory</span><span>,</span>
            
                <span class="caps">Daniel M. Best</span><span>,</span>
            
                <span class="caps">Liam McGrath</span><span>,</span>
            
                <span class="caps">Alex Endert</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CTS 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Towards+efficient+collaboration+in+cyber+security." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="250">
        <h5 class="caps">Space to think: large high-resolution displays for sensemaking.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Space+to+think%3A+large+high-resolution+displays+for+sensemaking." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="251">
        <h5 class="caps">On the Vulnerability of Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">B. Aditya Prakash</span><span>,</span>
            
                <span class="caps">Charalampos E. Tsourakakis</span><span>,</span>
            
                <span class="caps">Tina Eliassi-Rad</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Eigenvalues And Eigenfunctions </span>
            
                <span class="badge badge-secondary caps"> Correlation </span>
            
                <span class="badge badge-secondary caps"> Robustness </span>
            
                <span class="badge badge-secondary caps"> Scalability </span>
            
                <span class="badge badge-secondary caps"> Computer Networks </span>
            
                <span class="badge badge-secondary caps"> Immune System </span>
            
                <span class="badge badge-secondary caps"> Approximation Methods </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>ICDM 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=On+the+Vulnerability+of+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="252">
        <h5 class="caps">Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.</h5>
        <div class="authors">
            
                <span class="caps">Mauro Martino</span><span>,</span>
            
                <span class="caps">Francesco Calabrese</span><span>,</span>
            
                <span class="caps">Giusy Di Lorenzo</span><span>,</span>
            
                <span class="caps">Clio Andris</span><span>,</span>
            
                <span class="caps">Liu Liang</span><span>,</span>
            
                <span class="caps">Carlo Ratti</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IUI 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Ocean+of+information%3A+fusing+aggregate++individual+dynamics+for+metropolitan+analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="253">
        <h5 class="caps">Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Cognitive Science </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Humans </span>
            
                <span class="badge badge-secondary caps"> Computational Modeling </span>
            
                <span class="badge badge-secondary caps"> Brain Modeling </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Mental+Models%2C+Visual+Reasoning+and+Interaction+in+Information+Visualization%3A+A+Top-down+Perspective." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="254">
        <h5 class="caps">GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.</h5>
        <div class="authors">
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Jaeeun Shim</span><span>,</span>
            
                <span class="caps">Jaeyeon Kihm</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GeneTracer%3A+Gene+sequence+analysis+of+disease+mutations+VAST+2010+mini+challenge+3+award%3A+Excellent+process+explanation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="255">
        <h5 class="caps">Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Jaeyeon Kihm</span><span>,</span>
            
                <span class="caps">Hanseung Lee</span><span>,</span>
            
                <span class="caps">Jaegul Choo</span><span>,</span>
            
                <span class="caps">Haesun Park</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Xml </span>
            
                <span class="badge badge-secondary caps"> Fixtures </span>
            
                <span class="badge badge-secondary caps"> Social Networking (online) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Data+ingestion+and+evidence+marshalling+in+Jigsaw+VAST+2010+Mini+Challenge+1+award%3A+Good+support+for+data+ingest." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="256">
        <h5 class="caps">Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.</h5>
        <div class="authors">
            
                <span class="caps">Dugald Ralph Hutchings</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ACM Southeast Regional Conference 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Controlling+information+display+in+larger+pixel+spaces%3A+a+study+of+window+snipping+by+multiple-monitor+users." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="257">
        <h5 class="caps">10241 Executive Summary - Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Andreas Kerren</span><span>,</span>
            
                <span class="caps">Catherine Plaisant</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Collaboration </span>
            
                <span class="badge badge-secondary caps"> Display Technologies </span>
            
                <span class="badge badge-secondary caps"> Human-computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Information Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Information Visualization 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=10241+Executive+Summary+-+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="258">
        <h5 class="caps">10241 Abstracts Collection - Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Andreas Kerren</span><span>,</span>
            
                <span class="caps">Catherine Plaisant</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Collaboration </span>
            
                <span class="badge badge-secondary caps"> Display Technologies </span>
            
                <span class="badge badge-secondary caps"> Human-computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Information Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Information Visualization 2010</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=10241+Abstracts+Collection+-+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="259">
        <h5 class="caps">Computational Explanations for Report Generation in Intelligence Analysis.</h5>
        <div class="authors">
            
                <span class="caps">Ashok K. Goel</span><span>,</span>
            
                <span class="caps">Emile L. Morse</span><span>,</span>
            
                <span class="caps">Anita Raja</span><span>,</span>
            
                <span class="caps">Jean Scholtz</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ExaCt 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Computational+Explanations+for+Report+Generation+in+Intelligence+Analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="260">
        <h5 class="caps">VAST contest dataset use in education.</h5>
        <div class="authors">
            
                <span class="caps">Mark A. Whiting</span><span>,</span>
            
                <span class="caps">Chris North</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Jean Scholtz</span><span>,</span>
            
                <span class="caps">Jereme Haack</span><span>,</span>
            
                <span class="caps">Carrie Varley</span><span>,</span>
            
                <span class="caps">Jim Thomas</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Computer Science Education </span>
            
                <span class="badge badge-secondary caps"> Government </span>
            
                <span class="badge badge-secondary caps"> Educational Technology </span>
            
                <span class="badge badge-secondary caps"> Laboratories </span>
            
                <span class="badge badge-secondary caps"> Application Software </span>
            
                <span class="badge badge-secondary caps"> Educational Institutions </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Computer Science </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=VAST+contest+dataset+use+in+education." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="261">
        <h5 class="caps">The science of interaction.</h5>
        <div class="authors">
            
                <span class="caps">William A. Pike</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Remco Chang</span><span>,</span>
            
                <span class="caps">Theresa A. O'Connell</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+science+of+interaction." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="262">
        <h5 class="caps">The conference room as a toolbox: technological and social routines in corporate meeting spaces.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Plaue</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Mark Baloga</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CT 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+conference+room+as+a+toolbox%3A+technological+and+social+routines+in+corporate+meeting+spaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="263">
        <h5 class="caps">SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Aniket Kittur</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Jason I. Hong</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SHIFTR%3A+a+user-directed%2C+link-based+system+for+ad+hoc+sensemaking+of+large+heterogeneous+data+collections." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="264">
        <h5 class="caps">SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Timothy Sullivan</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Companies </span>
            
                <span class="badge badge-secondary caps"> Logistics </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Books </span>
            
                <span class="badge badge-secondary caps"> Time Series Analysis </span>
            
                <span class="badge badge-secondary caps"> Failure Analysis </span>
            
                <span class="badge badge-secondary caps"> Feedback </span>
            
                <span class="badge badge-secondary caps"> Cities And Towns </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=SellTrend%3A+Inter-Attribute+Visual+Analysis+of+Temporal+Transaction+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="265">
        <h5 class="caps">Professional analysts using a large, high-resolution display.</h5>
        <div class="authors">
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Christopher Andrews</span><span>,</span>
            
                <span class="caps">Glenn A. Fink</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Liquid Crystal Displays </span>
            
                <span class="badge badge-secondary caps"> Large Screen Displays </span>
            
                <span class="badge badge-secondary caps"> Performance Analysis </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Professional+analysts+using+a+large%2C+high-resolution+display." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="266">
        <h5 class="caps">Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Plaue</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>GROUP 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Presence++placement%3A+exploring+the+benefits+of+multiple+shared+displays+on+an+intellective+sensemaking+task." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="267">
        <h5 class="caps">How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Sarah Cohen</span><span>,</span>
            
                <span class="caps">Lawrence Hunter</span><span>,</span>
            
                <span class="caps">Joe Parry</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Software Systems </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Text Analysis </span>
            
                <span class="badge badge-secondary caps"> Genomics </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Collaboration </span>
            
                <span class="badge badge-secondary caps"> Military Computing </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Bioinformatics </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=How+interactive+visualization+can+assist+investigative+analysis%3A+Views+and+perspectives+from+domain+experts." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="268">
        <h5 class="caps">Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Collaborative Tools </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Control Systems </span>
            
                <span class="badge badge-secondary caps"> Guidelines </span>
            
                <span class="badge badge-secondary caps"> Performance Analysis </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
                <span class="badge badge-secondary caps"> Collaborative Work </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluating+visual+analytics+systems+for+investigative+analysis%3A+Deriving+design+principles+from+a+case+study." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="269">
        <h5 class="caps">Visualizing cyber security: Usable workspaces.</h5>
        <div class="authors">
            
                <span class="caps">Glenn A. Fink</span><span>,</span>
            
                <span class="caps">Christopher L. North</span><span>,</span>
            
                <span class="caps">Alex Endert</span><span>,</span>
            
                <span class="caps">Stuart Rose</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VizSEC 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualizing+cyber+security%3A+Usable+workspaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="270">
        <h5 class="caps">A visual analytics system for radio frequency fingerprinting-based localization.</h5>
        <div class="authors">
            
                <span class="caps">Yi Han</span><span>,</span>
            
                <span class="caps">Erich P. Stuntebeck</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Gregory D. Abowd</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Signal Generators </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Satellite Broadcasting </span>
            
                <span class="badge badge-secondary caps"> Fingerprint Recognition </span>
            
                <span class="badge badge-secondary caps"> Debugging </span>
            
                <span class="badge badge-secondary caps"> Radio Frequency </span>
            
                <span class="badge badge-secondary caps"> Global Positioning System </span>
            
                <span class="badge badge-secondary caps"> Receivers </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
                <span class="badge badge-secondary caps"> Rf Signals </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2009</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+visual+analytics+system+for+radio+frequency+fingerprinting-based+localization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="271">
        <h5 class="caps">What to do when search fails: finding information by association.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span><span>,</span>
            
                <span class="caps">Andrew Faulring</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Computing Methodologies </span>
            
                <span class="badge badge-secondary caps"> Interaction Paradigms </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Computer Graphics </span>
            
                <span class="badge badge-secondary caps"> Graphics Input Devices </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Information Retrieval </span>
            
                <span class="badge badge-secondary caps"> Graphics Systems And Interfaces </span>
            
                <span class="badge badge-secondary caps"> Interaction Devices </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=What+to+do+when+search+fails%3A+finding+information+by+association." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="272">
        <h5 class="caps">Viz-A-Vis: Toward Visualizing Video through Computer Vision.</h5>
        <div class="authors">
            
                <span class="caps">Mario Romero</span><span>,</span>
            
                <span class="caps">Jay Summet</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Gregory D. Abowd</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Computer Vision </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Viz-A-Vis%3A+Toward+Visualizing+Video+through+Computer+Vision." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="273">
        <h5 class="caps">Visualization for information exploration and analysis.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VL/HCC 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualization+for+information+exploration+and+analysis." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="274">
        <h5 class="caps">Visualization for information exploration and analysis: keynote presentation.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>SOFTVIS 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualization+for+information+exploration+and+analysis%3A+keynote+presentation." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="275">
        <h5 class="caps">Understanding and characterizing insights: how do people gain insights using information visualization?</h5>
        <div class="authors">
            
                <span class="caps">Ji Soo Yi</span><span>,</span>
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Julie A. Jacko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Hci Design And Evaluation Methods </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>BELIV 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Understanding+and+characterizing+insights%3A+how+do+people+gain+insights+using+information+visualization%3F" target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="276">
        <h5 class="caps">The Value of Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Jean-Daniel Fekete</span><span>,</span>
            
                <span class="caps">Jarke J. van Wijk</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Chris North</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Preattentive Processing </span>
            
                <span class="badge badge-secondary caps"> Perceptual Chunk </span>
            
                <span class="badge badge-secondary caps"> Information Visualization </span>
            
                <span class="badge badge-secondary caps"> Exploratory Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Perceptual Inference </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Information Visualization 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+Value+of+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="277">
        <h5 class="caps">Teaching Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Andreas Kerren</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Jason Dykes</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Georgia Tech </span>
            
                <span class="badge badge-secondary caps"> Graph Drawing </span>
            
                <span class="badge badge-secondary caps"> Interaction Technique </span>
            
                <span class="badge badge-secondary caps"> Visualization Technique </span>
            
                <span class="badge badge-secondary caps"> Information Visualization </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Information Visualization 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Teaching+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="278">
        <h5 class="caps">Lightweight task/application performance using single versus multiple monitors: a comparative study.</h5>
        <div class="authors">
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Lightweight+task%2Fapplication+performance+using+single+versus+multiple+monitors%3A+a+comparative+study." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="279">
        <h5 class="caps">GRAPHITE: A Visual Query System for Large Graphs.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span><span>,</span>
            
                <span class="caps">Hanghang Tong</span><span>,</span>
            
                <span class="caps">Jason I. Hong</span><span>,</span>
            
                <span class="caps">Brian Gallagher</span><span>,</span>
            
                <span class="caps">Tina Eliassi-Rad</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICDM Workshops 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=GRAPHITE%3A+A+Visual+Query+System+for+Large+Graphs." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="280">
        <h5 class="caps">Evaluating Visual Analytics at the 2007 VAST Symposium Contest.</h5>
        <div class="authors">
            
                <span class="caps">Catherine Plaisant</span><span>,</span>
            
                <span class="caps">Georges G. Grinstein</span><span>,</span>
            
                <span class="caps">Jean Scholtz</span><span>,</span>
            
                <span class="caps">Mark A. Whiting</span><span>,</span>
            
                <span class="caps">Theresa A. O'Connell</span><span>,</span>
            
                <span class="caps">Sharon J. Laskowski</span><span>,</span>
            
                <span class="caps">Lynn Chien</span><span>,</span>
            
                <span class="caps">Annie Tat</span><span>,</span>
            
                <span class="caps">William Wright</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Neel Parekh</span><span>,</span>
            
                <span class="caps">Kanupriyah Singhal</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Information Services </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Web Sites </span>
            
                <span class="badge badge-secondary caps"> Laboratories </span>
            
                <span class="badge badge-secondary caps"> Nist </span>
            
                <span class="badge badge-secondary caps"> Internet </span>
            
                <span class="badge badge-secondary caps"> Radio Access Networks </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Computer Graphics and Applications 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Evaluating+Visual+Analytics+at+the+2007+VAST+Symposium+Contest." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="281">
        <h5 class="caps">Effectiveness of Animation in Trend Visualization.</h5>
        <div class="authors">
            
                <span class="caps">George G. Robertson</span><span>,</span>
            
                <span class="caps">Roland Fernandez</span><span>,</span>
            
                <span class="caps">Danyel Fisher</span><span>,</span>
            
                <span class="caps">Bongshin Lee</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Displays </span>
            
                <span class="badge badge-secondary caps"> Animation </span>
            
                <span class="badge badge-secondary caps"> Dictionaries </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Effectiveness+of+Animation+in+Trend+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="282">
        <h5 class="caps">Distributed Cognition as a Theoretical Framework for Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Nancy J. Nersessian</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Cognitive Science </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Testing </span>
            
                <span class="badge badge-secondary caps"> Humans </span>
            
                <span class="badge badge-secondary caps"> Buildings </span>
            
                <span class="badge badge-secondary caps"> Distributed Computing </span>
            
                <span class="badge badge-secondary caps"> Guidelines </span>
            
                <span class="badge badge-secondary caps"> Performance Evaluation </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2008</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Distributed+Cognition+as+a+Theoretical+Framework+for+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="283">
        <h5 class="caps">Netprobe: a fast and scalable system for fraud detection in online auction networks.</h5>
        <div class="authors">
            
                <span class="caps">Shashank Pandit</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Samuel Wang</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WWW 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Netprobe%3A+a+fast+and+scalable+system+for+fraud+detection+in+online+auction+networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="284">
        <h5 class="caps">Toward a Deeper Understanding of the Role of Interaction in Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Ji Soo Yi</span><span>,</span>
            
                <span class="caps">Youn ah Kang</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Julie A. Jacko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Computer Displays </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Taxonomy </span>
            
                <span class="badge badge-secondary caps"> Filters </span>
            
                <span class="badge badge-secondary caps"> Rendering (computer Graphics) </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction </span>
            
                <span class="badge badge-secondary caps"> Conference Proceedings </span>
            
                <span class="badge badge-secondary caps"> Computer Graphics </span>
            
                <span class="badge badge-secondary caps"> Research And Development </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Toward+a+Deeper+Understanding+of+the+Role+of+Interaction+in+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="285">
        <h5 class="caps">Parallel crawling for online social networks.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Shashank Pandit</span><span>,</span>
            
                <span class="caps">Samuel Wang</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>WWW 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Parallel+crawling+for+online+social+networks." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="286">
        <h5 class="caps">Visual Analytics with Jigsaw.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Neel Parekh</span><span>,</span>
            
                <span class="caps">Kanupriya Singhal</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Calendars </span>
            
                <span class="badge badge-secondary caps"> Usability </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Displays </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Yarn </span>
            
                <span class="badge badge-secondary caps"> Data Mining </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visual+Analytics+with+Jigsaw." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="287">
        <h5 class="caps">Jigsaw: Supporting Investigative Analysis through Interactive Visualization.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Kanupriya Singhal</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Algorithm Design And Analysis </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Yarn </span>
            
                <span class="badge badge-secondary caps"> Costs </span>
            
                <span class="badge badge-secondary caps"> Performance Analysis </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Embedded Computing </span>
            
                <span class="badge badge-secondary caps"> Electronic Mail </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Jigsaw%3A+Supporting+Investigative+Analysis+through+Interactive+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="288">
        <h5 class="caps">Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.</h5>
        <div class="authors">
            
                <span class="caps">Carsten G</span><span>,</span>
            
                <span class="caps">Zhicheng Liu</span><span>,</span>
            
                <span class="caps">Neel Parekh</span><span>,</span>
            
                <span class="caps">Kanupriyah Singhal</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Gas Insulated Transmission Lines </span>
            
                <span class="badge badge-secondary caps"> Visualization </span>
            
                <span class="badge badge-secondary caps"> Filters </span>
            
                <span class="badge badge-secondary caps"> Information Systems </span>
            
                <span class="badge badge-secondary caps"> Usa Councils </span>
            
                <span class="badge badge-secondary caps"> Radio Access Networks </span>
            
                <span class="badge badge-secondary caps"> Java </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Yarn </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Jigsaw+meets+Blue+Iguanodon+-+The+VAST+2007+Contest." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="289">
        <h5 class="caps">Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.</h5>
        <div class="authors">
            
                <span class="caps">Iv</span><span>,</span>
            
                <span class="caps">Jacob O. Wobbrock</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Andrew Faulring</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Touch Screens </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Devices </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Eyes+on+the+road%2C+hands+on+the+wheel%3A+thumb-based+interaction+techniques+for+input+on+steering+wheels." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="290">
        <h5 class="caps">Demonstrating the viability of automatically generated user interfaces.</h5>
        <div class="authors">
            
                <span class="caps">Jeffrey Nichols</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Interaction Paradigms </span>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Development Frameworks And Environments </span>
            
                <span class="badge badge-secondary caps"> Software Notations And Tools </span>
            
                <span class="badge badge-secondary caps"> Software And Its Engineering </span>
            
                <span class="badge badge-secondary caps"> Graphical User Interfaces </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Integrated And Visual Development Environments </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Demonstrating+the+viability+of+automatically+generated+user+interfaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="291">
        <h5 class="caps">DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.</h5>
        <div class="authors">
            
                <span class="caps">Niklas Elmqvist</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Philippas Tsigas</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Large-scale Systems </span>
            
                <span class="badge badge-secondary caps"> Visual Analytics </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Bars </span>
            
                <span class="badge badge-secondary caps"> Protocols </span>
            
                <span class="badge badge-secondary caps"> History </span>
            
                <span class="badge badge-secondary caps"> Mice </span>
            
                <span class="badge badge-secondary caps"> Filtering </span>
            
                <span class="badge badge-secondary caps"> Feedback </span>
            
                <span class="badge badge-secondary caps"> Multidimensional Systems </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE VAST 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=DataMeadow%3A+A+Visual+Canvas+for+Analysis+of+Large-Scale+Multivariate+Data." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="292">
        <h5 class="caps">Casual Information Visualization: Depictions of Data in Everyday Life.</h5>
        <div class="authors">
            
                <span class="caps">Zachary Pousman</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Michael Mateas</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Focusing </span>
            
                <span class="badge badge-secondary caps"> Cognition </span>
            
                <span class="badge badge-secondary caps"> Vocabulary </span>
            
                <span class="badge badge-secondary caps"> Refining </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Testing </span>
            
                <span class="badge badge-secondary caps"> Finance </span>
            
                <span class="badge badge-secondary caps"> Information Analysis </span>
            
                <span class="badge badge-secondary caps"> Government </span>
            
                <span class="badge badge-secondary caps"> Technology Management </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Casual+Information+Visualization%3A+Depictions+of+Data+in+Everyday+Life." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="293">
        <h5 class="caps">Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Plaue</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Animation+in+a+peripheral+display%3A+distraction%2C+appeal%2C+and+information+conveyance+in+varying+display+configurations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="294">
        <h5 class="caps">An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.</h5>
        <div class="authors">
            
                <span class="caps">Jacob O. Wobbrock</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Touch Screens </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Devices </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2007</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=An+alternative+to+push%2C+press%2C+and+tap-tap-tap%3A+gesturing+on+an+isometric+joystick+for+mobile+phone+text+entry." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="295">
        <h5 class="caps">Detecting Fraudulent Personalities in Networks of Online Auctioneers.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Shashank Pandit</span><span>,</span>
            
                <span class="caps">Christos Faloutsos</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>PKDD 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Detecting+Fraudulent+Personalities+in+Networks+of+Online+Auctioneers." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="296">
        <h5 class="caps">In-stroke word completion.</h5>
        <div class="authors">
            
                <span class="caps">Jacob O. Wobbrock</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Touch Screens </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Devices </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>UIST 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=In-stroke+word+completion." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="297">
        <h5 class="caps">Huddle: automatically generating interfaces for systems of multiple connected appliances.</h5>
        <div class="authors">
            
                <span class="caps">Jeffrey Nichols</span><span>,</span>
            
                <span class="caps">Brandon Rothrock</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Graphical User Interfaces </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Interaction Paradigms </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>UIST 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Huddle%3A+automatically+generating+interfaces+for+systems+of+multiple+connected+appliances." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="298">
        <h5 class="caps">Integrating isometric joysticks into mobile phones for text entry.</h5>
        <div class="authors">
            
                <span class="caps">Duen Horng (Polo) Chau</span><span>,</span>
            
                <span class="caps">Jacob O. Wobbrock</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span><span>,</span>
            
                <span class="caps">Brandon Rothrock</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>CHI Extended Abstracts 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Integrating+isometric+joysticks+into+mobile+phones+for+text+entry." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="299">
        <h5 class="caps">Answering why and why not questions in user interfaces.</h5>
        <div class="authors">
            
                <span class="caps">Brad A. Myers</span><span>,</span>
            
                <span class="caps">David A. Weitzman</span><span>,</span>
            
                <span class="caps">Amy J. Ko</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Human-centered Computing </span>
            
                <span class="badge badge-secondary caps"> Development Frameworks And Environments </span>
            
                <span class="badge badge-secondary caps"> Software Development Process Management </span>
            
                <span class="badge badge-secondary caps"> Software Notations And Tools </span>
            
                <span class="badge badge-secondary caps"> Software And Its Engineering </span>
            
                <span class="badge badge-secondary caps"> Human Computer Interaction (hci) </span>
            
                <span class="badge badge-secondary caps"> Software Creation And Management </span>
            
                <span class="badge badge-secondary caps"> Designing Software </span>
            
                <span class="badge badge-secondary caps"> Software Implementation Planning </span>
            
                <span class="badge badge-secondary caps"> Software Design Techniques </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>CHI 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Answering+why+and+why+not+questions+in+user+interfaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="300">
        <h5 class="caps">A Linguistic Analysis of How People Describe Software Problems.</h5>
        <div class="authors">
            
                <span class="caps">Amy J. Ko</span><span>,</span>
            
                <span class="caps">Brad A. Myers</span><span>,</span>
            
                <span class="caps">Duen Horng (Polo) Chau</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VL/HCC 2006</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=A+Linguistic+Analysis+of+How+People+Describe+Software+Problems." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="301">
        <h5 class="caps">Dust  Magnet: multivariate information visualization using a magnet metaphor.</h5>
        <div class="authors">
            
                <span class="caps">Ji Soo Yi</span><span>,</span>
            
                <span class="caps">Rachel Melton</span><span>,</span>
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Julie A. Jacko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Inf. Vis. 2005</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Dust++Magnet%3A+multivariate+information+visualization+using+a+magnet+metaphor." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="302">
        <h5 class="caps">Knowledge Precepts for Design and Evaluation of Information Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Robert A. Amar</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Risk Management </span>
            
                <span class="badge badge-secondary caps"> Data Visualization </span>
            
                <span class="badge badge-secondary caps"> Data Analysis </span>
            
                <span class="badge badge-secondary caps"> Humans </span>
            
                <span class="badge badge-secondary caps"> Scattering </span>
            
                <span class="badge badge-secondary caps"> Decision Making </span>
            
                <span class="badge badge-secondary caps"> Displays </span>
            
                <span class="badge badge-secondary caps"> Uncertainty </span>
            
                <span class="badge badge-secondary caps"> Information Filtering </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>IEEE Trans. Vis. Comput. Graph. 2005</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Knowledge+Precepts+for+Design+and+Evaluation+of+Information+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="303">
        <h5 class="caps">IDS RainStorm: Visualizing IDS Alarms.</h5>
        <div class="authors">
            
                <span class="caps">Kulsoom Abdullah</span><span>,</span>
            
                <span class="caps">Christopher P. Lee</span><span>,</span>
            
                <span class="caps">Gregory J. Conti</span><span>,</span>
            
                <span class="caps">John A. Copeland</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>VizSEC 2005</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=IDS+RainStorm%3A+Visualizing+IDS+Alarms." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="304">
        <h5 class="caps">Low-Level Components of Analytic Activity in Information Visualization.</h5>
        <div class="authors">
            
                <span class="caps">Robert A. Amar</span><span>,</span>
            
                <span class="caps">James Eagan</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>INFOVIS 2005</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Low-Level+Components+of+Analytic+Activity+in+Information+Visualization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="305">
        <h5 class="caps">Gammatella: Visualization of Program-Execution Data for Deployed Software.</h5>
        <div class="authors">
            
                <span class="caps">Alessandro Orso</span><span>,</span>
            
                <span class="caps">James A. Jones</span><span>,</span>
            
                <span class="caps">Mary Jean Harrold</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICSE 2004</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Gammatella%3A+Visualization+of+Program-Execution+Data+for+Deployed+Software." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="306">
        <h5 class="caps">Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.</h5>
        <div class="authors">
            
                <span class="caps">Christopher Plaue</span><span>,</span>
            
                <span class="caps">Todd Miller</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Graphics Interface 2004</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Is+a+Picture+Worth+a+Thousand+Words%3F+An+Evaluation+of+Information+Awareness+Displays." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="307">
        <h5 class="caps">Personalized Peripheral Information Awareness Through Information Art.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Todd Miller</span><span>,</span>
            
                <span class="caps">Zachary Pousman</span><span>,</span>
            
                <span class="caps">Christopher Plaue</span><span>,</span>
            
                <span class="caps">Osman Ullah</span>
            
        </div>
        <div>
            
                <span class="badge badge-secondary caps"> Visual Continuity </span>
            
                <span class="badge badge-secondary caps"> News Headline </span>
            
                <span class="badge badge-secondary caps"> Design Interview </span>
            
                <span class="badge badge-secondary caps"> Background Image </span>
            
                <span class="badge badge-secondary caps"> Visual Element </span>
            
            
            <div class="vspace-xs-fixed"></div>
            
        </div>
        <div class="venue caps"><i>UbiComp 2004</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Personalized+Peripheral+Information+Awareness+Through+Information+Art." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="308">
        <h5 class="caps">BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">Robert A. Amar</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>INFOVIS 2004</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=BEST+PAPER%3A+A+Knowledge+Task-Based+Framework+for+Design+and+Evaluation+of+Information+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="309">
        <h5 class="caps">FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.</h5>
        <div class="authors">
            
                <span class="caps">Christoph Csallner</span><span>,</span>
            
                <span class="caps">Marcus Handte</span><span>,</span>
            
                <span class="caps">Othmar Lehmann</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>INFOVIS 2003</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=FundExplorer%3A+Supporting+the+Diversification+of+Mutual+Fund+Portfolios+Using+Context+Treemaps." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="310">
        <h5 class="caps">Visualization of test information to assist fault localization.</h5>
        <div class="authors">
            
                <span class="caps">James A. Jones</span><span>,</span>
            
                <span class="caps">Mary Jean Harrold</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>ICSE 2002</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Visualization+of+test+information+to+assist+fault+localization." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="311">
        <h5 class="caps">Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Eugene Zhang</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>INFOVIS 2000</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=Focus%2BContext+Display+and+Navigation+Techniques+for+Enhancing+Radial%2C+Space-Filling+Hierarchy+Visualizations." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="312">
        <h5 class="caps">An evaluation of space-filling information visualizations for depicting hierarchical structures.</h5>
        <div class="authors">
            
                <span class="caps">John T. Stasko</span><span>,</span>
            
                <span class="caps">Richard Catrambone</span><span>,</span>
            
                <span class="caps">Mark Guzdial</span><span>,</span>
            
                <span class="caps">Kevin McDonald</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>Int. J. Hum. Comput. Stud. 2000</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=An+evaluation+of+space-filling+information+visualizations+for+depicting+hierarchical+structures." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
    <div class="publication col-lg-12" data-pub-id="313">
        <h5 class="caps">The information mural: a technique for displaying and navigating large information spaces.</h5>
        <div class="authors">
            
                <span class="caps">Dean F. Jerding</span><span>,</span>
            
                <span class="caps">John T. Stasko</span>
            
        </div>
        <div>
            
            
        </div>
        <div class="venue caps"><i>INFOVIS 1995</i></div>
        <div>
            <!-- Commenting this out because DBLP and our current publications source does not have links for all publications. Google Scholar is more reliable. -->
            <!--  -->
            
            
            <a href="https://scholar.google.com/scholar?hl=en&amp;q=The+information+mural%3A+a+technique+for+displaying+and+navigating+large+information+spaces." target="_blank">Google Scholar&nbsp;<i class="fa fa-external-link"></i></a>
        </div>
        <div class="separator"></div>
    </div>
    
</div>

<script>
    const pubsData = [{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","next":{"url":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.html","relative_path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","id":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Zijie J. Wang","Nilaksh Das","Anindya S. Paul","Pruthvi Perumalla","Zhiyan Zhou","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.","venue":"arXiv","year":2021,"slug":"2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","ext":".md"},"path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},"url":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.html","relative_path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","next":null,"path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","id":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Haekyu Park","Zijie J. Wang","Nilaksh Das","Anindya S. Paul","Pruthvi Perumalla","Zhiyan Zhou","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.","venue":"arXiv","year":2021,"slug":"2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","next":{"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},"path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","next":{"output":"\n","previous":{"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},"url":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.html","relative_path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","next":null,"path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","id":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Haekyu Park","Zijie J. Wang","Nilaksh Das","Anindya S. Paul","Pruthvi Perumalla","Zhiyan Zhou","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.","venue":"arXiv","year":2021,"slug":"2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","ext":".md"},"path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","next":{"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","next":{"url":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.html","relative_path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","path":"_publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models.md","id":"/publications/2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Zijie J. Wang","Nilaksh Das","Anindya S. Paul","Pruthvi Perumalla","Zhiyan Zhou","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models.","venue":"arXiv","year":2021,"slug":"2021-skeletonvis-interactive-visualization-for-understanding-adversarial-attacks-on-human-action-recognition-models","ext":".md"},"path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},"path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","next":{"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","next":{"output":"\n","previous":{"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","next":{"url":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.html","relative_path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","path":"_publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles.md","id":"/publications/2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Ahsan Qamar","Alex Endert"],"link":null,"tags":[],"title":"SafetyLens: Visual Data Analysis of Functional Safety of Vehicles.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-safetylens-visual-data-analysis-of-functional-safety-of-vehicles","ext":".md"},"path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","next":{"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","next":{"output":"\n","previous":{"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","next":{"url":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.html","relative_path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","path":"_publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization.md","id":"/publications/2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Omar Shaikh","Haekyu Park","Will Epperson","Muhammed Ahmed","Stephane Pinel","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-recast-enabling-user-recourse-and-interpretability-of-toxicity-detection-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","next":{"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","next":{"output":"\n","previous":{"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","next":{"url":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.html","relative_path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","path":"_publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries.md","id":"/publications/2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","collection":"publications","draft":false,"categories":[],"authors":["Arpit Narechania","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-nl4dv-a-toolkit-for-generating-analytic-specifications-for-data-visualization-from-natural-language-queries","ext":".md"},"path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},{"output":"\n","previous":{"output":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","previous":{"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","next":{"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","excerpt":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","collection":"publications","content":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","next":{"url":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.html","relative_path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","path":"_publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software.md","id":"/publications/2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Rahul Duggal","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software.","venue":"arXiv","year":2021,"slug":"2021-malnet-a-largescale-cybersecurity-image-database-of-malicious-software","ext":".md"},"path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},{"output":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","next":{"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","next":{"output":"\n","previous":{"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","next":{"url":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.html","relative_path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","path":"_publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models.md","id":"/publications/2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jon Saad-Falcon","Austin P. Wright","Nilaksh Das","Scott Freitas","Omar Isaac Asensio","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models.","venue":"arXiv","year":2021,"slug":"2021-energyvis-interactively-tracking-and-exploring-energy-consumption-for-ml-models","ext":".md"},"path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","excerpt":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","collection":"publications","content":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","next":{"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","next":{"output":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","previous":{"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","next":{"url":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.html","relative_path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","path":"_publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization.md","id":"/publications/2021-dodrio-exploring-transformer-models-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Dodrio: Exploring Transformer Models with Interactive Visualization.","venue":"arXiv","year":2021,"slug":"2021-dodrio-exploring-transformer-models-with-interactive-visualization","ext":".md"},"path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","excerpt":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","collection":"publications","content":"<p>Animation helps viewers follow transitions in data graphics. When authoring animations that incorporate data, designers must carefully coordinate the behaviors of visual objects such as entering, exiting, merging and splitting, and specify the temporal rhythms of transition through staging and staggering. We present Data Animator, a system for authoring animated data graphics without programming. Data Animator leverages the Data Illustrator framework to analyze and match objects between two static visualizations, and generates automated transitions by default. Designers have the flexibility to interpret and adjust the matching results through a visual interface. Data Animator also supports the division of a complex animation into stages through hierarchical keyframes, and uses data attributes to stagger the start time and vary the speed of animating objects through a novel timeline interface. We validate Data Animator’s expressiveness via a gallery of examples, and evaluate its usability in a re-creation study with designers.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","next":{"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","next":{"output":"\n","previous":{"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","next":{"url":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics.html","relative_path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","path":"_publications/2021-data-animator-authoring-expressive-animated-data-grahics.md","id":"/publications/2021-data-animator-authoring-expressive-animated-data-grahics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Human-centered computing"],"title":"Data Animator: Authoring Expressive Animated Data Graphics.","venue":"CHI","year":2021,"slug":"2021-data-animator-authoring-expressive-animated-data-grahics","ext":".md"},"path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},{"output":"\n","previous":{"output":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","previous":{"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","next":{"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","collection":"publications","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","excerpt":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","collection":"publications","content":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","next":{"output":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","previous":{"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","collection":"publications","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","next":{"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","collection":"publications","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","excerpt":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},{"output":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","next":{"output":"\n","previous":{"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","next":{"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","excerpt":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","collection":"publications","content":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","collection":"publications","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","next":{"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","next":{"output":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","previous":{"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","next":{"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","excerpt":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","collection":"publications","content":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","collection":"publications","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","next":{"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","next":{"output":"\n","previous":{"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","next":{"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","collection":"publications","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","next":{"output":"\n","previous":{"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","collection":"publications","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","next":{"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","next":{"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","collection":"publications","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","next":{"output":"\n","previous":{"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","collection":"publications","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","next":{"url":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.html","relative_path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","path":"_publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis.md","id":"/publications/2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ayshwarya Saktheeswaran","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-touch-speech-or-touch-and-speech-investigating-multimodal-interaction-for-visual-network-exploration-and-analysis","ext":".md"},"path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},{"output":"\n","previous":{"output":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","previous":{"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","next":{"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","excerpt":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","collection":"publications","content":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","next":{"output":"\n","previous":{"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","next":{"url":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.html","relative_path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","path":"_publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks.md","id":"/publications/2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","collection":"publications","draft":false,"categories":[],"authors":["Joshua Baker","Clio Andris","Daniel DellaPosta"],"link":null,"tags":[],"title":"Spatial Social Network (SSN) Hot Spot Detection: Scan Methods for Non-Planar Networks.","venue":"arXiv","year":2020,"slug":"2020-spatial-social-network-ssn-hot-spot-detection-scan-methods-for-nonplanar-networks","ext":".md"},"path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},{"output":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","previous":{"output":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","previous":{"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","collection":"publications","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","next":{"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","excerpt":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","collection":"publications","content":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","next":{"url":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.html","relative_path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","path":"_publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations.md","id":"/publications/2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","collection":"publications","draft":false,"categories":[],"authors":["Fabian Sperrle","Mennatallah El-Assady","Grace Guo","Duen Horng (Polo) Chau","Alex Endert","Daniel A. Keim"],"link":null,"tags":[],"title":"Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.","venue":"arXiv","year":2020,"slug":"2020-should-we-trust-xai-design-dimensions-for-structured-experimental-evaluations","ext":".md"},"path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","excerpt":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","collection":"publications","content":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},{"output":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","previous":{"output":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","previous":{"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","next":{"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","excerpt":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","collection":"publications","content":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","next":{"output":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","previous":{"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","next":{"url":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.html","relative_path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","path":"_publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild.md","id":"/publications/2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Cao Xiao","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild.","venue":"WWW","year":2020,"slug":"2020-rest-robust-and-efficient-neural-networks-for-sleep-monitoring-in-the-wild","ext":".md"},"path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","excerpt":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","collection":"publications","content":"<p>The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics-personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization-that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","excerpt":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","collection":"publications","content":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},{"output":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","previous":{"output":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","previous":{"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","collection":"publications","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","next":{"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","collection":"publications","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","excerpt":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","next":{"output":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","previous":{"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","collection":"publications","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","next":{"url":"/publications/2020-reaching-broader-audiences-with-data-visualization.html","relative_path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","path":"_publications/2020-reaching-broader-audiences-with-data-visualization.md","id":"/publications/2020-reaching-broader-audiences-with-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Eun Kyoung Choe","Petra Isenberg","Kim Marriott","John T. Stasko","Theresa-Marie Rhyne"],"link":null,"tags":["Software Tools","Visualization","Data Visualization","Smart Phones","Public Infrastructure"],"title":"Reaching Broader Audiences With Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-reaching-broader-audiences-with-data-visualization","ext":".md"},"path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","excerpt":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","collection":"publications","content":"<p>Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta‐information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain‐specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","excerpt":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","collection":"publications","content":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},{"output":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","next":{"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","next":{"output":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","previous":{"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"url":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.html","relative_path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","next":{"url":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.html","relative_path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","path":"_publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks.md","id":"/publications/2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Shenyu Xu","Michael Gleicher","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Computing Methodologies","Mathematics Of Computing","Interactive Objective Functions","Machine Learning Task","Model Construction And Selection","Classification","Humancentered Computing"],"title":"QUESTO: Interactive Construction of Objective Functions for Classification Tasks.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-questo-interactive-construction-of-objective-functions-for-classification-tasks","ext":".md"},"path":"_publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals.md","id":"/publications/2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","excerpt":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","collection":"publications","content":"<p>Interactive data exploration and analysis is an inherently personal process. One’s background, experience, interests, cognitive style, personality, and other sociotechnical factors often shape such a process, as well as the provenance of exploring, analyzing, and interpreting data. This Viewpoint posits both what personal information and how such personal information could be taken into account to design more effective visual analytic systems, a valuable and under-explored direction.</p>\n","draft":false,"categories":[],"authors":["John E. Wenskovitch","Michelle X. Zhou","Christopher Collins","Remco Chang","Michelle Dowling","Alex Endert","Kai Xu","Theresa-Marie Rhyne"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Adaptive Systems","Memory Management","Task Analysis"],"title":"Putting the \"I\" in Interaction: Interactive Interfaces Personalized to Individuals.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-putting-the-i-in-interaction-interactive-interfaces-personalized-to-individuals","ext":".md"},"path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","excerpt":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as “black boxes” in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},{"output":"\n","previous":{"output":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","previous":{"url":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.html","relative_path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","path":"_publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics.md","id":"/publications/2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Adam Coscia","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Toward a Bias-Aware Future for Mixed-Initiative Visual Analytics.","venue":"arXiv","year":2020,"slug":"2020-toward-a-biasaware-future-for-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.html","relative_path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","next":{"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"path":"_publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics.md","id":"/publications/2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","excerpt":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","collection":"publications","content":"<p>Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next‐generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real‐world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets &amp; templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers’ preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Zhicheng Liu","Wilmot Li","John T. Stasko"],"link":null,"tags":["Empirical Studies In Visualization","Humancentered Computing","Visualization Theory","Concepts And Paradigms"],"title":"Understanding the Design Space and Authoring Paradigms for Animated Data Graphics.","venue":"Comput. Graph. Forum","year":2020,"slug":"2020-understanding-the-design-space-and-authoring-paradigms-for-animated-data-graphics","ext":".md"},"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","next":{"output":"\n","previous":{"url":"/publications/2020-what-are-data-insights-to-professional-visualization-users.html","relative_path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},"url":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.html","relative_path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","next":{"url":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.html","relative_path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","path":"_publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs.md","id":"/publications/2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shenyu Xu","Subhajit Das","Florian Heimerl","Cong Liu","Shah Rukh Humayoun","Michael Gleicher","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2021,"slug":"2021-cava-a-visual-analytics-system-for-exploratory-columnar-data-augmentation-using-knowledge-graphs","ext":".md"},"path":"_publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions.md","id":"/publications/2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Alex Endert"],"link":null,"tags":[],"title":"CACTUS: Detecting and Resolving Conflicts in Objective Functions.","venue":"arXiv","year":2021,"slug":"2021-cactus-detecting-and-resolving-conflicts-in-objective-functions","ext":".md"},"path":"_publications/2020-what-are-data-insights-to-professional-visualization-users.md","id":"/publications/2020-what-are-data-insights-to-professional-visualization-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"What are Data Insights to Professional Visualization Users?","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-what-are-data-insights-to-professional-visualization-users","ext":".md"},{"output":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","next":{"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","next":{"output":"\n","previous":{"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","next":{"url":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.html","relative_path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","path":"_publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning.md","id":"/publications/2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/massif","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-massif-interactive-interpretation-of-adversarial-attacks-on-deep-learning","ext":".md"},"path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","excerpt":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","collection":"publications","content":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","collection":"publications","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","next":{"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","next":{"output":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","previous":{"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","next":{"url":"/publications/2020-mapping-researchers-with-peoplemap.html","relative_path":"_publications/2020-mapping-researchers-with-peoplemap.md","path":"_publications/2020-mapping-researchers-with-peoplemap.md","id":"/publications/2020-mapping-researchers-with-peoplemap","collection":"publications","draft":false,"categories":[],"authors":["Jon Saad-Falcon","Omar Shaikh","Zijie J. Wang","Austin P. Wright","Sasha Richardson","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mapping Researchers with PeopleMap.","venue":"arXiv","year":2020,"slug":"2020-mapping-researchers-with-peoplemap","ext":".md"},"path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","excerpt":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","collection":"publications","content":"<p>We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","next":{"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","next":{"output":"\n","previous":{"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","next":{"url":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.html","relative_path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","path":"_publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction.md","id":"/publications/2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Samuel Huron","Charles Perin","Alex Endert"],"link":null,"tags":["Visualization","Instruments","Data Visualization","Bars","Encoding","Image Color Analysis","Tools"],"title":"Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-investigating-direct-manipulation-of-graphical-encodings-as-a-method-for-user-interaction","ext":".md"},"path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},{"output":"\n","previous":{"output":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","previous":{"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","next":{"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","collection":"publications","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","excerpt":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","collection":"publications","content":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","next":{"output":"\n","previous":{"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","collection":"publications","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","next":{"url":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.html","relative_path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","path":"_publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration.md","id":"/publications/2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Bongshin Lee","John T. Stasko"],"link":null,"tags":[],"title":"Interweaving Multimodal Interaction with Flexible Unit Visualizations for Data Exploration.","venue":"arXiv","year":2020,"slug":"2020-interweaving-multimodal-interaction-with-flexible-unit-visualizations-for-data-exploration","ext":".md"},"path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},{"output":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","previous":{"output":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","previous":{"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","next":{"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","excerpt":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","collection":"publications","content":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","next":{"url":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.html","relative_path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","path":"_publications/2020-interpreting-deep-neural-networks-through-prototype-factorization.md","id":"/publications/2020-interpreting-deep-neural-networks-through-prototype-factorization","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Panpan Xu","Zeng Dai","Alex Endert","Liu Ren"],"link":null,"tags":[],"title":"Interpreting Deep Neural Networks through Prototype Factorization.","venue":"ICDM (Workshops)","year":2020,"slug":"2020-interpreting-deep-neural-networks-through-prototype-factorization","ext":".md"},"path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","excerpt":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","collection":"publications","content":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},{"output":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","next":{"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","next":{"output":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","previous":{"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","next":{"url":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.html","relative_path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","path":"_publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382.md","id":"/publications/2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","collection":"publications","draft":false,"categories":[],"authors":["Daniela Oelke","Daniel A. Keim","Duen Horng (Polo) Chau","Alex Endert"],"link":null,"tags":[],"title":"Interactive Visualization for Fostering Trust in AI (Dagstuhl Seminar 20382).","venue":"Dagstuhl Reports","year":2020,"slug":"2020-interactive-visualization-for-fostering-trust-in-ai-dagstuhl-seminar-20382","ext":".md"},"path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","excerpt":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","collection":"publications","content":"<p>In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","excerpt":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","collection":"publications","content":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","next":{"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","next":{"output":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","previous":{"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","next":{"url":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.html","relative_path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","path":"_publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization.md","id":"/publications/2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko","Daniel F. Keefe","Melanie Tory"],"link":null,"tags":["Visualization","Data Visualization","Measurement","Task Analysis","Tools","Training","Natural Languages"],"title":"How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2020,"slug":"2020-how-to-ask-what-to-say-strategies-for-evaluating-natural-language-interfaces-for-data-visualization","ext":".md"},"path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","excerpt":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","collection":"publications","content":"<p>Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","collection":"publications","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","next":{"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","next":{"output":"\n","previous":{"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","next":{"url":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding.html","relative_path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","path":"_publications/2020-goggles-automatic-image-labeling-with-affinity-coding.md","id":"/publications/2020-goggles-automatic-image-labeling-with-affinity-coding","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Sanya Chaba","Renzhi Wu","Sakshi Gandhi","Duen Horng (Polo) Chau","Xu Chu"],"link":null,"tags":["Learning Settings","Learning Paradigms","Cluster Analysis","Computer Vision","Mathematics Of Computing","Computing Methodologies","Computer Vision Representations","Machine Learning","Unsupervised Learning","Probability And Statistics","Artificial Intelligence","Probabilistic Inference Problems"],"title":"GOGGLES: Automatic Image Labeling with Affinity Coding.","venue":"SIGMOD Conference","year":2020,"slug":"2020-goggles-automatic-image-labeling-with-affinity-coding","ext":".md"},"path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},{"output":"\n","previous":{"output":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","previous":{"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","next":{"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","excerpt":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","collection":"publications","content":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","next":{"output":"\n","previous":{"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","next":{"url":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation.html","relative_path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","path":"_publications/2020-gaggle-visual-analytics-for-model-space-navigation.md","id":"/publications/2020-gaggle-visual-analytics-for-model-space-navigation","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":[],"title":"Gaggle: Visual Analytics for Model Space Navigation.","venue":"Graphics Interface","year":2020,"slug":"2020-gaggle-visual-analytics-for-model-space-navigation","ext":".md"},"path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},{"output":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","next":{"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","collection":"publications","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","next":{"output":"\n","previous":{"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","collection":"publications","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","next":{"url":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.html","relative_path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","path":"_publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests.md","id":"/publications/2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","collection":"publications","draft":false,"categories":[],"authors":["Omar Shaikh","Jiaao Chen","Jon Saad-Falcon","Duen Horng (Polo) Chau","Diyi Yang"],"link":null,"tags":[],"title":"Examining the Ordering of Rhetorical Strategies in Persuasive Requests.","venue":"arXiv","year":2020,"slug":"2020-examining-the-ordering-of-rhetorical-strategies-in-persuasive-requests","ext":".md"},"path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","excerpt":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","collection":"publications","content":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","collection":"publications","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","next":{"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","next":{"output":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","previous":{"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","next":{"url":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.html","relative_path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","path":"_publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger.md","id":"/publications/2020-evaluating-graph-vulnerability-and-robustness-using-tiger","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Evaluating Graph Vulnerability and Robustness using TIGER.","venue":"arXiv","year":2020,"slug":"2020-evaluating-graph-vulnerability-and-robustness-using-tiger","ext":".md"},"path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","excerpt":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","collection":"publications","content":"<p>Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.</p>\n","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},{"output":"\n","previous":{"output":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","previous":{"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","next":{"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","excerpt":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","collection":"publications","content":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","next":{"output":"\n","previous":{"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","next":{"url":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.html","relative_path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","path":"_publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos.md","id":"/publications/2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","collection":"publications","draft":false,"categories":[],"authors":["Haipeng Zeng","Xingbo Wang","Aoyu Wu","Yong Wang 0021","Quan Li","Alex Endert","Huamin Qu"],"link":null,"tags":["Visual Analytics","Videos","Feature Extraction","Emotion Recognition","Coherence","Face"],"title":"EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-emoco-visual-analysis-of-emotion-coherence-in-presentation-videos","ext":".md"},"path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},{"output":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","previous":{"output":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","next":{"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","collection":"publications","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","excerpt":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","next":{"output":"\n","previous":{"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","collection":"publications","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","next":{"url":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.html","relative_path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","path":"_publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification.md","id":"/publications/2020-elf-an-earlyexiting-framework-for-longtailed-classification","collection":"publications","draft":false,"categories":[],"authors":["Rahul Duggal","Scott Freitas","Sunny Dhamnani","Duen Horng (Polo) Chau","Jimeng Sun"],"link":null,"tags":[],"title":"ELF: An Early-Exiting Framework for Long-Tailed Classification.","venue":"arXiv","year":2020,"slug":"2020-elf-an-earlyexiting-framework-for-longtailed-classification","ext":".md"},"path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","excerpt":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","collection":"publications","content":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},{"output":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"output":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","next":{"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","excerpt":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","next":{"output":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","previous":{"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","next":{"url":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.html","relative_path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","path":"_publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks.md","id":"/publications/2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Andrew Wicker","Duen Horng (Polo) Chau","Joshua Neil"],"link":null,"tags":[],"title":"D2M: Dynamic Defense and Modeling of Adversarial Movement in Networks.","venue":"arXiv","year":2020,"slug":"2020-d2m-dynamic-defense-and-modeling-of-adversarial-movement-in-networks","ext":".md"},"path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","excerpt":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","collection":"publications","content":"<p>An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed - Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.</p>\n","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","excerpt":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},{"output":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","next":{"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","next":{"output":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","next":{"url":"/publications/2020-critical-reflections-on-visualization-authoring-systems.html","relative_path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","path":"_publications/2020-critical-reflections-on-visualization-authoring-systems.md","id":"/publications/2020-critical-reflections-on-visualization-authoring-systems","collection":"publications","draft":false,"categories":[],"authors":["Arvind Satyanarayan","Bongshin Lee","Donghao Ren","Jeffrey Heer","John T. Stasko","John Thompson","Matthew Brehmer","Zhicheng Liu"],"link":null,"tags":["Visualization","Data Visualization","Interactive Systems","Libraries","Programming","Authoring Systems","Grammar"],"title":"Critical Reflections on Visualization Authoring Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-critical-reflections-on-visualization-authoring-systems","ext":".md"},"path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","excerpt":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>The success of deep learning solving previously-thought hard problems has inspired many non-experts to learn and understand this exciting technology. However, it is often challenging for learners to take the first steps due to the complexity of deep learning models. We present our ongoing work, CNN 101, an interactive visualization system for explaining and teaching convolutional neural networks. Through tightly integrated interactive views, CNN 101 offers both overview and detailed descriptions of how a model works. Built using modern web technologies, CNN 101 runs locally in users’ web browsers without requiring specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","excerpt":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","next":{"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","next":{"output":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","previous":{"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","next":{"url":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.html","relative_path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","path":"_publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks.md","id":"/publications/2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://zijie.wang/papers/cnn-101/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN 101: Interactive Visual Learning for Convolutional Neural Networks.","venue":"CHI Extended Abstracts","year":2020,"slug":"2020-cnn101-interactive-visual-learning-for-convolutional-neural-networks","ext":".md"},"path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","excerpt":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","collection":"publications","content":"<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques.</p>\n","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},{"output":"\n","previous":{"output":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","previous":{"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","collection":"publications","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","next":{"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","excerpt":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","next":{"output":"\n","previous":{"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","next":{"url":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.html","relative_path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","path":"_publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization.md","id":"/publications/2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zijie J. Wang","Robert Turko","Omar Shaikh","Haekyu Park","Nilaksh Das","Fred Hohman","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":"https://poloclub.github.io/cnn-explainer/","tags":["Machine Learning Education","Deep Learning Visualization","Convolutional Neural Networks"],"title":"CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2020,"slug":"2020-cnn-explainer-learning-convolutional-neural-networks-with-interactive-visualization","ext":".md"},"path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},{"output":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","next":{"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","next":{"output":"\n","previous":{"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","next":{"url":"/publications/2020-characterizing-automated-data-insights.html","relative_path":"_publications/2020-characterizing-automated-data-insights.md","path":"_publications/2020-characterizing-automated-data-insights.md","id":"/publications/2020-characterizing-automated-data-insights","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing Automated Data Insights.","venue":"IEEE VIS (Short Papers)","year":2020,"slug":"2020-characterizing-automated-data-insights","ext":".md"},"path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","excerpt":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","next":{"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","collection":"publications","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","next":{"output":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","previous":{"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","collection":"publications","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","next":{"url":"/publications/2020-causal-perception-in-questionanswering-systems.html","relative_path":"_publications/2020-causal-perception-in-questionanswering-systems.md","path":"_publications/2020-causal-perception-in-questionanswering-systems.md","id":"/publications/2020-causal-perception-in-questionanswering-systems","collection":"publications","draft":false,"categories":[],"authors":["Po-Ming Law","Leo Yu-Ho Lo","Alex Endert","John T. Stasko","Huamin Qu"],"link":null,"tags":[],"title":"Causal Perception in Question-Answering Systems.","venue":"arXiv","year":2020,"slug":"2020-causal-perception-in-questionanswering-systems","ext":".md"},"path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","excerpt":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully-crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model’s internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","collection":"publications","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","next":{"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","next":{"output":"\n","previous":{"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","next":{"url":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.html","relative_path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","path":"_publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks.md","id":"/publications/2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Haekyu Park","Zijie J. Wang","Fred Hohman","Robert Firstman","Emily Rogers","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/bluff","tags":["Deep Learning Visualization","Adversarial Machine Learning","Graph Analytics"],"title":"Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks.","venue":"VIS","year":2020,"slug":"2020-bluff-interactively-deciphering-adversarial-attacks-on-deep-neural-networks","ext":".md"},"path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},{"output":"\n","previous":{"output":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","previous":{"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","collection":"publications","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","next":{"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","excerpt":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","collection":"publications","content":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","next":{"output":"\n","previous":{"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","next":{"url":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.html","relative_path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","path":"_publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems.md","id":"/publications/2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","collection":"publications","draft":false,"categories":[],"authors":["Dong-Kyu Chae","Jihoo Kim","Duen Horng (Polo) Chau","Sang-Wook Kim"],"link":null,"tags":[],"title":"AR-CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems.","venue":"SIGIR","year":2020,"slug":"2020-arcf-augmenting-virtual-users-and-items-in-collaborative-filtering-for-addressing-coldstart-problems","ext":".md"},"path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},{"output":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","collection":"publications","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","next":{"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","next":{"output":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","previous":{"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","next":{"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","excerpt":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","collection":"publications","content":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","excerpt":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","collection":"publications","content":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},{"output":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","previous":{"output":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","previous":{"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","next":{"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","excerpt":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","collection":"publications","content":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","next":{"output":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","previous":{"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","next":{"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","collection":"publications","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","excerpt":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","collection":"publications","content":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","excerpt":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","collection":"publications","content":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},{"output":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","previous":{"output":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","previous":{"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","next":{"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","excerpt":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","collection":"publications","content":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","next":{"output":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","previous":{"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","next":{"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","collection":"publications","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","excerpt":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","collection":"publications","content":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","excerpt":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","collection":"publications","content":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},{"output":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","next":{"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","next":{"output":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","previous":{"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","next":{"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","collection":"publications","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","excerpt":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","collection":"publications","content":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","excerpt":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","collection":"publications","content":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","next":{"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","next":{"output":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","previous":{"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","next":{"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","excerpt":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","collection":"publications","content":"<p>Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or “targets” rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},{"output":"\n","previous":{"output":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","previous":{"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","next":{"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","excerpt":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","collection":"publications","content":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","next":{"output":"\n","previous":{"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","next":{"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},{"output":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","previous":{"output":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","previous":{"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","next":{"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","excerpt":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","collection":"publications","content":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","next":{"output":"\n","previous":{"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","next":{"url":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.html","relative_path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","path":"_publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data.md","id":"/publications/2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Brynne Godfrey","Carleen F. Maitland","Matthew McGee"],"link":null,"tags":[],"title":"The built environment and syrian refugee integration in Turkey: an analysis of mobile phone data.","venue":"GeoHumanities@SIGSPATIAL","year":2019,"slug":"2019-the-built-environment-and-syrian-refugee-integration-in-turkey-an-analysis-of-mobile-phone-data","ext":".md"},"path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","excerpt":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","collection":"publications","content":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},{"output":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","next":{"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","next":{"output":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","previous":{"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","next":{"url":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.html","relative_path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","path":"_publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections.md","id":"/publications/2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Jessica Chang","Alex Endert","Nick Cramer","David Gillen","Shawn D. Hampton","Russ Burtner","Ralph Perko","Kristin A. Cook"],"link":null,"tags":[],"title":"TexTonic: Interactive visualization for exploration and discovery of very large text collections.","venue":"Inf. Vis.","year":2019,"slug":"2019-textonic-interactive-visualization-for-exploration-and-discovery-of-very-large-text-collections","ext":".md"},"path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","excerpt":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","collection":"publications","content":"<p>Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types-Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart-across ten common data analysis tasks using two datasets. We find the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks. We finally train a decision tree on the data we collected to drive a recommender, showcasing how to effectively engineer experimental user data into practical visualization systems.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","excerpt":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","collection":"publications","content":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},{"output":"\n","previous":{"output":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","previous":{"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","collection":"publications","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","next":{"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","excerpt":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","collection":"publications","content":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","next":{"output":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","previous":{"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","next":{"url":"/publications/2019-taskbased-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","path":"_publications/2019-taskbased-effectiveness-of-basic-visualizations.md","id":"/publications/2019-taskbased-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Visualization","Correlation","Data Visualization","Bars","Task Analysis","Automobiles","Motion Pictures"],"title":"Task-Based Effectiveness of Basic Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-taskbased-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","excerpt":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","collection":"publications","content":"<p>Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},{"output":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","next":{"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","next":{"output":"\n","previous":{"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","next":{"url":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.html","relative_path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","path":"_publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations.md","id":"/publications/2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Haekyu Park","Caleb Robinson","Duen Horng (Polo) Chau"],"link":null,"tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-summit-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations","ext":".md"},"path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","excerpt":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","collection":"publications","content":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","next":{"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","collection":"publications","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","next":{"output":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","previous":{"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","collection":"publications","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","next":{"url":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.html","relative_path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","path":"_publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning.md","id":"/publications/2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Sohrab Rahimi"],"link":null,"tags":[],"title":"Place niche and its regional variability: Measuring spatial context patterns for points of interest with representation learning.","venue":"Comput. Environ. Urban Syst.","year":2019,"slug":"2019-place-niche-and-its-regional-variability-measuring-spatial-context-patterns-for-points-of-interest-with-representation-learning","ext":".md"},"path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","excerpt":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","collection":"publications","content":"<p>As deep neural networks are increasingly used in solving highstake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.</p>\n","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","next":{"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","next":{"output":"\n","previous":{"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","next":{"url":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.html","relative_path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","path":"_publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions.md","id":"/publications/2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","collection":"publications","draft":false,"categories":[],"authors":["Haekyu Park","Fred Hohman","Duen Horng (Polo) Chau"],"link":"http://haekyu.com/neural-divergence/","tags":["Machine Learning Interpretability","Deep Learning Visualization","Activation Summarization"],"title":"NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions.","venue":"PacificVis","year":2019,"slug":"2019-neuraldivergence-exploring-and-understanding-neural-networks-by-comparing-activation-distributions","ext":".md"},"path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},{"output":"\n","previous":{"output":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","previous":{"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","next":{"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","excerpt":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","collection":"publications","content":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","next":{"output":"\n","previous":{"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","next":{"url":"/publications/2019-mixed-reality-for-learning-programming.html","relative_path":"_publications/2019-mixed-reality-for-learning-programming.md","path":"_publications/2019-mixed-reality-for-learning-programming.md","id":"/publications/2019-mixed-reality-for-learning-programming","collection":"publications","draft":false,"categories":[],"authors":["Joonyoung Kim","Kristina Marotta","Jonathan Leo","Sudeep Agarwal","Siwei Li","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Mixed Reality for Learning Programming.","venue":"IDC","year":2019,"slug":"2019-mixed-reality-for-learning-programming","ext":".md"},"path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},{"output":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","next":{"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","next":{"output":"\n","previous":{"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","next":{"url":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.html","relative_path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","path":"_publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks.md","id":"/publications/2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Dipto Sarkar","Clio Andris","Colin A. Chapman","Raja R. Sengupta"],"link":null,"tags":[],"title":"Metrics for characterizing network structure and node importance in Spatial Social Networks.","venue":"Int. J. Geogr. Inf. Sci.","year":2019,"slug":"2019-metrics-for-characterizing-network-structure-and-node-importance-in-spatial-social-networks","ext":".md"},"path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","excerpt":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","collection":"publications","content":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","collection":"publications","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","next":{"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","next":{"output":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","previous":{"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","next":{"url":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.html","relative_path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","path":"_publications/2019-liger-combining-interaction-paradigms-for-visual-analysis.md","id":"/publications/2019-liger-combining-interaction-paradigms-for-visual-analysis","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Lei Jiang","Charles Perin","Alex Endert"],"link":null,"tags":[],"title":"Liger: Combining Interaction Paradigms for Visual Analysis.","venue":"arXiv","year":2019,"slug":"2019-liger-combining-interaction-paradigms-for-visual-analysis","ext":".md"},"path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","excerpt":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","collection":"publications","content":"<p>Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two‐phase study comparing people’s visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade‐offs and open challenges with these interaction paradigms.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","next":{"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","next":{"output":"\n","previous":{"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","next":{"url":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.html","relative_path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","path":"_publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction.md","id":"/publications/2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":["Human Computer Interaction","Visual Analytics","Humancentered Computing"],"title":"Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-investigating-the-manual-view-specification-and-visualization-by-demonstration-paradigms-for-visualization-construction","ext":".md"},"path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","next":{"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","collection":"publications","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","next":{"output":"\n","previous":{"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","collection":"publications","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","next":{"url":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.html","relative_path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","path":"_publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161.md","id":"/publications/2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Jean-Daniel Fekete","Bongshin Lee","Shixia Liu"],"link":null,"tags":[],"title":"Interactive Visualization for Interpretable Machine Learning ~ Beyond Visualization and Steering of the Parametric Space (NII Shonan Meeting 161).","venue":"NII Shonan Meet. Rep.","year":2019,"slug":"2019-interactive-visualization-for-interpretable-machine-learning--beyond-visualization-and-steering-of-the-parametric-space-nii-shonan-meeting-161","ext":".md"},"path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},{"output":"\n","previous":{"output":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","previous":{"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","collection":"publications","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","next":{"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","excerpt":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","collection":"publications","content":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","next":{"output":"\n","previous":{"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","next":{"url":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.html","relative_path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","path":"_publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning.md","id":"/publications/2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","collection":"publications","draft":false,"categories":[],"authors":["Xi Liu","Clio Andris","Zixuan Huang","Sohrab Rahimi"],"link":null,"tags":[],"title":"Inside 50, 000 living rooms: an assessment of global residential ornamentation using transfer learning.","venue":"EPJ Data Sci.","year":2019,"slug":"2019-inside-50-000-living-rooms-an-assessment-of-global-residential-ornamentation-using-transfer-learning","ext":".md"},"path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},{"output":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","previous":{"output":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","previous":{"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","collection":"publications","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","next":{"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","excerpt":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","collection":"publications","content":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","next":{"output":"\n","previous":{"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","next":{"url":"/publications/2019-geospatial-privacy-and-security.html","relative_path":"_publications/2019-geospatial-privacy-and-security.md","path":"_publications/2019-geospatial-privacy-and-security.md","id":"/publications/2019-geospatial-privacy-and-security","collection":"publications","draft":false,"categories":[],"authors":["Grant McKenzie","Carsten Ke","Clio Andris"],"link":null,"tags":[],"title":"Geospatial Privacy and Security.","venue":"J. Spatial Inf. Sci.","year":2019,"slug":"2019-geospatial-privacy-and-security","ext":".md"},"path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","excerpt":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","collection":"publications","content":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},{"output":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","previous":{"output":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","previous":{"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","next":{"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","collection":"publications","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","excerpt":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","collection":"publications","content":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","next":{"output":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","previous":{"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","collection":"publications","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","next":{"url":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.html","relative_path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","path":"_publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists.md","id":"/publications/2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Subhajit Das","Bum Chul Kwon","Alex Endert"],"link":null,"tags":[],"title":"Geono-Cluster: Interactive Visual Cluster Analysis for Biologists.","venue":"arXiv","year":2019,"slug":"2019-geonocluster-interactive-visual-cluster-analysis-for-biologists","ext":".md"},"path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","excerpt":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","collection":"publications","content":"<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’s coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","excerpt":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","collection":"publications","content":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},{"output":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","previous":{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","next":{"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","collection":"publications","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","next":{"output":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","previous":{"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","collection":"publications","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","next":{"url":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.html","relative_path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","path":"_publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning.md","id":"/publications/2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/fairvis","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning.","venue":"VAST","year":2019,"slug":"2019-fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning","ext":".md"},"path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","excerpt":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","collection":"publications","content":"<p>Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.</p>\n","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","excerpt":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","collection":"publications","content":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"output":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","previous":{"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","next":{"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","excerpt":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","collection":"publications","content":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","next":{"output":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","previous":{"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","next":{"url":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.html","relative_path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","path":"_publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping.md","id":"/publications/2019-embedded-merge--split-visual-adjustment-of-data-grouping","collection":"publications","draft":false,"categories":[],"authors":["Ali Sarvghad","Bahador Saket","Alex Endert","Nadir Weibel"],"link":null,"tags":["Visualization","Data Visualization","Synthetic Aperture Sonar","Data Analysis","Bars","Tools","Histograms"],"title":"Embedded Merge  Split: Visual Adjustment of Data Grouping.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-embedded-merge--split-visual-adjustment-of-data-grouping","ext":".md"},"path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","excerpt":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","collection":"publications","content":"<p>In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract “features” that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.</p>\n","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},{"output":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","previous":{"output":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","previous":{"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","next":{"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","excerpt":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","collection":"publications","content":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","next":{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","next":{"url":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.html","relative_path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","path":"_publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features.md","id":"/publications/2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","collection":"publications","draft":false,"categories":[],"authors":["Xiangyun Lei","Fred Hohman","Duen Horng (Polo) Chau","Andrew J. Medford"],"link":"https://fredhohman.com/papers/electrolens","tags":["Visual Analytics","3D Visualization","Visualization Application"],"title":"ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features.","venue":"VIS","year":2019,"slug":"2019-electrolens-understanding-atomistic-simulations-through-spatiallyresolved-visualization-of-highdimensional-features","ext":".md"},"path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","excerpt":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","collection":"publications","content":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},{"output":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","previous":{"output":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","previous":{"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","next":{"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","excerpt":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","collection":"publications","content":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","next":{"output":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","previous":{"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","next":{"url":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.html","relative_path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","path":"_publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation.md","id":"/publications/2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Will Epperson","Fred Hohman","Minsuk Kahng","Jamie Morgenstern","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/subgroup-gen","tags":["Visual Analytics","Machine Learning Fairness","Subgroup Analysis"],"title":"Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation.","venue":"DebugML @ ICML","year":2019,"slug":"2019-discovery-of-intersectional-bias-in-machine-learning-using-automatic-subgroup-generation","ext":".md"},"path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","excerpt":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","collection":"publications","content":"<p>Recently, there has been an increasing trend to extend the demonstrational interaction paradigm to visualization tools. As more analytic operations can be performed by demonstration, new user tasks can be supported. In this paper, we discuss the properties of tasks where the by-demonstration paradigm can be effective and describe the main components needed to implement the demonstrational paradigm in visualization tools.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","excerpt":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","collection":"publications","content":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},{"output":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","collection":"publications","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","next":{"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","next":{"output":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","previous":{"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","next":{"url":"/publications/2019-demonstrational-interaction-for-data-visualization.html","relative_path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","path":"_publications/2019-demonstrational-interaction-for-data-visualization.md","id":"/publications/2019-demonstrational-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Visualization","Data Visualization"],"title":"Demonstrational Interaction for Data Visualization.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-demonstrational-interaction-for-data-visualization","ext":".md"},"path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","excerpt":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","collection":"publications","content":"<p>Promoting a wider range of contribution types can facilitate healthy growth of the visualization community, while increasing the intellectual diversity of visualization research papers. In this paper, we discuss the importance of contribution types and summarize contribution types that can be meaningful in visualization research. We also propose several concrete next steps we can and should take to ensure a successful launch of the contribution types.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","excerpt":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","collection":"publications","content":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},{"output":"\n","previous":{"output":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","previous":{"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","next":{"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","excerpt":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","collection":"publications","content":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","next":{"output":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","previous":{"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","next":{"url":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.html","relative_path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","path":"_publications/2019-broadening-intellectual-diversity-in-visualization-research-papers.md","id":"/publications/2019-broadening-intellectual-diversity-in-visualization-research-papers","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Kate Isaacs","Danielle Albers Szafir","G. Elisabeta Marai","Cagatay Turkay","Melanie Tory","Sheelagh Carpendale","Alex Endert","Theresa-Marie Rhyne"],"link":null,"tags":["Cognition","Visualization","Data Visualization","Analytical Models","Task Analysis","Lenses"],"title":"Broadening Intellectual Diversity in Visualization Research Papers.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-broadening-intellectual-diversity-in-visualization-research-papers","ext":".md"},"path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","excerpt":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","collection":"publications","content":"<p>Interactive model steering helps people incrementally build machine learning models that are tailored to their domain and task. Existing visual analytic tools allow people to steer a single model (e.g., assignment attribute weights used by a dimension reduction model). However, the choice of model is critical in such situations. What if the model chosen is suboptimal for the task, dataset, or question being asked? What if instead of parameterizing and steering this model, a different model provides a better fit? This paper presents a technique to allow users to inspect and steer multiple machine learning models. The technique steers and samples models from a broader set of learning algorithms and model types. We incorporate this technique into a visual analytic prototype, BEAMES, that allows users to perform regression tasks via multimodel steering. This paper demonstrates the effectiveness of BEAMES via a use case, and discusses broader implications for multimodel steering.</p>\n","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},{"output":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","previous":{"output":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","previous":{"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","collection":"publications","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","next":{"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","collection":"publications","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","excerpt":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","collection":"publications","content":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","next":{"output":"\n","previous":{"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","collection":"publications","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","next":{"url":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.html","relative_path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","path":"_publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks.md","id":"/publications/2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","collection":"publications","draft":false,"categories":[],"authors":["Subhajit Das","Dylan Cashman","Remco Chang","Alex Endert"],"link":null,"tags":["Visual Analytics","Machine Learning","Analytical Models","Computational Modeling","Task Analysis","Inspection","Data Models"],"title":"BEAMES: Interactive Multimodel Steering, Selection, and Inspection for Regression Tasks.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-beames-interactive-multimodel-steering-selection-and-inspection-for-regression-tasks","ext":".md"},"path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","excerpt":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","collection":"publications","content":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},{"output":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","previous":{"output":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","previous":{"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","next":{"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","excerpt":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","collection":"publications","content":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","next":{"output":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","previous":{"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","next":{"url":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.html","relative_path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","path":"_publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication.md","id":"/publications/2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Steven Mark Drucker","Alex Endert","John T. Stasko"],"link":null,"tags":[],"title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-augmenting-visualizations-with-interactive-data-facts-to-facilitate-interpretation-and-communication","ext":".md"},"path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","excerpt":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","collection":"publications","content":"<p>Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced.</p>\n","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","excerpt":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","collection":"publications","content":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},{"output":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","previous":{"output":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","previous":{"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","next":{"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","collection":"publications","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","excerpt":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","collection":"publications","content":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","next":{"output":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","previous":{"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","collection":"publications","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","next":{"url":"/publications/2019-atlas-local-graph-exploration-in-a-global-context.html","relative_path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","path":"_publications/2019-atlas-local-graph-exploration-in-a-global-context.md","id":"/publications/2019-atlas-local-graph-exploration-in-a-global-context","collection":"publications","draft":false,"categories":[],"authors":["James Abello","Fred Hohman","Varun Bezzam","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/atlas","tags":["Visual Graph Analytics","Graph Algorithms","Interactive Graph Decomposition"],"title":"Atlas: Local Graph Exploration in a Global Context.","venue":"IUI","year":2019,"slug":"2019-atlas-local-graph-exploration-in-a-global-context","ext":".md"},"path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","excerpt":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","collection":"publications","content":"<p>Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well‐known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.</p>\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","excerpt":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","collection":"publications","content":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},{"output":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","previous":{"output":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","previous":{"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","next":{"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","excerpt":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","collection":"publications","content":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","next":{"output":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","previous":{"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2019-a-provenance-task-abstraction-framework.html","relative_path":"_publications/2019-a-provenance-task-abstraction-framework.md","next":{"url":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.html","relative_path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","path":"_publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis.md","id":"/publications/2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":["Visual Analytics","Model Development And Analysis","Computing Methodologies","Mathematics Of Computing","Humancentered Computing","Exploratory Data Analysis"],"title":"A User-based Visual Analytics Workflow for Exploratory Model Analysis.","venue":"Comput. Graph. Forum","year":2019,"slug":"2019-a-userbased-visual-analytics-workflow-for-exploratory-model-analysis","ext":".md"},"path":"_publications/2019-a-provenance-task-abstraction-framework.md","id":"/publications/2019-a-provenance-task-abstraction-framework","excerpt":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","collection":"publications","content":"<p>Visual analytics tools integrate provenance recording to externalize analytic processes or user insights. Provenance can be captured on varying levels of detail, and in turn activities can be characterized from different granularities. However, current approaches do not support inferring activities that can only be characterized across multiple levels of provenance. We propose a task abstraction framework that consists of a three stage approach, composed of 1) initializing a provenance task hierarchy, 2) parsing the provenance hierarchy by using an abstraction mapping mechanism, and 3) leveraging the task hierarchy in an analytical tool. Furthermore, we identify implications to accommodate iterative refinement, context, variability, and uncertainty during all stages of the framework. We describe a use case which exemplifies our abstraction framework, demonstrating how context can influence the provenance hierarchy to support analysis. The article concludes with an agenda, raising and discussing challenges that need to be considered for successfully implementing such a framework.</p>\n","draft":false,"categories":[],"authors":["Christian Bors","John E. Wenskovitch","Michelle Dowling","Simon Attfield","Leilani Battle","Alex Endert","Olga Kulyk","Robert S. Laramee"],"link":null,"tags":["Cognition","Visualization","Data Visualization","History","Analytical Models","Task Analysis"],"title":"A Provenance Task Abstraction Framework.","venue":"IEEE Computer Graphics and Applications","year":2019,"slug":"2019-a-provenance-task-abstraction-framework","ext":".md"},"path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","excerpt":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","collection":"publications","content":"<p>Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization’s value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},{"output":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","previous":{"output":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","previous":{"url":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.html","relative_path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","path":"_publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling.md","id":"/publications/2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Dongjin Choi","Barry L. Drake","Alex Endert","Haesun Park"],"link":null,"tags":["Retrieved Subset","Computational Modeling","Simple Keyword Matching Search","Visual Analytics","Large-scale Document Retrieval","Document Collections","Topicsifter","Data Analysis","Document Handling","Missed Relevant Documents","Query Processing","Irrelevant Documents","Keyword Queries","Interactive Search Space Reduction","Data Visualisation","Negative Feedback","Relevance Feedback","Targeted Topic Modeling","Buildings","Matrix Decomposition","Search Problems"],"title":"TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling.","venue":"VAST","year":2019,"slug":"2019-topicsifter-interactive-search-space-reduction-through-targeted-topic-modeling","ext":".md"},"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","next":{"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","collection":"publications","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","excerpt":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","collection":"publications","content":"<p>The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user’s cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","next":{"output":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","previous":{"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","collection":"publications","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","next":{"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","collection":"publications","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","excerpt":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","collection":"publications","content":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","excerpt":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","collection":"publications","content":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},{"output":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","previous":{"output":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","previous":{"url":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.html","relative_path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","path":"_publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis.md","id":"/publications/2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","John T. Stasko","Alex Endert"],"link":null,"tags":["Cognitive Biases","Cognition","Visualization","Cognitive Processes","Data Visualization","Data Analysis","Bias Mitigation Strategies","Design Space","Behavioural Sciences Computing","Task Analysis","Visual Analytic Tools","Decision Making","Cognitive Heuristics","Tools","Training","Data Visualisation"],"title":"Toward a Design Space for Mitigating Cognitive Bias in Vis.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-toward-a-design-space-for-mitigating-cognitive-bias-in-vis","ext":".md"},"url":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.html","relative_path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","next":{"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","collection":"publications","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"path":"_publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking.md","id":"/publications/2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","excerpt":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","collection":"publications","content":"<p>In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</p>\n","draft":false,"categories":[],"authors":["Julia Deeb-Swihart","Alex Endert","Amy S. Bruckman"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking.","venue":"CHI","year":2019,"slug":"2019-understanding-law-enforcement-strategies-and-needs-for-combating-human-trafficking","ext":".md"},"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","next":{"output":"\n","previous":{"url":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.html","relative_path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","collection":"publications","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},"url":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.html","relative_path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","next":{"url":"/publications/2020-a-largescale-database-for-graph-representation-learning.html","relative_path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","path":"_publications/2020-a-largescale-database-for-graph-representation-learning.md","id":"/publications/2020-a-largescale-database-for-graph-representation-learning","collection":"publications","draft":false,"categories":[],"authors":["Scott Freitas","Yuxiao Dong","Joshua Neil","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Large-Scale Database for Graph Representation Learning.","venue":"arXiv","year":2020,"slug":"2020-a-largescale-database-for-graph-representation-learning","ext":".md"},"path":"_publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines.md","id":"/publications/2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Austin P. Wright","Zijie J. Wang","Haekyu Park","Grace Guo","Fabian Sperrle","Mennatallah El-Assady","Alex Endert","Daniel A. Keim","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Comparative Analysis of Industry Human-AI Interaction Guidelines.","venue":"arXiv","year":2020,"slug":"2020-a-comparative-analysis-of-industry-humanai-interaction-guidelines","ext":".md"},"path":"_publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays.md","id":"/publications/2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","excerpt":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","collection":"publications","content":"<p>An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Mallika Agarwal","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Tiles","Strips","Motion Pictures","Tools","Histograms"],"title":"VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays.","venue":"IEEE VIS (Short Papers)","year":2019,"slug":"2019-viswall-visual-data-exploration-using-direct-combination-on-large-touch-displays","ext":".md"},{"output":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","previous":{"output":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","previous":{"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","next":{"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","excerpt":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","collection":"publications","content":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","next":{"output":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","previous":{"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","next":{"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","excerpt":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","collection":"publications","content":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","excerpt":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","collection":"publications","content":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},{"output":"\n","previous":{"output":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","previous":{"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","collection":"publications","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","next":{"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","excerpt":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","collection":"publications","content":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","next":{"output":"\n","previous":{"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","next":{"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},{"output":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","next":{"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","next":{"output":"\n","previous":{"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","next":{"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","excerpt":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","collection":"publications","content":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","next":{"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","collection":"publications","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","next":{"output":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","previous":{"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","collection":"publications","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","next":{"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","excerpt":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","collection":"publications","content":"<p>Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR’s ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},{"output":"\n","previous":{"output":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","previous":{"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","next":{"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","excerpt":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","collection":"publications","content":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","next":{"output":"\n","previous":{"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","next":{"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},{"output":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","previous":{"output":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","previous":{"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","next":{"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","excerpt":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","collection":"publications","content":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","next":{"output":"\n","previous":{"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","next":{"url":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.html","relative_path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","path":"_publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china.md","id":"/publications/2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","collection":"publications","draft":false,"categories":[],"authors":["Mengxue Yue","Chaogui Kang","Clio Andris","Kun Qin","Yu Liu","Qingxiang Meng"],"link":null,"tags":[],"title":"Understanding the interplay between bus, metro, and cab ridership dynamics in Shenzhen, China.","venue":"Trans. GIS","year":2018,"slug":"2018-understanding-the-interplay-between-bus-metro-and-cab-ridership-dynamics-in-shenzhen-china","ext":".md"},"path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","excerpt":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","collection":"publications","content":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},{"output":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","previous":{"output":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","previous":{"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","collection":"publications","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","next":{"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","excerpt":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","next":{"output":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","previous":{"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","next":{"url":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.html","relative_path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","path":"_publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers.md","id":"/publications/2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Meeshu Agnihotri","John T. Stasko"],"link":null,"tags":[],"title":"Touching Data: A Discoverability-based Evaluation of a Visualization Interface for Tablet Computers.","venue":"arXiv","year":2018,"slug":"2018-touching-data-a-discoverabilitybased-evaluation-of-a-visualization-interface-for-tablet-computers","ext":".md"},"path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","excerpt":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","collection":"publications","content":"<p>Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.</p>\n","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","excerpt":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","collection":"publications","content":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},{"output":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","previous":{"output":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","previous":{"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","next":{"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","excerpt":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","collection":"publications","content":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","next":{"output":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","previous":{"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","next":{"url":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.html","relative_path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","path":"_publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures.md","id":"/publications/2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","collection":"publications","draft":false,"categories":[],"authors":["John Thompson","Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Interaction Techniques","Visualization","Visualization Techniques","Graph Drawings","Human-centered Computing","Gestural Input","Human Computer Interaction (hci)"],"title":"Tangraphe: interactive exploration of network visualizations using single hand, multi-touch gestures.","venue":"AVI","year":2018,"slug":"2018-tangraphe-interactive-exploration-of-network-visualizations-using-single-hand-multitouch-gestures","ext":".md"},"path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","excerpt":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","collection":"publications","content":"<p>In this report, we organize and reflect on recent advances and challenges in the field of sports data visualization. The exponentially‐growing body of visualization research based on sports data is a prime indication of the importance and timeliness of this report. Sports data visualization research encompasses the breadth of visualization tasks and goals: exploring the design of new visualization techniques; adapting existing visualizations to a novel domain; and conducting design studies and evaluations in close collaboration with experts, including practitioners, enthusiasts, and journalists. Frequently this research has impact beyond sports in both academia and in industry because it is i) grounded in realistic, highly heterogeneous data, ii) applied to real‐world problems, and iii) designed in close collaboration with domain experts. In this report, we analyze current research contributions through the lens of three categories of sports data: box score data (data containing statistical summaries of a sport event such as a game), tracking data (data about in‐game actions and trajectories), and meta‐data (data about the sport and its participants but not necessarily a given game). We conclude this report with a high‐level discussion of sports visualization research informed by our analysis—identifying critical research gaps and valuable opportunities for the visualization community. More information is available at the STAR’s website: https://sportsdataviz.github.io/.</p>\n","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","excerpt":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},{"output":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","next":{"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","collection":"publications","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","next":{"output":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","previous":{"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","collection":"publications","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","next":{"url":"/publications/2018-state-of-the-art-of-sports-data-visualization.html","relative_path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","path":"_publications/2018-state-of-the-art-of-sports-data-visualization.md","id":"/publications/2018-state-of-the-art-of-sports-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles Perin","Romain Vuillemot","Charles D. Stolper","John T. Stasko","Jo Wood","Sheelagh Carpendale"],"link":null,"tags":[],"title":"State of the Art of Sports Data Visualization.","venue":"Comput. Graph. Forum","year":2018,"slug":"2018-state-of-the-art-of-sports-data-visualization","ext":".md"},"path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","excerpt":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed SHIELD defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, SHIELD “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, SHIELD adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes SHIELD a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","excerpt":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","collection":"publications","content":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},{"output":"\n","previous":{"output":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","previous":{"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","excerpt":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","collection":"publications","content":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","next":{"output":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","previous":{"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","next":{"url":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.html","relative_path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","path":"_publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression.md","id":"/publications/2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Siwei Li","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shield","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"SHIELD: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.","venue":"KDD","year":2018,"slug":"2018-shield-fast-practical-defense-and-vaccination-for-deep-learning-using-jpeg-compression","ext":".md"},"path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","excerpt":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","collection":"publications","content":"<p>We are developing an interactive graph exploration system called Graph Playground for making sense of large graphs. Graph Playground offers a fast and scalable edge decomposition algorithm, based on iterative vertex-edge peeling, to decompose million-edge graphs in seconds. Graph Playground introduces a novel graph exploration approach and a 3D representation framework that simultaneously reveals (1) peculiar subgraph structure discovered through the decomposition’s layers, (e.g., quasi-cliques), and (2) possible vertex roles in linking such subgraph patterns across layers.</p>\n","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},{"output":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","previous":{"output":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","previous":{"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","next":{"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","excerpt":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","collection":"publications","content":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","next":{"url":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.html","relative_path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","path":"_publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure.md","id":"/publications/2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","collection":"publications","draft":false,"categories":[],"authors":["Alok Tripathy","Fred Hohman","Duen Horng (Polo) Chau","Oded Green"],"link":"https://fredhohman.com/papers/kcore","tags":["Graph Analytics","Graph Decomposition","Scalable Graph Algorithms"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure.","venue":"IEEE BigData","year":2018,"slug":"2018-scalable-kcore-decomposition-for-static-graphs-using-a-dynamic-graph-data-structure","ext":".md"},"path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","excerpt":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","collection":"publications","content":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},{"output":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","previous":{"output":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","previous":{"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","next":{"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","excerpt":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","collection":"publications","content":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","next":{"output":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","previous":{"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.html","relative_path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","path":"_publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector.md","id":"/publications/2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Cory Cornelius","Jason Martin","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Robust Physical Adversarial Attack on Faster R-CNN Object Detector.","venue":"arXiv","year":2018,"slug":"2018-robust-physical-adversarial-attack-on-faster-rcnn-object-detector","ext":".md"},"path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","excerpt":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","collection":"publications","content":"<p>People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user’s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user’s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","excerpt":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","collection":"publications","content":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},{"output":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","previous":{"output":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","previous":{"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","next":{"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","excerpt":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","collection":"publications","content":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","next":{"output":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","previous":{"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","next":{"url":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.html","relative_path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","path":"_publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics.md","id":"/publications/2018-podium-ranking-data-using-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Subhajit Das","Ravish Chawla","Bharath Kalidindi","Eli T. Brown","Alex Endert"],"link":null,"tags":["Support Vector Machines","Visual Analytics","Data Visualization","Computational Modeling","Prototypes","Data Models"],"title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-podium-ranking-data-using-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","excerpt":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","collection":"publications","content":"<p>Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","excerpt":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","collection":"publications","content":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},{"output":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","previous":{"output":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","previous":{"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","next":{"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","excerpt":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","collection":"publications","content":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","next":{"output":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","previous":{"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","next":{"url":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.html","relative_path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","path":"_publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks.md","id":"/publications/2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Speech","Taxonomy","Prototypes","Natural Languages","Mice"],"title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-orko-facilitating-multimodal-interaction-for-visual-exploration-and-analysis-of-networks","ext":".md"},"path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","excerpt":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","collection":"publications","content":"<p>Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.</p>\n","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","excerpt":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","collection":"publications","content":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},{"output":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","previous":{"output":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","previous":{"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","next":{"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","excerpt":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","collection":"publications","content":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","next":{"output":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","previous":{"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","next":{"url":"/publications/2018-multimodal-interaction-for-data-visualization.html","relative_path":"_publications/2018-multimodal-interaction-for-data-visualization.md","path":"_publications/2018-multimodal-interaction-for-data-visualization.md","id":"/publications/2018-multimodal-interaction-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bongshin Lee","Arjun Srinivasan","John T. Stasko","Melanie Tory","Vidya Setlur"],"link":null,"tags":["Visualization","Human-centered Computing","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Multimodal interaction for data visualization.","venue":"AVI","year":2018,"slug":"2018-multimodal-interaction-for-data-visualization","ext":".md"},"path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","excerpt":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","collection":"publications","content":"<p>We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms to obtain new classifications in real time. This system allows users to compare and contrast what image regions humans and machine learning models use for classification.</p>\n","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","excerpt":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","collection":"publications","content":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},{"output":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","next":{"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","next":{"output":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","previous":{"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","next":{"url":"/publications/2018-interactive-classification-for-deep-learning-interpretation.html","relative_path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","path":"_publications/2018-interactive-classification-for-deep-learning-interpretation.md","id":"/publications/2018-interactive-classification-for-deep-learning-interpretation","collection":"publications","draft":false,"categories":[],"authors":["Alex Cabrera","Fred Hohman","Jason Lin","Duen Horng (Polo) Chau"],"link":"https://cabreraalex.com/#/paper/interactive-classification","tags":["Machine Learning Interpretability","Computer Vision"],"title":"Interactive Classification for Deep Learning Interpretation.","venue":"CVPR, Demo","year":2018,"slug":"2018-interactive-classification-for-deep-learning-interpretation","ext":".md"},"path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","excerpt":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","collection":"publications","content":"<p>Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.</p>\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","excerpt":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","collection":"publications","content":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","previous":{"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","next":{"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","excerpt":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","collection":"publications","content":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","next":{"output":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","previous":{"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","next":{"url":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.html","relative_path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","path":"_publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization.md","id":"/publications/2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","Hyunwoo Park","Alex Endert","Rahul C. Basole"],"link":null,"tags":["Data Visualization","Computational Modeling","Prototypes","Joining Processes","Image Color Analysis","Data Models","Tools"],"title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-graphiti-interactive-specification-of-attributebased-edges-for-network-modeling-and-visualization","ext":".md"},"path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","excerpt":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","collection":"publications","content":"<p>Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process’s intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN’s structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.</p>\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},{"output":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","next":{"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","next":{"url":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.html","relative_path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","path":"_publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation.md","id":"/publications/2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Nikhil Thorat","Duen Horng (Polo) Chau","Fernanda B. Vi","Martin Wattenberg"],"link":null,"tags":["Tools","Data Visualization","Machine Learning","Training","Gallium Nitride","Generative Adversarial Networks","Generators"],"title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation.","venue":"arXiv","year":2018,"slug":"2018-gan-lab-understanding-complex-deep-generative-models-using-interactive-visual-experimentation","ext":".md"},"path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","excerpt":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","collection":"publications","content":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","previous":{"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","next":{"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","excerpt":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","collection":"publications","content":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","next":{"output":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","previous":{"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","next":{"url":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study.html","relative_path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","path":"_publications/2018-fraud-detection-using-social-network-analysis-a-case-study.md","id":"/publications/2018-fraud-detection-using-social-network-analysis-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Fraud Detection Using Social Network Analysis: A Case Study.","venue":"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.","year":2018,"slug":"2018-fraud-detection-using-social-network-analysis-a-case-study","ext":".md"},"path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","excerpt":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","collection":"publications","content":"<p>Visual analytic systems, especially mixed-initiative systems, can steer analytical models and adapt views by making inferences from users’ behavioral patterns with the system. Because such systems rely on incorporating implicit and explicit user feedback, they are particularly susceptible to the injection and propagation of human biases. To ultimately guard against the potentially negative effects of systems biased by human users, we must first qualify what we mean by the term bias. Thus, in this chapter we describe four different perspectives on human bias that are particularly relevant to visual analytics. We discuss the interplay of human and computer system biases, particularly their roles in mixed-initiative systems. Given that the term bias is used to describe several different concepts, our goal is to facilitate a common language in research and development efforts by encouraging researchers to mindfully choose the perspective(s) considered in their work.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},{"output":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","previous":{"output":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","previous":{"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","next":{"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","excerpt":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","collection":"publications","content":"<p>Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers’ workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.html","relative_path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},"url":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.html","relative_path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","next":{"url":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics.html","relative_path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","path":"_publications/2018-four-perspectives-on-human-bias-in-visual-analytics.md","id":"/publications/2018-four-perspectives-on-human-bias-in-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Kristin A. Cook","Alex Endert"],"link":null,"tags":[],"title":"Four Perspectives on Human Bias in Visual Analytics.","venue":"Cognitive Biases in Visualizations","year":2018,"slug":"2018-four-perspectives-on-human-bias-in-visual-analytics","ext":".md"},"path":"_publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification.md","id":"/publications/2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Evaluation of Visualization by Demonstration and Manual View Specification.","venue":"arXiv","year":2018,"slug":"2018-evaluation-of-visualization-by-demonstration-and-manual-view-specification","ext":".md"},"path":"_publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization.md","id":"/publications/2018-evaluating-interactive-graphical-encodings-for-data-visualization","excerpt":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","collection":"publications","content":"<p>User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Arjun Srinivasan","Eric D. Ragan","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Encoding","Computational Modeling","Estimation"],"title":"Evaluating Interactive Graphical Encodings for Data Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-evaluating-interactive-graphical-encodings-for-data-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.html","relative_path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","path":"_publications/2018-vigor-interactive-visual-exploration-of-graph-query-results.md","id":"/publications/2018-vigor-interactive-visual-exploration-of-graph-query-results","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Alex Endert","Acar Tamersoy","Kevin A. Roundy","Christopher S. Gates","Shamkant B. Navathe","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/vigor","tags":["Visual Analytics","Interactive Graph Querying","Graph Query Summarization"],"title":"VIGOR: Interactive Visual Exploration of Graph Query Results.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-vigor-interactive-visual-exploration-of-graph-query-results","ext":".md"},"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","next":{"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","next":{"output":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","previous":{"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","collection":"publications","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","next":{"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","excerpt":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","collection":"publications","content":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},{"output":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.html","relative_path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","path":"_publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data.md","id":"/publications/2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hannah Kim","Edward Clarkson","Zhicheng Liu","Changhyun Lee","Fuxin Li","Hanseung Lee","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data.","venue":"ACM Trans. Knowl. Discov. Data","year":2018,"slug":"2018-visirr-a-visual-analytics-system-for-information-retrieval-and-recommendation-for-largescale-document-data","ext":".md"},"url":"/publications/2018-visual-analytics-for-automated-model-discovery.html","relative_path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","next":{"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"path":"_publications/2018-visual-analytics-for-automated-model-discovery.md","id":"/publications/2018-visual-analytics-for-automated-model-discovery","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dylan Cashman","Shah Rukh Humayoun","Florian Heimerl","Kendall Park","Subhajit Das","John Thompson","Bahador Saket","Abigail Mosca","John T. Stasko","Alex Endert","Michael Gleicher","Remco Chang"],"link":null,"tags":[],"title":"Visual Analytics for Automated Model Discovery.","venue":"arXiv","year":2018,"slug":"2018-visual-analytics-for-automated-model-discovery","ext":".md"},"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","next":{"output":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","previous":{"url":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.html","relative_path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},"url":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.html","relative_path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","next":{"url":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","path":"_publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations.md","id":"/publications/2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Meeshu Agnihotri","Laura E. Matzen","Kristin Divis","Michael Haass","Alex Endert","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Usability","Guidelines","Task Analysis","Benchmark Testing","Tools"],"title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2019,"slug":"2019-a-heuristic-approach-to-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias.md","id":"/publications/2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","excerpt":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","collection":"publications","content":"<p>Interaction is the cornerstone of how people perform tasks and gain insight in visual analytics. However, people’s inherent cognitive biases impact their behavior and decision making during their interactive visual analytic process. Understanding how bias impacts the visual analytic process, how it can be measured, and how its negative effects can be mitigated is a complex problem space. Nonetheless, recent work has begun to approach this problem by proposing theoretical computational metrics that are applied to user interaction sequences to measure bias in real-time. In this paper, we implement and apply these computational metrics in the context of anchoring bias. We present the results of a formative study examining how the metrics can capture anchoring bias in real-time during a visual analytic task. We present lessons learned in the form of considerations for applying the metrics in a visual analytic tool. Our findings suggest that these computational metrics are a promising approach for characterizing bias in users’ interactive behaviors.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Celeste Lyn Paul","Alex Endert"],"link":null,"tags":["Visual Analytics","Cognitive Bias","Anchoring Bias"],"title":"A Formative Study of Interactive Bias Metrics in Visual Analytics Using Anchoring Bias.","venue":"INTERACT (2)","year":2019,"slug":"2019-a-formative-study-of-interactive-bias-metrics-in-visual-analytics-using-anchoring-bias","ext":".md"},"path":"_publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers.md","id":"/publications/2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","excerpt":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","collection":"publications","content":"<p>Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Minsuk Kahng","Robert Pienta","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/visual-analytics-in-deep-learning/","tags":["Deep Learning Visualization","Neural Network Interpretability","Survey Paper"],"title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2018,"slug":"2018-visual-analytics-in-deep-learning-an-interrogative-survey-for-the-next-frontiers","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","next":{"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","next":{"output":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","previous":{"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","next":{"url":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.html","relative_path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","path":"_publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring.md","id":"/publications/2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John Thompson","Alan Wilson","Mira Dontcheva","James Delorey","Sam Grigg","Bernard Kerr","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring.","venue":"CHI","year":2018,"slug":"2018-data-illustrator-augmenting-vector-design-tools-with-lazy-data-binding-for-expressive-visualization-authoring","ext":".md"},"path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","excerpt":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","collection":"publications","content":"<p>In the spirit of previous computer-assisted cognition technologies, a new type of computational communication medium has emerged that leverages active reading techniques to make ideas more accessible to a broad range of people. These interactive articles have been shown to be more engaging, can help improve recall and learning, and attract broad readership and acclaim, yet we do not know that much about them. In this work, for the the first time, we connect the dots between interactive articles such as those featured in media publications and the techniques, theories, and empirical evaluations put forth by academic researchers across the fields of education, human-computer interaction, information visualization, and digital journalism. After describing the affordances of interactive articles, we provide critical reflections from our own experience with open-source, interactive publishing at scale. We conclude with discussing practical challenges and open research directions for authoring, designing, and publishing interactive articles.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","previous":{"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","next":{"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","collection":"publications","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","excerpt":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","collection":"publications","content":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","next":{"output":"\n","previous":{"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","collection":"publications","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","next":{"url":"/publications/2018-communicating-with-interactive-articles%20copy.html","relative_path":"_publications/2018-communicating-with-interactive-articles copy.md","path":"_publications/2018-communicating-with-interactive-articles copy.md","id":"/publications/2018-communicating-with-interactive-articles copy","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Matthew Conlen","Jeffrey Heer","Duen Horng (Polo) Chau"],"link":"https://distill.pub/2020/communicating-with-interactive-articles/","tags":["Interactive Articles","Data Storytelling","Explorable Explanations"],"title":"Communicating with Interactive Articles.","venue":"Distill","year":2020,"slug":"2018-communicating-with-interactive-articles copy","ext":".md"},"path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},{"output":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","next":{"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","next":{"output":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","previous":{"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","next":{"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","excerpt":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","collection":"publications","content":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","excerpt":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","collection":"publications","content":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},{"output":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","previous":{"output":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","previous":{"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","next":{"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","excerpt":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","collection":"publications","content":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","next":{"output":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","previous":{"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","next":{"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","collection":"publications","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","excerpt":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","collection":"publications","content":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","excerpt":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","collection":"publications","content":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},{"output":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","previous":{"output":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","previous":{"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","collection":"publications","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","next":{"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","excerpt":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","collection":"publications","content":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","next":{"output":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","previous":{"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","next":{"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","excerpt":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","collection":"publications","content":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","excerpt":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","collection":"publications","content":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},{"output":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","previous":{"output":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","previous":{"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","collection":"publications","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","next":{"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","excerpt":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","collection":"publications","content":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","next":{"output":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","previous":{"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","next":{"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","excerpt":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","collection":"publications","content":"<p>Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","excerpt":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","collection":"publications","content":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},{"output":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","next":{"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","collection":"publications","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","next":{"output":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","previous":{"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","collection":"publications","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","next":{"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","excerpt":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","collection":"publications","content":"<p>Locating and extracting subgraphs from large network datasets is a challenge in many domains, one that often requires learning new querying languages. We will present the first demonstration of Visage, an interactive visual graph querying approach that empowers analysts to construct expressive queries, without writing complex code. Visage guides the construction of graph queries using a data-driven approach, enabling analysts to specify queries with varying levels of specificity, by sampling matches to a query during the analyst’s interaction. We will demonstrate and invite the audience to try Visage on a popular film-actor-director graph from Rotten Tomatoes.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","excerpt":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","collection":"publications","content":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","next":{"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","collection":"publications","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","next":{"output":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","previous":{"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","collection":"publications","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","next":{"url":"/publications/2017-visual-graph-query-construction-and-refinement.html","relative_path":"_publications/2017-visual-graph-query-construction-and-refinement.md","path":"_publications/2017-visual-graph-query-construction-and-refinement.md","id":"/publications/2017-visual-graph-query-construction-and-refinement","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Fred Hohman","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/visage","tags":["Graph Querying","Interactive Querying","Query Construction"],"title":"Visual Graph Query Construction and Refinement.","venue":"SIGMOD Conference","year":2017,"slug":"2017-visual-graph-query-construction-and-refinement","ext":".md"},"path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","excerpt":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","collection":"publications","content":"<p>We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.</p>\n","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","next":{"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","next":{"output":"\n","previous":{"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","next":{"url":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.html","relative_path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","path":"_publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications.md","id":"/publications/2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","collection":"publications","draft":false,"categories":[],"authors":["Petra Isenberg","Florian Heimerl","Steffen Koch","Tobias Isenberg","Panpan Xu","Charles D. Stolper","Michael Sedlmair","Jian Chen","Torsten M","John T. Stasko"],"link":null,"tags":["Metadata","Data Visualization","Portable Document Format","Libraries","History","Conferences","Terminology"],"title":"Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-vispubdataorg-a-metadata-collection-about-ieee-visualization-vis-publications","ext":".md"},"path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","previous":{"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","next":{"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","excerpt":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","collection":"publications","content":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","next":{"url":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.html","relative_path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","path":"_publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality.md","id":"/publications/2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","collection":"publications","draft":false,"categories":[],"authors":["Taeheon Kim","Bahador Saket","Alex Endert","Blair MacIntyre"],"link":null,"tags":[],"title":"VisAR: Bringing Interactivity to Static Data Visualizations through Augmented Reality.","venue":"arXiv","year":2017,"slug":"2017-visar-bringing-interactivity-to-static-data-visualizations-through-augmented-reality","ext":".md"},"path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},{"output":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","previous":{"output":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","previous":{"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","next":{"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","excerpt":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","collection":"publications","content":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","next":{"output":"\n","previous":{"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","next":{"url":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.html","relative_path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","path":"_publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities.md","id":"/publications/2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Clio Andris","Xi Liu"],"link":null,"tags":[],"title":"Using Yelp to Find Romance in the City: A Case of Restaurants in Four Cities.","venue":"UrbanGIS@SIGSPATIAL","year":2017,"slug":"2017-using-yelp-to-find-romance-in-the-city-a-case-of-restaurants-in-four-cities","ext":".md"},"path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","excerpt":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","collection":"publications","content":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},{"output":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","collection":"publications","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","next":{"output":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","previous":{"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","next":{"url":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.html","relative_path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","path":"_publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization.md","id":"/publications/2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","Yongxin Wang","John T. Stasko"],"link":null,"tags":[],"title":"TypoTweet Maps: Characterizing Urban Areas through Typographic Social Media Visualization.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-typotweet-maps-characterizing-urban-areas-through-typographic-social-media-visualization","ext":".md"},"path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","excerpt":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","collection":"publications","content":"<p>Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.</p>\n","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","excerpt":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","collection":"publications","content":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","previous":{"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","next":{"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","excerpt":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","collection":"publications","content":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","next":{"output":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","previous":{"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","next":{"url":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.html","relative_path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","path":"_publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems.md","id":"/publications/2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["R. Jordan Crouser","Lyndsey Franklin","Alex Endert","Kristin A. Cook"],"link":null,"tags":["Animals","Visualization","Education","Biological Cells","Genomics","Vegetation","Bioinformatics"],"title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-toward-theoretical-techniques-for-measuring-the-use-of-human-effort-in-visual-analytic-systems","ext":".md"},"path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","excerpt":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","collection":"publications","content":"<p>Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},{"output":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","previous":{"output":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","previous":{"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","next":{"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","collection":"publications","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","excerpt":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","collection":"publications","content":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","next":{"output":"\n","previous":{"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","collection":"publications","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"url":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.html","relative_path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","next":{"url":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.html","relative_path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","path":"_publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics.md","id":"/publications/2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","W. Ribarsky","Cagatay Turkay","B. L. William Wong","Ian T. Nabney","Ignacio D","Fabrice Rossi"],"link":null,"tags":["Visualization","Visual Analytics","Humancentred Computing","Data Mining","Information Visualization"],"title":"The State of the Art in Integrating Machine Learning into Visual Analytics.","venue":"Comput. Graph. Forum","year":2017,"slug":"2017-the-state-of-the-art-in-integrating-machine-learning-into-visual-analytics","ext":".md"},"path":"_publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics.md","id":"/publications/2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nick Cramer","Grant Nakamura","Alex Endert"],"link":null,"tags":[],"title":"The Impact of Streaming Data on Sensemaking with Mixed-Initiative Visual Analytics.","venue":"HCI (14)","year":2017,"slug":"2017-the-impact-of-streaming-data-on-sensemaking-with-mixedinitiative-visual-analytics","ext":".md"},"path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","excerpt":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","collection":"publications","content":"<p>Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of “team-first” visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>\n","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},{"output":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","previous":{"output":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","previous":{"url":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.html","relative_path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","path":"_publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration.md","id":"/publications/2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Hannah Kim","Eli T. Brown","Alex Endert"],"link":null,"tags":["Visualization","Data Visualization","Bars","Spatial Databases","Encoding","Automobiles","Image Color Analysis"],"title":"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualization-by-demonstration-an-interaction-paradigm-for-visual-data-exploration","ext":".md"},"url":"/publications/2017-visualizing-social-media-content-with-sententree.html","relative_path":"_publications/2017-visualizing-social-media-content-with-sententree.md","next":{"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"path":"_publications/2017-visualizing-social-media-content-with-sententree.md","id":"/publications/2017-visualizing-social-media-content-with-sententree","excerpt":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","collection":"publications","content":"<p>We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Krist Wongsuphasawat","John T. Stasko"],"link":null,"tags":["Layout","Visualization","Tag Clouds","Media","Context","Games","Twitter"],"title":"Visualizing Social Media Content with SentenTree.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-visualizing-social-media-content-with-sententree","ext":".md"},"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.html","relative_path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},"url":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.html","relative_path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","next":{"url":"/publications/2018-challenges-for-social-flows.html","relative_path":"_publications/2018-challenges-for-social-flows.md","path":"_publications/2018-challenges-for-social-flows.md","id":"/publications/2018-challenges-for-social-flows","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Xi Liu","Joseph Ferreira"],"link":null,"tags":[],"title":"Challenges for social flows.","venue":"Comput. Environ. Urban Syst.","year":2018,"slug":"2018-challenges-for-social-flows","ext":".md"},"path":"_publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval.md","id":"/publications/2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Abhijit Suprem","Duen Horng (Polo) Chau","Calton Pu"],"link":null,"tags":[],"title":"Approximate Query Matching for Graph-Based Holistic Image Retrieval.","venue":"BigData Congress","year":2018,"slug":"2018-approximate-query-matching-for-graphbased-holistic-image-retrieval","ext":".md"},"path":"_publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics.md","id":"/publications/2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","excerpt":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","collection":"publications","content":"<p>Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.</p>\n","draft":false,"categories":[],"authors":["Emily Wall","Leslie M. Blaha","Lyndsey Franklin","Alex Endert"],"link":null,"tags":["User Interactions","Analytical Models","Measurement","Computational Modeling","Visual Analytic Tools","Interactive Visual Analytics","Interaxis","Human-in-the-loop Systems","Bias Assessment","Visual Analytics","Domain Expertise","Bias Measurement","Cognitive Bias","Data Analysis","Human Computer Interaction","Data Visualisation","Cognition","Sensemaking Capabilities","Human Biases","Decision Making"],"title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics.","venue":"VAST","year":2017,"slug":"2017-warning-bias-may-occur-a-proposed-approach-to-detecting-cognitive-bias-in-interactive-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","next":{"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","next":{"output":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","previous":{"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","next":{"url":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.html","relative_path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","path":"_publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations.md","id":"/publications/2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","collection":"publications","draft":false,"categories":[],"authors":["Sriram Karthik Badam","Zehua Zeng","Emily Wall","Alex Endert","Niklas Elmqvist"],"link":null,"tags":[],"title":"Supporting Team-First Visual Analytics through Group Activity Representations.","venue":"Graphics Interface","year":2017,"slug":"2017-supporting-teamfirst-visual-analytics-through-group-activity-representations","ext":".md"},"path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","excerpt":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","collection":"publications","content":"<p>Deep learning is the driving force behind many recent technologies; however, deep neural networks are often viewed as “black-boxes” due to their internal complexity that is hard to understand. Little research focuses on helping people explore and understand the relationship between a user’s data and the learned representations in deep learning models. We present our ongoing work, ShapeShop, an interactive system for visualizing and understanding what semantics a neural network model has learned. Built using standard web technologies, ShapeShop allows users to experiment with and compare deep learning models to help explore the robustness of image classifiers.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","next":{"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","next":{"output":"\n","previous":{"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","next":{"url":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.html","relative_path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","path":"_publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation.md","id":"/publications/2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Nathan O. Hodas","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/shapeshop","tags":["Machine Learning Visualization","Model Exploration","Interactive Machine Learning"],"title":"ShapeShop: Towards Understanding Deep Learning Representations via Interactive Experimentation.","venue":"CHI Extended Abstracts","year":2017,"slug":"2017-shapeshop-towards-understanding-deep-learning-representations-via-interactive-experimentation","ext":".md"},"path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","next":{"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","next":{"output":"\n","previous":{"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","next":{"url":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","path":"_publications/2017-search-rank-fraud-and-malware-detection-in-google-play.md","id":"/publications/2017-search-rank-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Search Rank Fraud and Malware Detection in Google Play.","venue":"IEEE Trans. Knowl. Data Eng.","year":2017,"slug":"2017-search-rank-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},{"output":"\n","previous":{"output":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","previous":{"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","next":{"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","excerpt":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","collection":"publications","content":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","next":{"output":"\n","previous":{"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","next":{"url":"/publications/2017-predicting-cyber-threats-with-virtual-security-products.html","relative_path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","path":"_publications/2017-predicting-cyber-threats-with-virtual-security-products.md","id":"/publications/2017-predicting-cyber-threats-with-virtual-security-products","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Yufei Han","Duen Horng (Polo) Chau","Christopher S. Gates","Michael Hart","Kevin A. Roundy"],"link":null,"tags":[],"title":"Predicting Cyber Threats with Virtual Security Products.","venue":"ACSAC","year":2017,"slug":"2017-predicting-cyber-threats-with-virtual-security-products","ext":".md"},"path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},{"output":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","previous":{"output":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","previous":{"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","next":{"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","excerpt":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","next":{"output":"\n","previous":{"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","next":{"url":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.html","relative_path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","path":"_publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces.md","id":"/publications/2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"Nodes, Paths, and Edges: Using Mental Maps to Augment Crime Data Analysis in Urban Spaces.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-nodes-paths-and-edges-using-mental-maps-to-augment-crime-data-analysis-in-urban-spaces","ext":".md"},"path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","excerpt":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","collection":"publications","content":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},{"output":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","next":{"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","next":{"output":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","previous":{"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","next":{"url":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.html","relative_path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","path":"_publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked.md","id":"/publications/2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","collection":"publications","draft":false,"categories":[],"authors":["Arjun Srinivasan","John T. Stasko"],"link":null,"tags":[],"title":"Natural Language Interfaces for Data Analysis with Visualization: Considering What Has and Could Be Asked.","venue":"EuroVis (Short Papers)","year":2017,"slug":"2017-natural-language-interfaces-for-data-analysis-with-visualization-considering-what-has-and-could-be-asked","ext":".md"},"path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","excerpt":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","collection":"publications","content":"<p>We present Discovery Dashboard, a visual analytics system for exploring large volumes of time series data from mobile medical field studies. Discovery Dashboard offers interactive exploration tools and a data mining motif discovery algorithm to help researchers formulate hypotheses, discover trends and patterns, and ultimately gain a deeper understanding of their data. Discovery Dashboard emphasizes user freedom and flexibility during the data exploration process and enables researchers to do things previously challenging or impossible to do — in the web-browser and in real time. We demonstrate our system visualizing data from a mobile sensor study conducted at the University of Minnesota that included 52 participants who were trying to quit smoking.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","excerpt":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","next":{"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","next":{"output":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","previous":{"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","next":{"url":"/publications/2017-mhealth-visual-discovery-dashboard.html","relative_path":"_publications/2017-mhealth-visual-discovery-dashboard.md","path":"_publications/2017-mhealth-visual-discovery-dashboard.md","id":"/publications/2017-mhealth-visual-discovery-dashboard","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Fred Hohman","Peter J. Polack Jr.","Hillol Sarker","Minsuk Kahng","Moushumi Sharmin","Mustafa al'Absi","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/dashboard","tags":["Visual Analytics","Health Informatics","Time Series Data","Motif Discovery"],"title":"mHealth Visual Discovery Dashboard.","venue":"Ubicomp, Demo","year":2017,"slug":"2017-mhealth-visual-discovery-dashboard","ext":".md"},"path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","excerpt":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","collection":"publications","content":"<p>Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.</p>\n","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","next":{"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","next":{"output":"\n","previous":{"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","next":{"url":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.html","relative_path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","path":"_publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression.md","id":"/publications/2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","collection":"publications","draft":false,"categories":[],"authors":["Nilaksh Das","Madhuri Shanbhogue","Shang-Tse Chen","Fred Hohman","Li Chen","Michael E. Kounavis","Duen Horng (Polo) Chau"],"link":"https://fredhohman.com/papers/defense","tags":["Adversarial Machine Learning","JPEG Compression","Deep Learning","Computer Vision"],"title":"Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.","venue":"arXiv","year":2017,"slug":"2017-keeping-the-bad-guys-out-protecting-and-vaccinating-deep-learning-with-jpeg-compression","ext":".md"},"path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","collection":"publications","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","next":{"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","next":{"output":"\n","previous":{"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","next":{"url":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.html","relative_path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","path":"_publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots.md","id":"/publications/2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","collection":"publications","draft":false,"categories":[],"authors":["Alex Godwin","John T. Stasko"],"link":null,"tags":[],"title":"HotSketch: Drawing Police Patrol Routes among Spatiotemporal Crime Hotspots.","venue":"HICSS","year":2017,"slug":"2017-hotsketch-drawing-police-patrol-routes-among-spatiotemporal-crime-hotspots","ext":".md"},"path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","previous":{"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","next":{"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","excerpt":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","collection":"publications","content":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","next":{"output":"\n","previous":{"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","next":{"url":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2017-facets-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2017-facets-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Minsuk Kahng","Zhiyuan Lin","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FACETS: Adaptive Local Exploration of Large Graphs.","venue":"SDM","year":2017,"slug":"2017-facets-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},{"output":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","next":{"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","collection":"publications","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","next":{"output":"\n","previous":{"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","collection":"publications","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","next":{"url":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","path":"_publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities.md","id":"/publications/2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Moushumi Sharmin","Kaya de Barbaro","Minsuk Kahng","Shang-Tse Chen","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Exploratory Visual Analytics of Mobile Health Data: Sensemaking Challenges and Opportunities.","venue":"Mobile Health - Sensors, Analytic Methods, and Applications","year":2017,"slug":"2017-exploratory-visual-analytics-of-mobile-health-data-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","excerpt":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","collection":"publications","content":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","next":{"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","next":{"output":"\n","previous":{"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","next":{"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},{"output":"\n","previous":{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","next":{"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","next":{"output":"\n","previous":{"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","next":{"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","collection":"publications","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","next":{"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","next":{"output":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","previous":{"url":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.html","relative_path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},"url":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.html","relative_path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","next":{"url":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.html","relative_path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","path":"_publications/2017-data-and-task-based-effectiveness-of-basic-visualizations.md","id":"/publications/2017-data-and-task-based-effectiveness-of-basic-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert"],"link":null,"tags":[],"title":"Data and Task Based Effectiveness of Basic Visualizations.","venue":"arXiv","year":2017,"slug":"2017-data-and-task-based-effectiveness-of-basic-visualizations","ext":".md"},"path":"_publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings.md","id":"/publications/2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","excerpt":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","collection":"publications","content":"<p>Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users’ complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user’s drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users’ nonlinear domain knowledge; 2) the underlying model that translates users’ input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users’ complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.</p>\n","draft":false,"categories":[],"authors":["Bum Chul Kwon","Hannah Kim","Emily Wall","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Analytical Models","Computational Modeling","Automobiles","Data Models","Manifolds"],"title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2017,"slug":"2017-axisketcher-interactive-nonlinear-axis-mapping-of-visualizations-through-user-drawings","ext":".md"},"path":"_publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community.md","id":"/publications/2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Duen Horng (Polo) Chau","Munmun De Choudhury"],"link":null,"tags":[],"title":"Analysis of Smoking and Drinking Relapse in an Online Community.","venue":"DH","year":2017,"slug":"2017-analysis-of-smoking-and-drinking-relapse-in-an-online-community","ext":".md"},{"output":"\n","previous":{"output":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","previous":{"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","next":{"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","excerpt":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","collection":"publications","content":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","next":{"output":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","previous":{"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","next":{"url":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.html","relative_path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","path":"_publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models.md","id":"/publications/2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","collection":"publications","draft":false,"categories":[],"authors":["Minsuk Kahng","Pierre Y. Andrews","Aditya Kalro","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.","venue":"arXiv","year":2017,"slug":"2017-activis-visual-exploration-of-industryscale-deep-neural-network-models","ext":".md"},"path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","excerpt":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","collection":"publications","content":"<p>As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p>\n","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},{"output":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","next":{"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","next":{"output":"\n","previous":{"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","next":{"url":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.html","relative_path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","path":"_publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue.md","id":"/publications/2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","collection":"publications","draft":false,"categories":[],"authors":["Fred Hohman","Sandeep Soni","Ian Stewart","John Stasko"],"link":"https://fredhohman.com/a-viz-of-ice-and-fire/","tags":["Video Analytics","Data Storytelling"],"title":"A Viz of Ice and Fire: Exploring Entertainment Video Using Color and Dialogue.","venue":"VIS4DH @ VIS","year":2017,"slug":"2017-a-viz-of-ice-and-fire-exploring-entertainment-video-using-color-and-dialogue","ext":".md"},"path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","excerpt":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","collection":"publications","content":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","next":{"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","next":{"output":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","previous":{"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","next":{"url":"/publications/2016-whats-hot-in-intelligent-user-interfaces.html","relative_path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","path":"_publications/2016-whats-hot-in-intelligent-user-interfaces.md","id":"/publications/2016-whats-hot-in-intelligent-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Shimei Pan","Oliver Brdiczka","Giuseppe Carenini","Duen Horng (Polo) Chau","Per Ola Kristensson"],"link":null,"tags":[],"title":"What's Hot in Intelligent User Interfaces.","venue":"AAAI","year":2016,"slug":"2016-whats-hot-in-intelligent-user-interfaces","ext":".md"},"path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","excerpt":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","collection":"publications","content":"<p>Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with “wildcard” nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage’s ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.</p>\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},{"output":"\n","previous":{"output":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","previous":{"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","next":{"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","excerpt":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","collection":"publications","content":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","next":{"output":"\n","previous":{"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","next":{"url":"/publications/2016-visage-interactive-visual-graph-querying.html","relative_path":"_publications/2016-visage-interactive-visual-graph-querying.md","path":"_publications/2016-visage-interactive-visual-graph-querying.md","id":"/publications/2016-visage-interactive-visual-graph-querying","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Alex Endert","Shamkant B. Navathe","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"VISAGE: Interactive Visual Graph Querying.","venue":"AVI","year":2016,"slug":"2016-visage-interactive-visual-graph-querying","ext":".md"},"path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},{"output":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","previous":{"output":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","previous":{"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","collection":"publications","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","next":{"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","excerpt":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","collection":"publications","content":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","next":{"output":"\n","previous":{"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","next":{"url":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research.html","relative_path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","path":"_publications/2016-redefining-a-contribution-for-immersive-visualization-research.md","id":"/publications/2016-redefining-a-contribution-for-immersive-visualization-research","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Vidya Setlur","John T. Stasko"],"link":null,"tags":[],"title":"Redefining a Contribution for Immersive Visualization Research.","venue":"ISS Companion","year":2016,"slug":"2016-redefining-a-contribution-for-immersive-visualization-research","ext":".md"},"path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","excerpt":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","collection":"publications","content":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},{"output":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","previous":{"output":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","previous":{"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","next":{"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","excerpt":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","collection":"publications","content":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","next":{"output":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","previous":{"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","next":{"url":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.html","relative_path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","path":"_publications/2016-node-immunization-on-large-graphs-theory-and-algorithms.md","id":"/publications/2016-node-immunization-on-large-graphs-theory-and-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Chen Chen 0022","Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Node Immunization on Large Graphs: Theory and Algorithms.","venue":"IEEE Trans. Knowl. Data Eng.","year":2016,"slug":"2016-node-immunization-on-large-graphs-theory-and-algorithms","ext":".md"},"path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","excerpt":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","collection":"publications","content":"<p>To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory’s strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster.</p>\n","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","excerpt":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","collection":"publications","content":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},{"output":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","next":{"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","collection":"publications","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","next":{"output":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","previous":{"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","collection":"publications","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","next":{"url":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.html","relative_path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","path":"_publications/2016-m3-scaling-up-machine-learning-via-memory-mapping.md","id":"/publications/2016-m3-scaling-up-machine-learning-via-memory-mapping","collection":"publications","draft":false,"categories":[],"authors":["Dezhi Fang","Duen Horng (Polo) Chau"],"link":null,"tags":["Computing Methodologies","Machine Learning","Operating Systems","Contextual Software Domains","Memory Management","Software And Its Engineering","Software Organization And Properties","Virtual Memory"],"title":"M3: Scaling Up Machine Learning via Memory Mapping.","venue":"SIGMOD Conference","year":2016,"slug":"2016-m3-scaling-up-machine-learning-via-memory-mapping","ext":".md"},"path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","excerpt":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","collection":"publications","content":"<p>Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.</p>\n","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","excerpt":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","collection":"publications","content":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","collection":"publications","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","next":{"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","next":{"output":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","previous":{"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","next":{"url":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.html","relative_path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","path":"_publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction.md","id":"/publications/2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","collection":"publications","draft":false,"categories":[],"authors":["Hannah Kim","Jaegul Choo","Haesun Park","Alex Endert"],"link":null,"tags":["Visual Analytics","Data Visualization","Semantics","Scalability","Data Models","Principal Component Analysis"],"title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-interaxis-steering-scatterplot-axes-via-observationlevel-interaction","ext":".md"},"path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","excerpt":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","collection":"publications","content":"<p>A bipartite graph models the relation between two different types of entities. It is applicable, for example, to describe persons’ affiliations to different social groups or their association with subjects such as topics of interest. In these applications, it is important to understand the connectivity patterns among the entities in the bipartite graph. For the example of a bipartite relation between persons and their topics of interest, people may form groups based on their common interests, and the topics also can be grouped or categorized based on the interested audiences. Co-clustering methods can identify such connectivity patterns and find clusters within the two types of entities simultaneously. In this paper, we propose an interactive visualization design that incorporates co-clustering methods to facilitate the identification of node clusters formed by their common connections in a bipartite graph. Besides highlighting the automatically detected node clusters and the connections among them, the visual interface also provides visual cues for evaluating the homogeneity of the bipartite connections in a cluster, identifying potential outliers, and analyzing the correlation of node attributes with the cluster structure. The interactive visual interface allows users to flexibly adjust the node grouping to incorporate their prior knowledge of the domain, either by direct manipulation (i.e., splitting and merging the clusters), or by providing explicit feedback on the cluster quality, based on which the system will learn a parametrization of the co-clustering algorithm to better align with the users’ notion of node similarity. To demonstrate the utility of the system, we present two example usage scenarios on real world datasets.</p>\n","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","collection":"publications","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","next":{"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","next":{"output":"\n","previous":{"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","next":{"url":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.html","relative_path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","path":"_publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs.md","id":"/publications/2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","collection":"publications","draft":false,"categories":[],"authors":["Panpan Xu","Nan Cao","Huamin Qu","John T. Stasko"],"link":null,"tags":["Correlation","Visualization","Data Visualization","Algorithm Design And Analysis","Bipartite Graph","Measurement","Clustering Algorithms"],"title":"Interactive visual co-cluster analysis of bipartite graphs.","venue":"PacificVis","year":2016,"slug":"2016-interactive-visual-cocluster-analysis-of-bipartite-graphs","ext":".md"},"path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},{"output":"\n","previous":{"output":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","previous":{"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","next":{"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","collection":"publications","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","excerpt":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","collection":"publications","content":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","next":{"output":"\n","previous":{"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","collection":"publications","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","next":{"url":"/publications/2016-integrating-social-network-data-into-gisystems.html","relative_path":"_publications/2016-integrating-social-network-data-into-gisystems.md","path":"_publications/2016-integrating-social-network-data-into-gisystems.md","id":"/publications/2016-integrating-social-network-data-into-gisystems","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Integrating social network data into GISystems.","venue":"Int. J. Geogr. Inf. Sci.","year":2016,"slug":"2016-integrating-social-network-data-into-gisystems","ext":".md"},"path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},{"output":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","next":{"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","collection":"publications","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","next":{"output":"\n","previous":{"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","collection":"publications","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","next":{"url":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.html","relative_path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","path":"_publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities.md","id":"/publications/2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","collection":"publications","draft":false,"categories":[],"authors":["Sohrab Rahimi","Xi Liu","Clio Andris"],"link":null,"tags":[],"title":"Hidden style in the city: an analysis of geolocated airbnb rental images in ten major cities.","venue":"UrbanGIS@SIGSPATIAL","year":2016,"slug":"2016-hidden-style-in-the-city-an-analysis-of-geolocated-airbnb-rental-images-in-ten-major-cities","ext":".md"},"path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","excerpt":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","collection":"publications","content":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","next":{"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","next":{"output":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","previous":{"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","next":{"url":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data.html","relative_path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","path":"_publications/2016-generating-graph-snapshots-from-streaming-edge-data.md","id":"/publications/2016-generating-graph-snapshots-from-streaming-edge-data","collection":"publications","draft":false,"categories":[],"authors":["Sucheta Soundarajan","Acar Tamersoy","Elias B. Khalil","Tina Eliassi-Rad","Duen Horng (Polo) Chau","Brian Gallagher","Kevin A. Roundy"],"link":null,"tags":[],"title":"Generating Graph Snapshots from Streaming Edge Data.","venue":"WWW (Companion Volume)","year":2016,"slug":"2016-generating-graph-snapshots-from-streaming-edge-data","ext":".md"},"path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","excerpt":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","collection":"publications","content":"<p>The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.</p>\n","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},{"output":"\n","previous":{"output":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","previous":{"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","collection":"publications","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","next":{"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","excerpt":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","collection":"publications","content":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","next":{"output":"\n","previous":{"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","next":{"url":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.html","relative_path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","path":"_publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta.md","id":"/publications/2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","collection":"publications","draft":false,"categories":[],"authors":["Michael A. Madaio","Shang-Tse Chen","Oliver L. Haimson","Wenwen Zhang","Xiang Cheng","Matthew Hinds-Aldrich","Duen Horng (Polo) Chau","Bistra Dilkina"],"link":null,"tags":["Computing / Technology Policy","Human-centered Computing","Geographic Information Systems","Commerce Policy","Supervised Learning By Classification","Data Cleaning","Governmental Regulations","Visualization","Information Systems","Supervised Learning","Data Mining","Learning Paradigms","Spatial-temporal Systems","Visualization Application Domains","Computing Methodologies","Machine Learning","Social And Professional Topics","Information Systems Applications","Geographic Visualization"],"title":"Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta.","venue":"KDD","year":2016,"slug":"2016-firebird-predicting-fire-risk-and-prioritizing-fire-inspections-in-atlanta","ext":".md"},"path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},{"output":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","next":{"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","next":{"output":"\n","previous":{"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","next":{"url":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play.html","relative_path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","path":"_publications/2016-fairplay-fraud-and-malware-detection-in-google-play.md","id":"/publications/2016-fairplay-fraud-and-malware-detection-in-google-play","collection":"publications","draft":false,"categories":[],"authors":["Mahmudur Rahman","Mizanur Rahman","Bogdan Carbunar","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"FairPlay: Fraud and Malware Detection in Google Play.","venue":"SDM","year":2016,"slug":"2016-fairplay-fraud-and-malware-detection-in-google-play","ext":".md"},"path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","excerpt":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","collection":"publications","content":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","next":{"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","collection":"publications","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","next":{"output":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","previous":{"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","collection":"publications","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","next":{"url":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.html","relative_path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","path":"_publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices.md","id":"/publications/2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":[],"title":"Expanding Selection for Information Visualization Systems on Tablet Devices.","venue":"ISS","year":2016,"slug":"2016-expanding-selection-for-information-visualization-systems-on-tablet-devices","ext":".md"},"path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","excerpt":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","collection":"publications","content":"<p>The use of multiple coordinated views (MCV) in data visualization provides analytic power because it allows a person to explore data under a variety of different perspectives. Since this design pattern utilizes multiple visualizations and requires coordinated interactions across the views, a clever use of screen space is vital and many synchronized interface operations must be provided. Bringing this design pattern to tablet computers is challenging due to their small display size and the absence of keyboard and mouse input. In this article, we explain important design considerations for MCV visualization on tablets and describe a prototype MCV visualization system we have built for the iPad. The design is based on the principles of maximizing screen space for data presentation, promoting consistent interactions across visualizations, and minimizing occlusion from a person’s hands.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","collection":"publications","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","next":{"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","next":{"output":"\n","previous":{"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","next":{"url":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets.html","relative_path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","path":"_publications/2016-designing-multiple-coordinated-visualizations-for-tablets.md","id":"/publications/2016-designing-multiple-coordinated-visualizations-for-tablets","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Computer Graphics"],"title":"Designing Multiple Coordinated Visualizations for Tablets.","venue":"Comput. Graph. Forum","year":2016,"slug":"2016-designing-multiple-coordinated-visualizations-for-tablets","ext":".md"},"path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","next":{"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","next":{"output":"\n","previous":{"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","next":{"url":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.html","relative_path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","path":"_publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction.md","id":"/publications/2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","collection":"publications","draft":false,"categories":[],"authors":["Kunal Malhotra","Shamkant B. Navathe","Duen Horng (Polo) Chau","Costas Hadjipanayis","Jimeng Sun"],"link":null,"tags":[],"title":"Constraint based temporal event sequence mining for Glioblastoma survival prediction.","venue":"J. Biomed. Informatics","year":2016,"slug":"2016-constraint-based-temporal-event-sequence-mining-for-glioblastoma-survival-prediction","ext":".md"},"path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},{"output":"\n","previous":{"output":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","previous":{"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","next":{"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","collection":"publications","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","excerpt":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","collection":"publications","content":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","next":{"output":"\n","previous":{"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","collection":"publications","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","next":{"url":"/publications/2016-communication-efficient-distributed-agnostic-boosting.html","relative_path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","path":"_publications/2016-communication-efficient-distributed-agnostic-boosting.md","id":"/publications/2016-communication-efficient-distributed-agnostic-boosting","collection":"publications","draft":false,"categories":[],"authors":["Shang-Tse Chen","Maria-Florina Balcan","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Communication Efficient Distributed Agnostic Boosting.","venue":"AISTATS","year":2016,"slug":"2016-communication-efficient-distributed-agnostic-boosting","ext":".md"},"path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},{"output":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","next":{"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","next":{"url":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.html","relative_path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","path":"_publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes.md","id":"/publications/2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Jibonananda Sanyal","Jian Chen"],"link":null,"tags":[],"title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2016,"slug":"2016-characterizing-provenance-in-visualization-and-data-analysis-an-organizational-framework-of-provenance-types-and-purposes","ext":".md"},"path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","excerpt":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","collection":"publications","content":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","next":{"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","next":{"output":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","previous":{"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","next":{"url":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines.html","relative_path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","path":"_publications/2016-building-a-research-data-science-platform-from-industrial-machines.md","id":"/publications/2016-building-a-research-data-science-platform-from-industrial-machines","collection":"publications","draft":false,"categories":[],"authors":["Fang (Cherry) Liu","Fu Shen","Duen Horng (Polo) Chau","Neil Bright","Mehmet Belgin"],"link":null,"tags":[],"title":"Building a research data science platform from industrial machines.","venue":"IEEE BigData","year":2016,"slug":"2016-building-a-research-data-science-platform-from-industrial-machines","ext":".md"},"path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","excerpt":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","collection":"publications","content":"<p>Traditionally, studies of data visualization techniques and systems have evaluated visualizations with respect to usability goals such as effectiveness and efficiency. These studies assess performance-related metrics such as time and correctness of participants completing analytic tasks. Alternatively, several studies in InfoVis recently have evaluated visualizations by investigating user experience goals such as memorability, engagement, enjoyment and fun. These studies employ somewhat different evaluation methodologies to assess these other goals. The growing number of these studies, their alternative methodologies, and disagreements concerning their importance have motivated us to more carefully examine them. In this article, we review this growing collection of visualization evaluations that examine user experience goals and we discuss multiple issues regarding the studies including questions about their motivation and utility. Our aim is to provide a resource for future work that plans to evaluate visualizations using these goals.</p>\n","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},{"output":"\n","previous":{"output":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","previous":{"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","collection":"publications","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","next":{"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","excerpt":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","collection":"publications","content":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","next":{"output":"\n","previous":{"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","next":{"url":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.html","relative_path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","path":"_publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization.md","id":"/publications/2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","collection":"publications","draft":false,"categories":[],"authors":["Bahador Saket","Alex Endert","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Beyond Usability and Performance: A Review of User Experience-focused Evaluations in Visualization.","venue":"BELIV","year":2016,"slug":"2016-beyond-usability-and-performance-a-review-of-user-experiencefocused-evaluations-in-visualization","ext":".md"},"path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},{"output":"\n","previous":{"output":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","previous":{"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","next":{"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","excerpt":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","collection":"publications","content":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","next":{"output":"\n","previous":{"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","next":{"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","collection":"publications","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},{"output":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","previous":{"output":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","previous":{"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","collection":"publications","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","next":{"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","collection":"publications","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","excerpt":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","collection":"publications","content":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","next":{"output":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","previous":{"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","collection":"publications","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","next":{"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","excerpt":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","collection":"publications","content":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","excerpt":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","collection":"publications","content":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},{"output":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","next":{"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","next":{"output":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","previous":{"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","next":{"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","excerpt":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","collection":"publications","content":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","excerpt":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","collection":"publications","content":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},{"output":"\n","previous":{"output":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","previous":{"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","next":{"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","collection":"publications","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","excerpt":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","collection":"publications","content":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","next":{"output":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","previous":{"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","collection":"publications","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","next":{"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","collection":"publications","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","excerpt":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","collection":"publications","content":"<p>Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.</p>\n","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},{"output":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","next":{"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","next":{"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","excerpt":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","collection":"publications","content":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","collection":"publications","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","next":{"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","next":{"output":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","previous":{"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","next":{"url":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.html","relative_path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","path":"_publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction.md","id":"/publications/2015-spotting-suspicious-reviews-via-quasiclique-extraction","collection":"publications","draft":false,"categories":[],"authors":["Paras Jain","Shang-Tse Chen","Mozhgan Azimpourkivi","Duen Horng (Polo) Chau","Bogdan Carbunar"],"link":null,"tags":[],"title":"Spotting Suspicious Reviews via (Quasi-)clique Extraction.","venue":"arXiv","year":2015,"slug":"2015-spotting-suspicious-reviews-via-quasiclique-extraction","ext":".md"},"path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","excerpt":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","collection":"publications","content":"<p>The success of visual analytics is predicated on the ability of users to interactively explore information. Humans think about their data through interactive visual exploration, including testing hypotheses, exploring anomalies, and other cognitive processes of building understanding from data. The claim that these insights are generated as a result of the interaction led the attendees at the Pacific Northwest National Laboratory (PNNL) workshop on “Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics” to posit that user interaction must play a more central role in visual analytics systems, serving as the method for coupling cognition and computation. The claims and design principles discussed in this workshop report present research directions to advance visual analytics via a user interaction approach called semantic interaction.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","previous":{"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","collection":"publications","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","next":{"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","excerpt":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","collection":"publications","content":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","next":{"output":"\n","previous":{"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","next":{"url":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.html","relative_path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","path":"_publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics.md","id":"/publications/2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Remco Chang","Chris North","Michelle X. Zhou"],"link":null,"tags":["Cognition","Visual Analytics","Semantics","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.","venue":"IEEE Computer Graphics and Applications","year":2015,"slug":"2015-semantic-interaction-coupling-cognition-and-computation-through-usable-interactive-analytics","ext":".md"},"path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},{"output":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","next":{"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","collection":"publications","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","next":{"output":"\n","previous":{"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","collection":"publications","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","next":{"url":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.html","relative_path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","path":"_publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs.md","id":"/publications/2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Zhiyuan Lin","Minsuk Kahng","Jilles Vreeken","Partha P. Talukdar","James Abello","Ganesh Parameswaran","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Seeing the Forest through the Trees: Adaptive Local Exploration of Large Graphs.","venue":"arXiv","year":2015,"slug":"2015-seeing-the-forest-through-the-trees-adaptive-local-exploration-of-large-graphs","ext":".md"},"path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","excerpt":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","collection":"publications","content":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","next":{"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","collection":"publications","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","next":{"output":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","previous":{"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","collection":"publications","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"url":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.html","relative_path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","next":{"url":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.html","relative_path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","path":"_publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities.md","id":"/publications/2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","James Abello","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Scalable graph exploration and visualization: Sensemaking challenges and opportunities.","venue":"BigComp","year":2015,"slug":"2015-scalable-graph-exploration-and-visualization-sensemaking-challenges-and-opportunities","ext":".md"},"path":"_publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations.md","id":"/publications/2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","excerpt":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","collection":"publications","content":"<p>Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.</p>\n","draft":false,"categories":[],"authors":["Kristin A. Cook","Nick Cramer","David J. Israel","Michael Wolverton","Joe Bruce","Russ Burtner","Alex Endert"],"link":null,"tags":["Semantic Interaction","Analytic Discourse","User Interactions","Mixed-initiative Systems","Analytical Models","Myriad Analytic Models","Active Data Environment","Computational Modeling","Visual Analytic Tools","Visual Analytics","Mixed-initiative Visual Analytics","Discovery Tasks","Analytic Process","Design Guidelines","Automated Activities","Data Analysis","Human Computer Interaction","Sensemaking Tasks","Task-driven Recommendations","Iterative Sensemaking","Data Visualisation","Spatial Workspace","Recommender Systems","Interactive Visual Interfaces","Task Recommendations","Data Visualization","Ade Prototype","Visual Data Analysis","Usability","Task Model","Analytic Sensemaking","Data Models","Human Activities","Cognitive Actions"],"title":"Mixed-initiative visual analytics using task-driven recommendations.","venue":"VAST","year":2015,"slug":"2015-mixedinitiative-visual-analytics-using-taskdriven-recommendations","ext":".md"},"path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},{"output":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","previous":{"output":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","previous":{"url":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.html","relative_path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","path":"_publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison.md","id":"/publications/2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","collection":"publications","draft":false,"categories":[],"authors":["Peter J. Polack Jr.","Shang-Tse Chen","Minsuk Kahng","Moushumi Sharmin","Duen Horng (Polo) Chau"],"link":null,"tags":["Mobile Computing","Abstinent Smoker","Event-based Timeline","Medical Computing","Congestive Heart Failure","Medical Services","Mobile Communication","Interactive Systems","Interactive Multifocus Cohort Discovery","Visualization Technique","Timestitch","Electronic Mail","Health Care","Healthcare","Cloning","Data Mining","Data Visualisation","Mobile Health Sensor Data","Data Visualization","Mortality Risk","Heart","Interactive Technique"],"title":"TimeStitch: Interactive multi-focus cohort discovery and comparison.","venue":"VAST","year":2015,"slug":"2015-timestitch-interactive-multifocus-cohort-discovery-and-comparison","ext":".md"},"url":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.html","relative_path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","next":{"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"path":"_publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery.md","id":"/publications/2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","excerpt":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","collection":"publications","content":"<p>Data analysts are tasked with the challenge of transforming an abundance of data into knowledge and insights. This complex cognitive process has been studied, and models created to describe how the process works in specific domains. Two popular models used for this generalization are the sensemaking and signature discovery models, which apply a cognitive and computational focus to describe the analytic process, respectively. This work seeks to deepen our understanding of the data analysis process in light of these two models. We present the results of interviews and observations of analysts and scientists in four domains (Biology, Cyber Security, Intelligence Analysis, and Data Science). Our results indicate that specific aspects of both models are exhibited in the analysts from our study, but neither describe the holistic analysis process.</p>\n","draft":false,"categories":[],"authors":["Sheriff Jolaoso","Russ Burtner","Alex Endert"],"link":null,"tags":["Visual Analytics","Sensemaking","Data Analysis","Signature Discovery","Analytic Process"],"title":"Toward a Deeper Understanding of Data Analysis, Sensemaking, and Signature Discovery.","venue":"INTERACT (2)","year":2015,"slug":"2015-toward-a-deeper-understanding-of-data-analysis-sensemaking-and-signature-discovery","ext":".md"},"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","next":{"output":"\n","previous":{"url":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.html","relative_path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},"url":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.html","relative_path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","next":{"url":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.html","relative_path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","path":"_publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction.md","id":"/publications/2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","collection":"publications","draft":false,"categories":[],"authors":["Nathan Oken Hodas","Alex Endert"],"link":null,"tags":[],"title":"Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction.","venue":"arXiv","year":2016,"slug":"2016-adding-semantic-information-into-data-models-by-learning-domain-expertise-from-user-interaction","ext":".md"},"path":"_publications/2016-a-computational-model-for-dyadic-relationships-invited-paper.md","id":"/publications/2016-a-computational-model-for-dyadic-relationships-invited-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"A Computational Model for Dyadic Relationships (Invited Paper).","venue":"IRI","year":2016,"slug":"2016-a-computational-model-for-dyadic-relationships-invited-paper","ext":".md"},"path":"_publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors.md","id":"/publications/2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","excerpt":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","collection":"publications","content":"<p>Developmental psychology researchers examine the temporal relationships of social and communicative behaviors, such as how a child responds to a name call, to understand early typical and atypical development and to discover early signs of autism and developmental delay. These related behaviors occur together or within close temporal proximity, forming unique patterns and relationships of interest. However, the task of finding these early signs, which are in the form of atypical behavioral patterns, becomes more challenging when behaviors of multiple children at different ages need to be compared with each other in search of generalizable patterns. The ability to visually explore the temporal relationships of behaviors, including flexible redefinition of closeness, over multiple social interaction sessions with children of different ages, can make such knowledge extraction easier. We have designed a visualization tool called TipoVis that helps psychology researchers visually explore the temporal patterns of social and communicative behaviors. We present two case studies to show how TipoVis helped two researchers derive new understandings of their data.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Agata Rozga","Nevena Dimitrova","Gregory D. Abowd","John T. Stasko"],"link":null,"tags":["H.5.m."],"title":"Visual Analysis of Proximal Temporal Relationships of Social and Communicative Behaviors.","venue":"Comput. Graph. Forum","year":2015,"slug":"2015-visual-analysis-of-proximal-temporal-relationships-of-social-and-communicative-behaviors","ext":".md"},{"output":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","collection":"publications","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","next":{"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","next":{"url":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.html","relative_path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","path":"_publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model.md","id":"/publications/2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","collection":"publications","draft":false,"categories":[],"authors":["Hugo Gualdron","Robson L. F. Cordeiro","Jos","Duen Horng (Polo) Chau","Minsuk Kahng","U Kang"],"link":null,"tags":[],"title":"M-Flash: Fast Billion-scale Graph Computation Using Block Partition Model.","venue":"arXiv","year":2015,"slug":"2015-mflash-fast-billionscale-graph-computation-using-block-partition-model","ext":".md"},"path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","excerpt":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","collection":"publications","content":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","next":{"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","next":{"output":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","previous":{"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","next":{"url":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.html","relative_path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","path":"_publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper.md","id":"/publications/2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"LBSN Data and the Social Butterfly Effect (Vision Paper).","venue":"LBSN@SIGSPATIAL/GIS","year":2015,"slug":"2015-lbsn-data-and-the-social-butterfly-effect-vision-paper","ext":".md"},"path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","excerpt":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","collection":"publications","content":"<p>Analyzing text documents has been a key research topic in many areas. Countless approaches have been proposed to tackle this problem, and they are largely categorized into fully automated approaches (via statistical techniques) or human-involved exploratory ones (via interactive visualization). The primary purpose of this workshop is to bring together researchers from both sides and provide them with opportunities to discuss ways to harmonize the power of these two complementary approaches. The combination will allow us to push the boundary of text analytics. The detailed workshop schedule, proceedings, and agenda will be available at http://www.textvis.org.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","collection":"publications","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","next":{"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","collection":"publications","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","next":{"output":"\n","previous":{"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","collection":"publications","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","next":{"url":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.html","relative_path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","path":"_publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics.md","id":"/publications/2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Christopher Collins","Wenwen Dou","Alex Endert"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"IUI-TextVis 2015: Fourth Workshop on Interactive Visual Text Analytics.","venue":"IUI","year":2015,"slug":"2015-iuitextvis-2015-fourth-workshop-on-interactive-visual-text-analytics","ext":".md"},"path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","next":{"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","next":{"output":"\n","previous":{"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","next":{"url":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.html","relative_path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","path":"_publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design.md","id":"/publications/2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Alex Endert","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Interactive Querying over Large Network Data: Scalability, Visualization, and Interaction Design.","venue":"IUI Companion","year":2015,"slug":"2015-interactive-querying-over-large-network-data-scalability-visualization-and-interaction-design","ext":".md"},"path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","collection":"publications","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","next":{"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","collection":"publications","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","next":{"output":"\n","previous":{"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","collection":"publications","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","next":{"url":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem.html","relative_path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","path":"_publications/2015-identifying-successful-investors-in-the-startup-ecosystem.md","id":"/publications/2015-identifying-successful-investors-in-the-startup-ecosystem","collection":"publications","draft":false,"categories":[],"authors":["Srishti Gupta","Robert Pienta","Acar Tamersoy","Duen Horng (Polo) Chau","Rahul C. Basole"],"link":null,"tags":[],"title":"Identifying Successful Investors in the Startup Ecosystem.","venue":"WWW (Companion Volume)","year":2015,"slug":"2015-identifying-successful-investors-in-the-startup-ecosystem","ext":".md"},"path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","next":{"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","next":{"output":"\n","previous":{"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","next":{"url":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.html","relative_path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","path":"_publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales.md","id":"/publications/2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Zoe Andris"],"link":null,"tags":[],"title":"Exploring Institution-Based Mobility: Which Universities Attract Athletes from Distant and Diverse Locales?","venue":"MDM (2)","year":2015,"slug":"2015-exploring-institutionbased-mobility-which-universities-attract-athletes-from-distant-and-diverse-locales","ext":".md"},"path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","next":{"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","collection":"publications","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","next":{"output":"\n","previous":{"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","collection":"publications","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","next":{"url":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.html","relative_path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","path":"_publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients.md","id":"/publications/2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","collection":"publications","draft":false,"categories":[],"authors":["Robert Chen","Vikas Kumar","Natalie Fitch","Jitesh Jagadish","Lifan Zhang","William Dunn","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"explICU: A web-based visualization and predictive modeling toolkit for mortality in intensive care patients.","venue":"EMBC","year":2015,"slug":"2015-explicu-a-webbased-visualization-and-predictive-modeling-toolkit-for-mortality-in-intensive-care-patients","ext":".md"},"path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},{"output":"\n","previous":{"output":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","previous":{"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","next":{"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","excerpt":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","collection":"publications","content":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","next":{"output":"\n","previous":{"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","next":{"url":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.html","relative_path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","path":"_publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media.md","id":"/publications/2015-characterizing-smoking-and-drinking-abstinence-from-social-media","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Munmun De Choudhury","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Characterizing Smoking and Drinking Abstinence from Social Media.","venue":"HT","year":2015,"slug":"2015-characterizing-smoking-and-drinking-abstinence-from-social-media","ext":".md"},"path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},{"output":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","collection":"publications","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","next":{"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","next":{"output":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","previous":{"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","next":{"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","excerpt":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","collection":"publications","content":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","excerpt":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","collection":"publications","content":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},{"output":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","previous":{"output":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","previous":{"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","next":{"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","excerpt":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","collection":"publications","content":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","next":{"output":"\n","previous":{"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"url":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.html","relative_path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","next":{"url":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.html","relative_path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","path":"_publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k.md","id":"/publications/2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","collection":"publications","draft":false,"categories":[],"authors":["Santosh Kumar","Gregory D. Abowd","William T. Abraham","Mustafa al'Absi","J. Gayle Beck","Duen Horng (Polo) Chau","Tyson Condie","David E. Conroy","Emre Ertin","Deborah Estrin","Deepak Ganesan","Cho Lam","Benjamin M. Marlin","Clay B. Marsh","Susan A. Murphy","Inbal Nahum-Shani","Kevin Patrick","James M. Rehg","Moushumi Sharmin","Vivek Shetty","Ida Sim","Bonnie Spring","Mani B. Srivastava","David W. Wetter"],"link":null,"tags":[],"title":"Center of excellence for mobile sensor data-to-knowledge (MD2K).","venue":"J. Am. Medical Informatics Assoc.","year":2015,"slug":"2015-center-of-excellence-for-mobile-sensor-datatoknowledge-md2k","ext":".md"},"path":"_publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance.md","id":"/publications/2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Hyunwoo Park","Mayank Gupta","Mark L. Braunstein","Duen Horng (Polo) Chau","Michael Thompson"],"link":null,"tags":[],"title":"A visual analytics approach to understanding care process variation and conformance.","venue":"VAHC","year":2015,"slug":"2015-a-visual-analytics-approach-to-understanding-care-process-variation-and-conformance","ext":".md"},"path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","excerpt":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","collection":"publications","content":"<p>We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","next":{"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","next":{"output":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","previous":{"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","next":{"url":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.html","relative_path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","path":"_publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data.md","id":"/publications/2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Changhyun Lee","Hannah Kim","Hanseung Lee","Zhicheng Liu","Ramakrishnan Kannan","Charles D. Stolper","John T. Stasko","Barry L. Drake","Haesun Park"],"link":null,"tags":["Alzheimer's Disease","Support Vector Machines","Visual Analytics","Data Visualization","Information Retrieval"],"title":"VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data.","venue":"IEEE VAST","year":2014,"slug":"2014-visirr-visual-analytics-for-information-retrieval-and-recommendation-with-largescale-document-data","ext":".md"},"path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","excerpt":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","collection":"publications","content":"<p>Existing evaluations of data visualizations often employ a series of low-level, detailed questions to be answered or benchmark tasks to be performed. While that methodology can be helpful to determine a visualization’s usability, such evaluations overlook the key benefits that visualization uniquely provides over other data analysis methods. I propose a value-driven evaluation of visualizations in which a person illustrates a system’s value through four important capabilities: minimizing the time to answer diverse questions, spurring the generation of insights and insightful questions, conveying the essence of the data, and generating confidence and knowledge about the data’s domain and context. Additionally, I explain how interaction is instrumental in creating much of the value that can be found in visualizations.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","collection":"publications","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","next":{"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","collection":"publications","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","next":{"output":"\n","previous":{"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","collection":"publications","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","next":{"url":"/publications/2014-valuedriven-evaluation-of-visualizations.html","relative_path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","path":"_publications/2014-valuedriven-evaluation-of-visualizations.md","id":"/publications/2014-valuedriven-evaluation-of-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Value-driven evaluation of visualizations.","venue":"BELIV","year":2014,"slug":"2014-valuedriven-evaluation-of-visualizations","ext":".md"},"path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","next":{"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","next":{"url":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.html","relative_path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","path":"_publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis.md","id":"/publications/2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Jean Scholtz","Alex Endert"],"link":null,"tags":[],"title":"User-centered design guidelines for collaborative software for intelligence analysis.","venue":"CTS","year":2014,"slug":"2014-usercentered-design-guidelines-for-collaborative-software-for-intelligence-analysis","ext":".md"},"path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","previous":{"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","next":{"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","collection":"publications","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","excerpt":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","collection":"publications","content":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","next":{"output":"\n","previous":{"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","collection":"publications","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","next":{"url":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices.html","relative_path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","path":"_publications/2014-towards-scalable-graph-computation-on-mobile-devices.md","id":"/publications/2014-towards-scalable-graph-computation-on-mobile-devices","collection":"publications","draft":false,"categories":[],"authors":["Yiqi Chen","Zhiyuan Lin","Robert Pienta","Minsuk Kahng","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Towards scalable graph computation on mobile devices.","venue":"IEEE BigData","year":2014,"slug":"2014-towards-scalable-graph-computation-on-mobile-devices","ext":".md"},"path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},{"output":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","next":{"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","next":{"output":"\n","previous":{"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","next":{"url":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.html","relative_path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","path":"_publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics.md","id":"/publications/2014-the-human-is-the-loop-new-directions-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Mahmud Shahriar Hossain","Naren Ramakrishnan","Chris North","Patrick Fiaux","Christopher Andrews"],"link":null,"tags":[],"title":"The human is the loop: new directions for visual analytics.","venue":"J. Intell. Inf. Syst.","year":2014,"slug":"2014-the-human-is-the-loop-new-directions-for-visual-analytics","ext":".md"},"path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","excerpt":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","collection":"publications","content":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","next":{"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","next":{"output":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","previous":{"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","next":{"url":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.html","relative_path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","path":"_publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration.md","id":"/publications/2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","collection":"publications","draft":false,"categories":[],"authors":["Evangelos E. Papalexakis","Tudor Dumitras","Duen Horng (Polo) Chau","B. Aditya Prakash","Christos Faloutsos"],"link":null,"tags":[],"title":"SharkFin: Spatio-temporal mining of software adoption and penetration.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-sharkfin-spatiotemporal-mining-of-software-adoption-and-penetration","ext":".md"},"path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","excerpt":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","collection":"publications","content":"<p>Alex Endert’s dissertation “Semantic Interaction for Visual Analytics: Inferring Analytical Reasoning for Model Steering” described semantic interaction, a user interaction methodology for visual analytics (VA). It showed that user interaction embodies users’ analytic process and can thus be mapped to model-steering functionality for “human-in-the-loop” system design. The dissertation contributed a framework (or pipeline) that describes such a process, a prototype VA system to test semantic interaction, and a user evaluation to demonstrate semantic interaction’s impact on the analytic process. This research is influencing current VA research and has implications for future VA research.</p>\n","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},{"output":"\n","previous":{"output":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","previous":{"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","next":{"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","excerpt":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","collection":"publications","content":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","next":{"output":"\n","previous":{"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","next":{"url":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.html","relative_path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","path":"_publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation.md","id":"/publications/2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert"],"link":null,"tags":["Cognition","Visual Analytics","Data Visualization","Semantics","Human Computer Interaction","Analytical Models","Computational Modeling","Data Models"],"title":"Semantic Interaction for Visual Analytics: Toward Coupling Cognition and Computation.","venue":"IEEE Computer Graphics and Applications","year":2014,"slug":"2014-semantic-interaction-for-visual-analytics-toward-coupling-cognition-and-computation","ext":".md"},"path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},{"output":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","next":{"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","next":{"output":"\n","previous":{"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","next":{"url":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.html","relative_path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","path":"_publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system.md","id":"/publications/2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Reflections on the evolution of the Jigsaw visual analytics system.","venue":"Inf. Vis.","year":2014,"slug":"2014-reflections-on-the-evolution-of-the-jigsaw-visual-analytics-system","ext":".md"},"path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","excerpt":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","collection":"publications","content":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","next":{"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","next":{"output":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","previous":{"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","next":{"url":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.html","relative_path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","path":"_publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks.md","id":"/publications/2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":[],"title":"Ploceus: Modeling, visualizing, and analyzing tabular data as networks.","venue":"Inf. Vis.","year":2014,"slug":"2014-ploceus-modeling-visualizing-and-analyzing-tabular-data-as-networks","ext":".md"},"path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","excerpt":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","collection":"publications","content":"<p>Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},{"output":"\n","previous":{"output":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","previous":{"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","next":{"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","excerpt":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","collection":"publications","content":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","next":{"output":"\n","previous":{"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","next":{"url":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.html","relative_path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","path":"_publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data.md","id":"/publications/2014-onset-a-visualization-technique-for-largescale-binary-set-data","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","Timothy Major","Alistair D. M. Dove","John T. Stasko"],"link":null,"tags":["Large-scale Systems","Data Visualization","Complexity Theory","Image Color Analysis","Nonhomogeneous Media"],"title":"OnSet: A Visualization Technique for Large-scale Binary Set Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-onset-a-visualization-technique-for-largescale-binary-set-data","ext":".md"},"path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},{"output":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","previous":{"output":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","previous":{"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","next":{"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","excerpt":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","collection":"publications","content":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","next":{"output":"\n","previous":{"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","next":{"url":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.html","relative_path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","path":"_publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs.md","id":"/publications/2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","collection":"publications","draft":false,"categories":[],"authors":["Robert Pienta","Acar Tamersoy","Hanghang Tong","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"MAGE: Matching approximate patterns in richly-attributed graphs.","venue":"IEEE BigData","year":2014,"slug":"2014-mage-matching-approximate-patterns-in-richlyattributed-graphs","ext":".md"},"path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","excerpt":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","collection":"publications","content":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},{"output":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","next":{"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","next":{"output":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","previous":{"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","next":{"url":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.html","relative_path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","path":"_publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries.md","id":"/publications/2014-largescale-insider-trading-analysis-patterns-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Elias B. Khalil","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Large-scale insider trading analysis: patterns and discoveries.","venue":"Soc. Netw. Anal. Min.","year":2014,"slug":"2014-largescale-insider-trading-analysis-patterns-and-discoveries","ext":".md"},"path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","excerpt":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","collection":"publications","content":"<p>The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that “a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.</p>\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","excerpt":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","collection":"publications","content":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","collection":"publications","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","next":{"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","next":{"output":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","previous":{"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","next":{"url":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.html","relative_path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","path":"_publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs.md","id":"/publications/2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Kevin A. Roundy","Duen Horng (Polo) Chau"],"link":null,"tags":["Information Systems","Security And Privacy","Information Systems Applications","Systems Security","Data Mining","Operating Systems Security"],"title":"Guilt by association: large scale malware detection by mining file-relation graphs.","venue":"KDD","year":2014,"slug":"2014-guilt-by-association-large-scale-malware-detection-by-mining-filerelation-graphs","ext":".md"},"path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","excerpt":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","collection":"publications","content":"<p>The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.</p>\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","next":{"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","next":{"output":"\n","previous":{"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","next":{"url":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.html","relative_path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","path":"_publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration.md","id":"/publications/2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Minsuk Kahng","Zhiyuan Lin","Florian Foerster","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":["Data Visualization","Semantics","Interactive Systems","Aggregates","Graph Theory"],"title":"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-glostix-graphlevel-operations-for-specifying-techniques-and-interactive-exploration","ext":".md"},"path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},{"output":"\n","previous":{"output":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","previous":{"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","next":{"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","collection":"publications","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","excerpt":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","collection":"publications","content":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","next":{"output":"\n","previous":{"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","collection":"publications","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","next":{"url":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.html","relative_path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","path":"_publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization.md","id":"/publications/2014-glos-graphlevel-operations-for-exploratory-network-visualization","collection":"publications","draft":false,"categories":[],"authors":["Charles D. Stolper","Florian Foerster","Minsuk Kahng","Zhiyuan Lin","Aakash Goel","John T. Stasko","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"GLOs: graph-level operations for exploratory network visualization.","venue":"CHI Extended Abstracts","year":2014,"slug":"2014-glos-graphlevel-operations-for-exploratory-network-visualization","ext":".md"},"path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},{"output":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","previous":{"output":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","previous":{"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","next":{"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","excerpt":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","collection":"publications","content":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","next":{"output":"\n","previous":{"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","next":{"url":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.html","relative_path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","path":"_publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1.md","id":"/publications/2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","collection":"publications","draft":false,"categories":[],"authors":["Celeste Lyn Paul","Chris Argenta","William C. Elm","Alex Endert"],"link":null,"tags":[],"title":"Future directions of humans in Big Data Research: Summary of the 1","venue":"IEEE BigData","year":2014,"slug":"2014-future-directions-of-humans-in-big-data-research-summary-of-the-1","ext":".md"},"path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","excerpt":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","collection":"publications","content":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},{"output":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","next":{"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","next":{"output":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","previous":{"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","next":{"url":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions.html","relative_path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","path":"_publications/2014-finding-waldo-learning-about-users-from-their-interactions.md","id":"/publications/2014-finding-waldo-learning-about-users-from-their-interactions","collection":"publications","draft":false,"categories":[],"authors":["Eli T. Brown","Alvitta Ottley","Helen Zhao","Quan Lin","Richard Souvenir","Alex Endert","Remco Chang"],"link":null,"tags":[],"title":"Finding Waldo: Learning about Users from their Interactions.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2014,"slug":"2014-finding-waldo-learning-about-users-from-their-interactions","ext":".md"},"path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","excerpt":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","collection":"publications","content":"<p>We present our process and analysis for VAST 2014 Mini Challenge 1 and 2, which integrate an off-the-shelf tool, Jigsaw, rapid web-based visualization prototyping using D3, and analytics-based visualizations using Matlab.</p>\n","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","excerpt":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","collection":"publications","content":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","collection":"publications","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","next":{"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","next":{"output":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","previous":{"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","next":{"url":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.html","relative_path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","path":"_publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2.md","id":"/publications/2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Yi Han","Mengdie Hu","Hannah Kim","James Nugent","Francesco Poggi","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Matlab","Organizations","Global Positioning System","Electronic Mail"],"title":"Exploring anomalies in GAStech: VAST 2014 Mini Challenge 1 and 2.","venue":"IEEE VAST","year":2014,"slug":"2014-exploring-anomalies-in-gastech-vast-2014-mini-challenge-1-and-2","ext":".md"},"path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","excerpt":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","collection":"publications","content":"<p>Tablet computers now offer screen sizes and computing capabilities that are competitive with traditional desktop PCs. Their popularity has grown tremendously, but we are just beginning to see information visualization applications designed for this platform. One potential reason for this limited development is the challenge of designing and implementing a multi-touch interface for visualizations on mobile, tablet devices. In this work, we identify the primary challenges that touch screen interactions pose for information visualization applications. We explore the design space of multi-touch interactions for visualizations and present a prototype information visualization application using a specific technique, a dynamic scatterplot, for an iPad.</p>\n","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","next":{"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","next":{"output":"\n","previous":{"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","next":{"url":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.html","relative_path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","path":"_publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer.md","id":"/publications/2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","collection":"publications","draft":false,"categories":[],"authors":["Ramik Sadana","John T. Stasko"],"link":null,"tags":["Human-centered Computing"],"title":"Designing and implementing an interactive scatterplot visualization for a tablet computer.","venue":"AVI","year":2014,"slug":"2014-designing-and-implementing-an-interactive-scatterplot-visualization-for-a-tablet-computer","ext":".md"},"path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","next":{"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","collection":"publications","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","next":{"output":"\n","previous":{"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","collection":"publications","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","next":{"url":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.html","relative_path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","path":"_publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics.md","id":"/publications/2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Characterizing the intelligence analysis process through a longitudinal field study: Implications for visual analytics.","venue":"Inf. Vis.","year":2014,"slug":"2014-characterizing-the-intelligence-analysis-process-through-a-longitudinal-field-study-implications-for-visual-analytics","ext":".md"},"path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","next":{"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","next":{"output":"\n","previous":{"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","next":{"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","collection":"publications","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","next":{"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","next":{"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","collection":"publications","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","next":{"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","next":{"output":"\n","previous":{"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","next":{"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","next":{"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","collection":"publications","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","next":{"output":"\n","previous":{"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","collection":"publications","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","next":{"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","collection":"publications","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","next":{"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","next":{"output":"\n","previous":{"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","next":{"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","next":{"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","next":{"output":"\n","previous":{"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","next":{"url":"/publications/2013-tasks-for-multivariate-network-analysis.html","relative_path":"_publications/2013-tasks-for-multivariate-network-analysis.md","path":"_publications/2013-tasks-for-multivariate-network-analysis.md","id":"/publications/2013-tasks-for-multivariate-network-analysis","collection":"publications","draft":false,"categories":[],"authors":["Johannes Pretorius","Helen C. Purchase","John T. Stasko"],"link":null,"tags":[],"title":"Tasks for Multivariate Network Analysis.","venue":"Multivariate Network Visualization","year":2013,"slug":"2013-tasks-for-multivariate-network-analysis","ext":".md"},"path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","next":{"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","collection":"publications","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","next":{"output":"\n","previous":{"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","collection":"publications","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"url":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.html","relative_path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","next":{"url":"/publications/2013-support-vector-machine-for-spatial-variation.html","relative_path":"_publications/2013-support-vector-machine-for-spatial-variation.md","path":"_publications/2013-support-vector-machine-for-spatial-variation.md","id":"/publications/2013-support-vector-machine-for-spatial-variation","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","David Cowen","Jason D. Wittenbach"],"link":null,"tags":[],"title":"Support Vector Machine for Spatial Variation.","venue":"Trans. GIS","year":2013,"slug":"2013-support-vector-machine-for-spatial-variation","ext":".md"},"path":"_publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays.md","id":"/publications/2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris Rooney","Alex Endert","Jean-Daniel Fekete","Kasper Hornb","Chris North"],"link":null,"tags":[],"title":"POWERWALL: int. workshop on interactive, ultra-high-resolution displays.","venue":"CHI Extended Abstracts","year":2013,"slug":"2013-powerwall-int-workshop-on-interactive-ultrahighresolution-displays","ext":".md"},"path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.html","relative_path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","path":"_publications/2013-typograph-multiscale-spatial-exploration-of-text-documents.md","id":"/publications/2013-typograph-multiscale-spatial-exploration-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Russ Burtner","Nick Cramer","Ralph Perko","Shawn D. Hampton","Kristin A. Cook"],"link":null,"tags":[],"title":"Typograph: Multiscale spatial exploration of text documents.","venue":"IEEE BigData","year":2013,"slug":"2013-typograph-multiscale-spatial-exploration-of-text-documents","ext":".md"},"url":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.html","relative_path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","next":{"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"path":"_publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization.md","id":"/publications/2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Trustin Clear","Mengdie Hu","Harshit Mehrotra","John T. Stasko"],"link":null,"tags":[],"title":"Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-understanding-interfirm-relationships-in-business-ecosystems-with-interactive-visualization","ext":".md"},"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","next":{"output":"\n","previous":{"url":"/publications/2013-visual-analytics-support-for-intelligence-analysis.html","relative_path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},"url":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.html","relative_path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","next":{"url":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.html","relative_path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","path":"_publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications.md","id":"/publications/2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Leman Akoglu","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"Big graph mining for the web and social media: algorithms, anomaly detection, and applications.","venue":"WSDM","year":2014,"slug":"2014-big-graph-mining-for-the-web-and-social-media-algorithms-anomaly-detection-and-applications","ext":".md"},"path":"_publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense.md","id":"/publications/2014-7-key-challenges-for-visualization-in-cyber-network-defense","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Daniel M. Best","Alex Endert","Daniel Kidwell"],"link":null,"tags":[],"title":"7 key challenges for visualization in cyber network defense.","venue":"VizSEC","year":2014,"slug":"2014-7-key-challenges-for-visualization-in-cyber-network-defense","ext":".md"},"path":"_publications/2013-visual-analytics-support-for-intelligence-analysis.md","id":"/publications/2013-visual-analytics-support-for-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Youn ah Kang","Zhicheng Liu","John T. Stasko"],"link":null,"tags":[],"title":"Visual Analytics Support for Intelligence Analysis.","venue":"Computer","year":2013,"slug":"2013-visual-analytics-support-for-intelligence-analysis","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","next":{"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","next":{"output":"\n","previous":{"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","next":{"url":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.html","relative_path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","path":"_publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections.md","id":"/publications/2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","collection":"publications","draft":false,"categories":[],"authors":["Michael J. Henry","Shawn D. Hampton","Alex Endert","Ian Roberts","Deborah Payne"],"link":null,"tags":[],"title":"MultiFacet: A Faceted Interface for Browsing Large Multimedia Collections.","venue":"ISM","year":2013,"slug":"2013-multifacet-a-faceted-interface-for-browsing-large-multimedia-collections","ext":".md"},"path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","collection":"publications","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","next":{"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","next":{"output":"\n","previous":{"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","next":{"url":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.html","relative_path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","path":"_publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs.md","id":"/publications/2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","Christos Faloutsos","Nikolaj Tatti","Hanghang Tong","Jilles Vreeken"],"link":null,"tags":[],"title":"Mining Connection Pathways for Marked Nodes in Large Graphs.","venue":"SDM","year":2013,"slug":"2013-mining-connection-pathways-for-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},{"output":"\n","previous":{"output":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","previous":{"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","next":{"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","excerpt":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","collection":"publications","content":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","next":{"output":"\n","previous":{"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","next":{"url":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.html","relative_path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","path":"_publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality.md","id":"/publications/2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Alex Endert","Kristen Koch","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Large high resolution displays for co-located collaborative sensemaking: Display usage and territoriality.","venue":"Int. J. Hum. Comput. Stud.","year":2013,"slug":"2013-large-high-resolution-displays-for-colocated-collaborative-sensemaking-display-usage-and-territoriality","ext":".md"},"path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},{"output":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","previous":{"output":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","previous":{"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","collection":"publications","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","next":{"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","collection":"publications","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","excerpt":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","collection":"publications","content":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","next":{"output":"\n","previous":{"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","collection":"publications","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","next":{"url":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.html","relative_path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","path":"_publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis.md","id":"/publications/2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","collection":"publications","draft":false,"categories":[],"authors":["Acar Tamersoy","Bo Xie","Stephen L. Lenkey","Bryan R. Routledge","Duen Horng (Polo) Chau","Shamkant B. Navathe"],"link":null,"tags":[],"title":"Inside insider trading: patterns  discoveries from a large scale exploratory analysis.","venue":"ASONAM","year":2013,"slug":"2013-inside-insider-trading-patterns--discoveries-from-a-large-scale-exploratory-analysis","ext":".md"},"path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","excerpt":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","collection":"publications","content":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},{"output":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","next":{"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","next":{"output":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","previous":{"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","next":{"url":"/publications/2013-how-analysts-cognitively-connect-the-dots.html","relative_path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","path":"_publications/2013-how-analysts-cognitively-connect-the-dots.md","id":"/publications/2013-how-analysts-cognitively-connect-the-dots","collection":"publications","draft":false,"categories":[],"authors":["Lauren Bradel","Jessica Zeitz Self","Alex Endert","Mahmud Shahriar Hossain","Chris North","Naren Ramakrishnan"],"link":null,"tags":[],"title":"How analysts cognitively \"connect the dots\".","venue":"ISI","year":2013,"slug":"2013-how-analysts-cognitively-connect-the-dots","ext":".md"},"path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","excerpt":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","collection":"publications","content":"<p>This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations’ information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users’ computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.</p>\n","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","excerpt":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","collection":"publications","content":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},{"output":"\n","previous":{"output":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","previous":{"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","next":{"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","collection":"publications","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","excerpt":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","collection":"publications","content":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","next":{"output":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","previous":{"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","collection":"publications","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","next":{"url":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.html","relative_path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","path":"_publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity.md","id":"/publications/2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","collection":"publications","draft":false,"categories":[],"authors":["Ted E. Senator","Henry G. Goldberg","Alex Memory","William T. Young","Brad Rees","Robert Pierce","Daniel Huang","Matthew Reardon","David A. Bader","Edmond Chow","Irfan A. Essa","Joshua Jones","Vinay Bettadapura","Duen Horng (Polo) Chau","Oded Green","Oguz Kaya","Anita Zakrzewska","Erica Briscoe","Rudolph L. Mappus IV","Robert McColl","Lora Weiss","Thomas G. Dietterich","Alan Fern","Weng-Keen Wong","Shubhomoy Das","Andrew Emmott","Jed Irvine","Jay Yoon Lee","Danai Koutra","Christos Faloutsos","Daniel D. Corkill","Lisa Friedland","Amanda Gentzel","David D. Jensen"],"link":null,"tags":["Information Systems Applications","Data Mining","Information Systems"],"title":"Detecting insider threats in a real corporate database of computer usage activity.","venue":"KDD","year":2013,"slug":"2013-detecting-insider-threats-in-a-real-corporate-database-of-computer-usage-activity","ext":".md"},"path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","excerpt":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","collection":"publications","content":"<p>Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},{"output":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","next":{"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","next":{"url":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.html","relative_path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","path":"_publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw.md","id":"/publications/2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Jaeyeon Kihm","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Algorithm Design And Analysis","Tag Clouds","Text Analysis","Measurement","Computational Modeling"],"title":"Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2013,"slug":"2013-combining-computational-analyses-and-interactive-visualization-for-document-exploration-and-sensemaking-in-jigsaw","ext":".md"},"path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","excerpt":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","collection":"publications","content":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","previous":{"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","next":{"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","excerpt":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","collection":"publications","content":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","next":{"output":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","previous":{"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","next":{"url":"/publications/2013-bixplorer-visual-analytics-with-biclusters.html","relative_path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","path":"_publications/2013-bixplorer-visual-analytics-with-biclusters.md","id":"/publications/2013-bixplorer-visual-analytics-with-biclusters","collection":"publications","draft":false,"categories":[],"authors":["Patrick Fiaux","Maoyuan Sun","Lauren Bradel","Chris North","Naren Ramakrishnan","Alex Endert"],"link":null,"tags":[],"title":"Bixplorer: Visual Analytics with Biclusters.","venue":"Computer","year":2013,"slug":"2013-bixplorer-visual-analytics-with-biclusters","ext":".md"},"path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","excerpt":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","collection":"publications","content":"<p>To tackle the onset of big data, visual analytics seeks to marry the human intuition of visualization with mathematical models’ analytical horsepower. A critical question is, how will humans interact with and steer these complex models? Initially, users applied direct manipulation to such models the same way they applied it to simpler visualizations in the premodel era–using control panels to directly manipulate model parameters. However, opportunities are arising for direct manipulation of the model outputs, where the users’ thought processes take place, rather than the inputs. This article presents this new agenda for direct manipulation for visual analytics.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},{"output":"\n","previous":{"output":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","previous":{"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","collection":"publications","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","next":{"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","collection":"publications","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","excerpt":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","collection":"publications","content":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","next":{"output":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","previous":{"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","collection":"publications","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","next":{"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","excerpt":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","collection":"publications","content":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},{"output":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","next":{"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","next":{"output":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","previous":{"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","next":{"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","collection":"publications","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","excerpt":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","collection":"publications","content":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","excerpt":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","collection":"publications","content":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},{"output":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","previous":{"output":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","previous":{"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","collection":"publications","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","next":{"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","excerpt":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","collection":"publications","content":"<p>We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a “close-by” group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","next":{"output":"\n","previous":{"url":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.html","relative_path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","collection":"publications","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},"url":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.html","relative_path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","next":{"url":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.html","relative_path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","path":"_publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics.md","id":"/publications/2013-beyond-control-panels-direct-manipulation-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Chris North"],"link":null,"tags":["Mathematical Model","Visual Analytics","Visualization","Data Visualization","Analytical Models","Computational Modeling"],"title":"Beyond Control Panels: Direct Manipulation for Visual Analytics.","venue":"IEEE Computer Graphics and Applications","year":2013,"slug":"2013-beyond-control-panels-direct-manipulation-for-visual-analytics","ext":".md"},"path":"_publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data.md","id":"/publications/2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jaegul Choo","Hanseung Lee","Zhicheng Liu","John T. Stasko","Haesun Park"],"link":null,"tags":[],"title":"An interactive visual testbed system for dimension reduction and clustering of large-scale high-dimensional data.","venue":"Visualization and Data Analysis","year":2013,"slug":"2013-an-interactive-visual-testbed-system-for-dimension-reduction-and-clustering-of-largescale-highdimensional-data","ext":".md"},"path":"_publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence.md","id":"/publications/2012-visual-analytics-for-convergingbusinessecosystem-intelligence","excerpt":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","collection":"publications","content":"<p>Visual analytics (VA)-the fusion of analytical reasoning and computational data analysis with rich, interactive visual representations-promises to provide many relevant techniques for business-ecosystem-intelligence systems. However, the effectiveness of such systems requires the careful curation of complex, heterogeneous, and distributed data; an in-depth understanding of the business ecosystem context and end-user domain; and the corresponding design of relevant visualizations and metrics.</p>\n","draft":false,"categories":[],"authors":["Rahul C. Basole","Mengdie Hu","Pritesh Patel","John T. Stasko"],"link":null,"tags":["Ecosystems","Data Visualization","Interactive Systems","Computational Intelligence","Analytical Models","Business"],"title":"Visual Analytics for Converging-Business-Ecosystem Intelligence.","venue":"IEEE Computer Graphics and Applications","year":2012,"slug":"2012-visual-analytics-for-convergingbusinessecosystem-intelligence","ext":".md"},{"output":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","collection":"publications","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","next":{"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","next":{"output":"\n","previous":{"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","next":{"url":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.html","relative_path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","path":"_publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs.md","id":"/publications/2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Information Retrieval","Information Systems"],"title":"TourViz: interactive visualization of connection pathways in large graphs.","venue":"KDD","year":2012,"slug":"2012-tourviz-interactive-visualization-of-connection-pathways-in-large-graphs","ext":".md"},"path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","excerpt":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","collection":"publications","content":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","collection":"publications","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","next":{"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","collection":"publications","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","next":{"output":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","previous":{"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","collection":"publications","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","next":{"url":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections.html","relative_path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","path":"_publications/2012-topicviz-interactive-topic-exploration-in-document-collections.md","id":"/publications/2012-topicviz-interactive-topic-exploration-in-document-collections","collection":"publications","draft":false,"categories":[],"authors":["Jacob Eisenstein","Duen Horng (Polo) Chau","Aniket Kittur","Eric P. Xing"],"link":null,"tags":[],"title":"TopicViz: interactive topic exploration in document collections.","venue":"CHI Extended Abstracts","year":2012,"slug":"2012-topicviz-interactive-topic-exploration-in-document-collections","ext":".md"},"path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","excerpt":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","collection":"publications","content":"<p>Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users’ intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users’ interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","previous":{"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","collection":"publications","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","next":{"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","collection":"publications","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","excerpt":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","collection":"publications","content":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","next":{"output":"\n","previous":{"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","collection":"publications","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","next":{"url":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.html","relative_path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","path":"_publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents.md","id":"/publications/2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Seth Fox","Dipayan Maiti","Scotland Leman","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"The semantics of clustering: analysis of user-generated spatializations of text documents.","venue":"AVI","year":2012,"slug":"2012-the-semantics-of-clustering-analysis-of-usergenerated-spatializations-of-text-documents","ext":".md"},"path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},{"output":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","previous":{"output":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","previous":{"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","next":{"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","collection":"publications","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","excerpt":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","collection":"publications","content":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","next":{"output":"\n","previous":{"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","collection":"publications","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","next":{"url":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.html","relative_path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","path":"_publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning.md","id":"/publications/2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","collection":"publications","draft":false,"categories":[],"authors":["Sharon Lynn Chu Yew Yee","Francis K. H. Quek","Alex Endert","Haeyong Chung","Blake Sawyer"],"link":null,"tags":[],"title":"The Physicality of Technological Devices in Education: Building a Digital Experience for Learning.","venue":"ICALT","year":2012,"slug":"2012-the-physicality-of-technological-devices-in-education-building-a-digital-experience-for-learning","ext":".md"},"path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","excerpt":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","collection":"publications","content":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},{"output":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","previous":{"output":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","previous":{"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","next":{"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","collection":"publications","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","excerpt":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","collection":"publications","content":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","next":{"output":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","previous":{"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","collection":"publications","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","next":{"url":"/publications/2012-the-parallel-coordinates-matrix.html","relative_path":"_publications/2012-the-parallel-coordinates-matrix.md","path":"_publications/2012-the-parallel-coordinates-matrix.md","id":"/publications/2012-the-parallel-coordinates-matrix","collection":"publications","draft":false,"categories":[],"authors":["Julian Heinrich","John T. Stasko","Daniel Weiskopf"],"link":null,"tags":[],"title":"The Parallel Coordinates Matrix.","venue":"EuroVis (Short Papers)","year":2012,"slug":"2012-the-parallel-coordinates-matrix","ext":".md"},"path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","excerpt":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","collection":"publications","content":"<p>Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.</p>\n","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","excerpt":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","collection":"publications","content":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},{"output":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","previous":{"output":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","previous":{"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","next":{"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","excerpt":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","collection":"publications","content":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","next":{"output":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","previous":{"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","next":{"url":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.html","relative_path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","path":"_publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems.md","id":"/publications/2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","collection":"publications","draft":false,"categories":[],"authors":["Nathalie Henry Riche","Kori Inkpen","John T. Stasko","Tom Gross","Mary Czerwinski"],"link":null,"tags":["Collaborative And Social Computing","Collaborative And Social Computing Systems And Tools","Human-centered Computing","Asynchronous Editors"],"title":"Supporting asynchronous collaboration in visual analytics systems.","venue":"AVI","year":2012,"slug":"2012-supporting-asynchronous-collaboration-in-visual-analytics-systems","ext":".md"},"path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","excerpt":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","collection":"publications","content":"<p>Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today’s sports analyst’s routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.</p>\n","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","excerpt":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","collection":"publications","content":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},{"output":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","next":{"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","next":{"output":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","previous":{"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","next":{"url":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.html","relative_path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","path":"_publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics.md","id":"/publications/2012-snapshot-visualization-to-propel-ice-hockey-analytics","collection":"publications","draft":false,"categories":[],"authors":["Hannah Pileggi","Charles D. Stolper","J. Michael Boyle","John T. Stasko"],"link":null,"tags":["Data Visualization","Human Computer Interaction","Sports Equipment","Games","Knowledge Discovery"],"title":"SnapShot: Visualization to Propel Ice Hockey Analytics.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-snapshot-visualization-to-propel-ice-hockey-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","excerpt":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","collection":"publications","content":"<p>Visual analytics emphasizes sensemaking of large, complex datasets through interactively exploring visualizations generated by statistical models. For example, dimensionality reduction methods use various similarity metrics to visualize textual document collections in a spatial metaphor, where similarities between documents are approximately represented through their relative spatial distances to each other in a 2D layout. This metaphor is designed to mimic analysts’ mental models of the document collection and support their analytic processes, such as clustering similar documents together. However, in current methods, users must interact with such visualizations using controls external to the visual metaphor, such as sliders, menus, or text fields, to directly control underlying model parameters that they do not understand and that do not relate to their analytic process occurring within the visual metaphor. In this paper, we present the opportunity for a new design space for visual analytic interaction, called semantic interaction, which seeks to enable analysts to spatially interact with such models directly within the visual metaphor using interactions that derive from their analytic process, such as searching, highlighting, annotating, and repositioning documents. Further, we demonstrate how semantic interactions can be implemented using machine learning techniques in a visual analytic tool, called ForceSPIRE, for interactive analysis of textual data within a spatial visualization. Analysts can express their expert domain knowledge about the documents by simply moving them, which guides the underlying model to improve the overall layout, taking the user’s feedback into account.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","excerpt":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","collection":"publications","content":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},{"output":"\n","previous":{"output":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","previous":{"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","next":{"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","excerpt":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","collection":"publications","content":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","next":{"output":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","previous":{"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","next":{"url":"/publications/2012-semantic-interaction-for-visual-text-analytics.html","relative_path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","path":"_publications/2012-semantic-interaction-for-visual-text-analytics.md","id":"/publications/2012-semantic-interaction-for-visual-text-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Semantic interaction for visual text analytics.","venue":"CHI","year":2012,"slug":"2012-semantic-interaction-for-visual-text-analytics","ext":".md"},"path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","excerpt":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","collection":"publications","content":"<p>Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users’ analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user’s reasoning and intuition.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},{"output":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","previous":{"output":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","previous":{"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","next":{"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","excerpt":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","collection":"publications","content":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","next":{"output":"\n","previous":{"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","next":{"url":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.html","relative_path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","path":"_publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering.md","id":"/publications/2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Chris North"],"link":null,"tags":["Mathematical Model","User Interfaces","Visual Analytics","Semantics","Analytical Models"],"title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-semantic-interaction-for-sensemaking-inferring-analytical-reasoning-for-model-steering","ext":".md"},"path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","excerpt":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","collection":"publications","content":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},{"output":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","next":{"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","next":{"output":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","previous":{"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","next":{"url":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.html","relative_path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","path":"_publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud.md","id":"/publications/2012-pegasus-mining-billionscale-graphs-in-the-cloud","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Pegasus: Mining billion-scale graphs in the cloud.","venue":"ICASSP","year":2012,"slug":"2012-pegasus-mining-billionscale-graphs-in-the-cloud","ext":".md"},"path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","excerpt":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","collection":"publications","content":"<p>Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes’ vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph.</p>\n","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","excerpt":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","collection":"publications","content":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},{"output":"\n","previous":{"output":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","previous":{"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","next":{"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","excerpt":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","collection":"publications","content":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","next":{"output":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","previous":{"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","next":{"url":"/publications/2012-opavion-mining-and-visualization-in-large-graphs.html","relative_path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","path":"_publications/2012-opavion-mining-and-visualization-in-large-graphs.md","id":"/publications/2012-opavion-mining-and-visualization-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Leman Akoglu","Duen Horng (Polo) Chau","U Kang","Danai Koutra","Christos Faloutsos"],"link":null,"tags":["Data Mining","Information Systems","Information Systems Applications"],"title":"OPAvion: mining and visualization in large graphs.","venue":"SIGMOD Conference","year":2012,"slug":"2012-opavion-mining-and-visualization-in-large-graphs","ext":".md"},"path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","excerpt":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","collection":"publications","content":"<p>Clustering plays an important role in many large‐scale data analyses providing users with an overall understanding of their data. Nonetheless, clustering is not an easy task due to noisy features and outliers existing in the data, and thus the clustering results obtained from automatic algorithms often do not make clear sense. To remedy this problem, automatic clustering should be complemented with interactive visualization strategies. This paper proposes an interactive visual analytics system for document clustering, called iVisClustering, based on a widely‐used topic modeling method, latent Dirichlet allocation (LDA). iVisClustering provides a summary of each cluster in terms of its most representative keywords and visualizes soft clustering results in parallel coordinates. The main view of the system provides a 2D plot that visualizes cluster similarities and the relation among data items with a graph‐based representation. iVisClustering provides several other views, which contain useful interaction methods. With help of these visualization modules, we can interactively refine the clustering results in various ways. Keywords can be adjusted so that they characterize each cluster better. In addition, our system can filter out noisy data and re‐cluster the data accordingly. Cluster hierarchy can be constructed using a tree structure and for this purpose, the system supports cluster‐level interactions such as sub‐clustering, removing unimportant clusters, merging the clusters that have similar meanings, and moving certain clusters to any other node in the tree structure. Furthermore, the system provides document‐level interactions such as moving mis‐clustered documents to another cluster and removing useless documents. Finally, we present how interactive clustering is performed via iVisClustering by using real‐world document data sets.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},{"output":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","previous":{"output":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","previous":{"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","next":{"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","excerpt":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","collection":"publications","content":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","next":{"output":"\n","previous":{"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","next":{"url":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.html","relative_path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","path":"_publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling.md","id":"/publications/2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaeyeon Kihm","Jaegul Choo","John T. Stasko","Haesun Park"],"link":null,"tags":["H.1.2","H.2.8"],"title":"iVisClustering: An Interactive Visual Document Clustering via Topic Modeling.","venue":"Comput. Graph. Forum","year":2012,"slug":"2012-ivisclustering-an-interactive-visual-document-clustering-via-topic-modeling","ext":".md"},"path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","excerpt":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","collection":"publications","content":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},{"output":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","previous":{"output":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","previous":{"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","collection":"publications","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","next":{"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","excerpt":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","collection":"publications","content":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","next":{"output":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","previous":{"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","next":{"url":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.html","relative_path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","path":"_publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs.md","id":"/publications/2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Leman Akoglu","Jilles Vreeken","Hanghang Tong","Christos Faloutsos"],"link":null,"tags":[],"title":"Interactively and Visually Exploring Tours of Marked Nodes in Large Graphs.","venue":"ASONAM","year":2012,"slug":"2012-interactively-and-visually-exploring-tours-of-marked-nodes-in-large-graphs","ext":".md"},"path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","excerpt":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","collection":"publications","content":"<p>With the growing need for visualization to aid users in understanding large, complex datasets, the ability for users to interact and explore these datasets is critical. As visual analytic systems have advanced to leverage powerful computational models and data analytics capabilities, the modes by which users engage and interact with the information are limited. Often, users are taxed with directly manipulating parameters of these models through traditional GUIs (e.g., using sliders to directly manipulate the value of a parameter). However, the purpose of user interaction in visual analytic systems is to enable visual data exploration – where users can focus on their task, as opposed to the tool or system. As a result, users can engage freely in data exploration and decision-making, for the purpose of gaining insight. In this position paper, we discuss how evaluating visual analytic systems can be approached through user interaction analysis, where the goal is to minimize the cognitive translation between the visual metaphor and the mode of interaction (i.e., reducing the “interaction junk”). We motivate this concept through a discussion of traditional GUIs used in visual analytics for direct manipulation of model parameters, and the importance of designing interactions the support visual data exploration.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","excerpt":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","collection":"publications","content":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},{"output":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","previous":{"output":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","previous":{"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","collection":"publications","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","next":{"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","excerpt":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","collection":"publications","content":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","next":{"output":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","previous":{"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","next":{"url":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.html","relative_path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","path":"_publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems.md","id":"/publications/2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Interaction junk: user interaction-based evaluation of visual analytic systems.","venue":"BELIV","year":2012,"slug":"2012-interaction-junk-user-interactionbased-evaluation-of-visual-analytic-systems","ext":".md"},"path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","excerpt":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","collection":"publications","content":"<p>Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.</p>\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","excerpt":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","collection":"publications","content":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},{"output":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","next":{"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","collection":"publications","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","next":{"output":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","previous":{"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","collection":"publications","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","next":{"url":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.html","relative_path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","path":"_publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays.md","id":"/publications/2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"How spatial layout, interactivity, and persistent visibility affect learning with large displays.","venue":"AVI","year":2012,"slug":"2012-how-spatial-layout-interactivity-and-persistent-visibility-affect-learning-with-large-displays","ext":".md"},"path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","excerpt":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","collection":"publications","content":"<p>While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","excerpt":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","collection":"publications","content":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},{"output":"\n","previous":{"output":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","previous":{"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","next":{"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","collection":"publications","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","excerpt":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","collection":"publications","content":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","next":{"output":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","previous":{"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","collection":"publications","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","next":{"url":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.html","relative_path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","path":"_publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts.md","id":"/publications/2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Market Research","Visual Analytics","Data Visualization","Qualitative Analysis","Electronic Mail"],"title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2012,"slug":"2012-examining-the-use-of-a-visual-analytics-system-for-sensemaking-tasks-case-studies-with-domain-experts","ext":".md"},"path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","excerpt":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","collection":"publications","content":"<p>In this paper, we introduce a GPU-accelerated visual analytics tool, AVIST. By adopting the in-situ visualization architecture on the GPUs, AVIST supports real-time data analysis and visualization of massive scale datasets, such as VAST 2012 Challenge dataset. The design objective of the tool is to identify temporal patterns from large and complex data. To achieve this goal, we introduce three unique features: automatic animation, disjunctive data filters, and time-synced visualization of multiple datasets.</p>\n","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},{"output":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","previous":{"output":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","previous":{"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","next":{"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","excerpt":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","collection":"publications","content":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","next":{"output":"\n","previous":{"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","next":{"url":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.html","relative_path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","path":"_publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays.md","id":"/publications/2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","collection":"publications","draft":false,"categories":[],"authors":["Yong Cao","Reese Moore","Peng Mi","Alex Endert","Chris North","Randy C. Marchany"],"link":null,"tags":[],"title":"Dynamic analysis of large datasets with animated and correlated views: VAST 2012 Mini Challenge # award: Honorable mention for good use of coordinated displays.","venue":"IEEE VAST","year":2012,"slug":"2012-dynamic-analysis-of-large-datasets-with-animated-and-correlated-views-vast-2012-mini-challenge--award-honorable-mention-for-good-use-of-coordinated-displays","ext":".md"},"path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","excerpt":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","collection":"publications","content":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},{"output":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","next":{"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","next":{"output":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","previous":{"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","next":{"url":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.html","relative_path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","path":"_publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data.md","id":"/publications/2012-discovering-spatial-patterns-in-origindestination-mobility-data","collection":"publications","draft":false,"categories":[],"authors":["Diansheng Guo","Xi Zhu","Hai Jin","Peng Gao","Clio Andris"],"link":null,"tags":[],"title":"Discovering Spatial Patterns in Origin-Destination Mobility Data.","venue":"Trans. GIS","year":2012,"slug":"2012-discovering-spatial-patterns-in-origindestination-mobility-data","ext":".md"},"path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","excerpt":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","collection":"publications","content":"<p>Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users’ perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","excerpt":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","collection":"publications","content":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},{"output":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","previous":{"output":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","previous":{"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","next":{"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","excerpt":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","collection":"publications","content":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","next":{"output":"\n","previous":{"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","next":{"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","excerpt":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","collection":"publications","content":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},{"output":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","next":{"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","next":{"output":"\n","previous":{"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","next":{"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","collection":"publications","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","excerpt":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","collection":"publications","content":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","next":{"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","next":{"output":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","previous":{"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","next":{"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","excerpt":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","collection":"publications","content":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","next":{"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","next":{"output":"\n","previous":{"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","next":{"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","next":{"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","next":{"output":"\n","previous":{"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","collection":"publications","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","next":{"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},{"output":"\n","previous":{"output":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","previous":{"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","next":{"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","excerpt":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","collection":"publications","content":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","next":{"output":"\n","previous":{"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"url":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.html","relative_path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","next":{"url":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.html","relative_path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","path":"_publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing.md","id":"/publications/2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","collection":"publications","draft":false,"categories":[],"authors":["Eric D. Ragan","Alex Endert","Doug A. Bowman","Francis K. H. Quek"],"link":null,"tags":[],"title":"The effects of spatial layout and view control on cognitive processing.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-the-effects-of-spatial-layout-and-view-control-on-cognitive-processing","ext":".md"},"path":"_publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays.md","id":"/publications/2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ankit Singh","Lauren Bradel","Alex Endert","Robert Kincaid","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"Supporting the cyber analytic process using visual history on large displays.","venue":"VizSEC","year":2011,"slug":"2011-supporting-the-cyber-analytic-process-using-visual-history-on-large-displays","ext":".md"},"path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},{"output":"\n","previous":{"output":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","previous":{"url":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.html","relative_path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","path":"_publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms.md","id":"/publications/2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","collection":"publications","draft":false,"categories":[],"authors":["Danai Koutra","Tai-You Ke","U Kang","Duen Horng (Polo) Chau","Hsing-Kuo Kenneth Pao","Christos Faloutsos"],"link":null,"tags":[],"title":"Unifying Guilt-by-Association Approaches: Theorems and Fast Algorithms.","venue":"ECML/PKDD (2)","year":2011,"slug":"2011-unifying-guiltbyassociation-approaches-theorems-and-fast-algorithms","ext":".md"},"url":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.html","relative_path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","next":{"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"path":"_publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays.md","id":"/publications/2011-visual-encodings-that-support-physical-navigation-on-large-displays","excerpt":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","collection":"publications","content":"<p>Visual encodings are the medium through which information is displayed, perceived, interpreted, and finally transferred from a visualization to the user. Traditionally, such encodings display information as representations of length, color, size, slope, position, and other glyphs. Guidelines for such encodings have been proposed, but they generally assume a small display, small datasets, and a relatively static user. Large, high-resolution visualizations are able to display far more information simultaneously, allowing users to leverage physical navigation (movement) as an effective interaction through which to explore the data space. In this paper, we analyze if and how the choice of visual encodings for large, high-resolution visualizations affects physical navigation, and ultimately task performance for a spatial information visualization task.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Yueh-Hua Lee","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Visual encodings that support physical navigation on large displays.","venue":"Graphics Interface","year":2011,"slug":"2011-visual-encodings-that-support-physical-navigation-on-large-displays","ext":".md"},"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","next":{"output":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","previous":{"url":"/publications/2011-weighted-radial-variation-for-node-feature-classification.html","relative_path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},"url":"/publications/2012-breaking-news-on-twitter.html","relative_path":"_publications/2012-breaking-news-on-twitter.md","next":{"url":"/publications/2012-designing-large-highresolution-display-workspaces.html","relative_path":"_publications/2012-designing-large-highresolution-display-workspaces.md","path":"_publications/2012-designing-large-highresolution-display-workspaces.md","id":"/publications/2012-designing-large-highresolution-display-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Lauren Bradel","Jessica Zeitz","Christopher Andrews","Chris North"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Designing large high-resolution display workspaces.","venue":"AVI","year":2012,"slug":"2012-designing-large-highresolution-display-workspaces","ext":".md"},"path":"_publications/2012-breaking-news-on-twitter.md","id":"/publications/2012-breaking-news-on-twitter","excerpt":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","collection":"publications","content":"<p>After the news of Osama Bin Laden’s death leaked through Twitter, many people wondered if Twitter would fundamentally change the way we produce, spread, and consume news. In this paper we provide an in-depth analysis of how the news broke and spread on Twitter. We confirm the claim that Twitter broke the news first, and find evidence that Twitter had convinced a large number of its audience before mainstream media confirmed the news. We also discover that attention on Twitter was highly concentrated on a small number of “opinion leaders” and identify three groups of opinion leaders who played key roles in spreading the news: individuals affiliated with media played a large part in breaking the news, mass media brought the news to a wider audience and provided eager Twitter users with content on external sites, and celebrities helped to spread the news and stimulate conversation. Our findings suggest Twitter has great potential as a news medium.</p>\n","draft":false,"categories":[],"authors":["Mengdie Hu","Shixia Liu","Furu Wei","Yingcai Wu","John T. Stasko","Kwan-Liu Ma"],"link":null,"tags":["Human-centered Computing"],"title":"Breaking news on twitter.","venue":"CHI","year":2012,"slug":"2012-breaking-news-on-twitter","ext":".md"},"path":"_publications/2011-weighted-radial-variation-for-node-feature-classification.md","id":"/publications/2011-weighted-radial-variation-for-node-feature-classification","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Clio Andris"],"link":null,"tags":[],"title":"Weighted Radial Variation for Node Feature Classification","venue":"arXiv","year":2011,"slug":"2011-weighted-radial-variation-for-node-feature-classification","ext":".md"},{"output":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","next":{"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","next":{"output":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","previous":{"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","next":{"url":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.html","relative_path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","path":"_publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities.md","id":"/publications/2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","collection":"publications","draft":false,"categories":[],"authors":["Clio Andris","Samuel Halverson","Frank Hardisty"],"link":null,"tags":[],"title":"Predicting migration system dynamics with conditional and posterior probabilities.","venue":"ICSDM","year":2011,"slug":"2011-predicting-migration-system-dynamics-with-conditional-and-posterior-probabilities","ext":".md"},"path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","excerpt":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","collection":"publications","content":"<p>In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","excerpt":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","collection":"publications","content":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","collection":"publications","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","next":{"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","next":{"output":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","previous":{"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","next":{"url":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.html","relative_path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","path":"_publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics.md","id":"/publications/2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Chao Han","Dipayan Maiti","Leanna House","Scotland Leman","Chris North"],"link":null,"tags":["Layout","Visual Analytics","Data Visualization","Analytical Models","Data Models","Principal Component Analysis"],"title":"Observation-level interaction with statistical models for visual analytics.","venue":"IEEE VAST","year":2011,"slug":"2011-observationlevel-interaction-with-statistical-models-for-visual-analytics","ext":".md"},"path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","excerpt":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","collection":"publications","content":"<p>Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},{"output":"\n","previous":{"output":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","previous":{"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","next":{"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","excerpt":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","collection":"publications","content":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","next":{"output":"\n","previous":{"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","next":{"url":"/publications/2011-networkbased-visual-analysis-of-tabular-data.html","relative_path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","path":"_publications/2011-networkbased-visual-analysis-of-tabular-data.md","id":"/publications/2011-networkbased-visual-analysis-of-tabular-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Shamkant B. Navathe","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Semantics","Organizations","Relational Databases","Aggregates","Cities And Towns"],"title":"Network-based visual analysis of tabular data.","venue":"IEEE VAST","year":2011,"slug":"2011-networkbased-visual-analysis-of-tabular-data","ext":".md"},"path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},{"output":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","next":{"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","collection":"publications","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","next":{"output":"\n","previous":{"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","collection":"publications","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","next":{"url":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.html","relative_path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","path":"_publications/2011-mining-large-graphs-algorithms-inference-and-discoveries.md","id":"/publications/2011-mining-large-graphs-algorithms-inference-and-discoveries","collection":"publications","draft":false,"categories":[],"authors":["U Kang","Duen Horng (Polo) Chau","Christos Faloutsos"],"link":null,"tags":[],"title":"Mining large graphs: Algorithms, inference, and discoveries.","venue":"ICDE","year":2011,"slug":"2011-mining-large-graphs-algorithms-inference-and-discoveries","ext":".md"},"path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","excerpt":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","collection":"publications","content":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","next":{"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","next":{"output":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","previous":{"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","next":{"url":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.html","relative_path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","path":"_publications/2011-large-scale-graph-mining-and-inference-for-malware-detection.md","id":"/publications/2011-large-scale-graph-mining-and-inference-for-malware-detection","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Carey Nachenberg","Jeffrey Wilhelm","Adam Wright","Christos Faloutsos"],"link":null,"tags":[],"title":"Large Scale Graph Mining and Inference for Malware Detection.","venue":"SDM","year":2011,"slug":"2011-large-scale-graph-mining-and-inference-for-malware-detection","ext":".md"},"path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","excerpt":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","collection":"publications","content":"<p>This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw’s computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.</p>\n","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},{"output":"\n","previous":{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","previous":{"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","collection":"publications","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","next":{"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","next":{"output":"\n","previous":{"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","next":{"url":"/publications/2011-jigsaw-to-save-vastopolis.html","relative_path":"_publications/2011-jigsaw-to-save-vastopolis.md","path":"_publications/2011-jigsaw-to-save-vastopolis.md","id":"/publications/2011-jigsaw-to-save-vastopolis","collection":"publications","draft":false,"categories":[],"authors":["Elizabeth Braunstein","Carsten G","Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Text Analysis","Organizations","Manuals","Information Retrieval","Electronic Mail"],"title":"Jigsaw to save vastopolis.","venue":"IEEE VAST","year":2011,"slug":"2011-jigsaw-to-save-vastopolis","ext":".md"},"path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","previous":{"output":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","previous":{"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","next":{"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","excerpt":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","collection":"publications","content":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","next":{"output":"\n","previous":{"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","next":{"url":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions.html","relative_path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","path":"_publications/2011-information-visualization-state-of-the-field-and-new-research-directions.md","id":"/publications/2011-information-visualization-state-of-the-field-and-new-research-directions","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":[],"title":"Information visualization: State of the field and new research directions.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-state-of-the-field-and-new-research-directions","ext":".md"},"path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},{"output":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","next":{"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","collection":"publications","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","next":{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","previous":{"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","collection":"publications","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","next":{"url":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.html","relative_path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","path":"_publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities.md","id":"/publications/2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Beth Yost","Chris North"],"link":null,"tags":[],"title":"Information visualization on large, high-resolution displays: Issues, challenges, and opportunities.","venue":"Inf. Vis.","year":2011,"slug":"2011-information-visualization-on-large-highresolution-displays-issues-challenges-and-opportunities","ext":".md"},"path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","excerpt":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","collection":"publications","content":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},{"output":"\n","previous":{"output":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","previous":{"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","collection":"publications","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","next":{"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","excerpt":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","collection":"publications","content":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","next":{"output":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","previous":{"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","next":{"url":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.html","relative_path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","path":"_publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation.md","id":"/publications/2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Control Systems","Computational Intelligence","Availability","Analytical Models","Computational Modeling","Performance Analysis","Information Analysis"],"title":"How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2011,"slug":"2011-how-can-visual-analytics-assist-investigative-analysis-design-implications-from-an-evaluation","ext":".md"},"path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","excerpt":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","collection":"publications","content":"<p>Lessons learned from developing four graph analytics applications reveal good research practices and grand challenges for future research. The application domains include electric-power-grid analytics, social-network and citation analytics, text and document analytics, and knowledge domain analytics.</p>\n","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},{"output":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","previous":{"output":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","previous":{"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","next":{"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","excerpt":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","collection":"publications","content":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","next":{"output":"\n","previous":{"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","next":{"url":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead.html","relative_path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","path":"_publications/2011-graph-analyticslessons-learned-and-challenges-ahead.md","id":"/publications/2011-graph-analyticslessons-learned-and-challenges-ahead","collection":"publications","draft":false,"categories":[],"authors":["Pak Chung Wong","Chaomei Chen","Carsten G","Ben Shneiderman","John T. Stasko","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Power Grids","Green Products","Tools","Data Mining"],"title":"Graph Analytics-Lessons Learned and Challenges Ahead.","venue":"IEEE Computer Graphics and Applications","year":2011,"slug":"2011-graph-analyticslessons-learned-and-challenges-ahead","ext":".md"},"path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","excerpt":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","collection":"publications","content":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","collection":"publications","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","next":{"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","collection":"publications","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","next":{"output":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","previous":{"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","collection":"publications","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","next":{"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","excerpt":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","collection":"publications","content":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},{"output":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","next":{"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","next":{"output":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","previous":{"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","next":{"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","excerpt":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","collection":"publications","content":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","excerpt":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","collection":"publications","content":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},{"output":"\n","previous":{"output":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","previous":{"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","next":{"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","excerpt":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","collection":"publications","content":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","next":{"output":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","previous":{"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","next":{"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","collection":"publications","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","excerpt":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","collection":"publications","content":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},{"output":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","previous":{"output":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","previous":{"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","collection":"publications","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","next":{"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","excerpt":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","collection":"publications","content":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","next":{"output":"\n","previous":{"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","next":{"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","excerpt":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","collection":"publications","content":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},{"output":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","next":{"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","next":{"output":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","previous":{"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"url":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.html","relative_path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","next":{"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"path":"_publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning.md","id":"/publications/2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","excerpt":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","collection":"publications","content":"<p>Extracting useful knowledge from large network datasets has become a fundamental challenge in many domains, from scientific literature to social networks and the web. We introduce Apolo, a system that uses a mixed-initiative approach - combining visualization, rich user interaction and machine learning - to guide the user to incrementally and interactively explore large network data and make sense of it. Apolo engages the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. Apolo also helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We evaluated Apolo with twelve participants in a between-subjects study, with the task being to find relevant new papers to update an existing survey paper. Using expert judges, participants using Apolo found significantly more relevant papers. Subjective feedback of Apolo was also very positive.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Network Services","Network Monitoring","Computing Methodologies","Machine Learning","Human-centered Computing","Human Computer Interaction (hci)","Networks"],"title":"Apolo: making sense of large network data by combining rich user interaction and machine learning.","venue":"CHI","year":2011,"slug":"2011-apolo-making-sense-of-large-network-data-by-combining-rich-user-interaction-and-machine-learning","ext":".md"},"path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","excerpt":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","collection":"publications","content":"<p>We present APOLO, a system that uses a mixed-initiative approach to help people interactively explore and make sense of large network datasets. It combines visualization, rich user interaction and machine learning to engage the user in bottom-up sensemaking to gradually build up an understanding over time by starting small, rather than starting big and drilling down. APOLO helps users find relevant information by specifying exemplars, and then using a machine learning method called Belief Propagation to infer which other nodes may be of interest. We demonstrate APOLO’s usage and benefits using a Google Scholar citation graph, consisting of 83,000 articles (nodes) and 150,000 citations relationships. A demo video of APOLO is available at http://www.cs.cmu.edu/~dchau/apolo/apolo.mp4.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},{"output":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","previous":{"output":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","previous":{"url":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.html","relative_path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","path":"_publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays.md","id":"/publications/2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Patrick Fiaux","Haeyong Chung","Michael Stewart","Christopher Andrews","Chris North"],"link":null,"tags":[],"title":"ChairMouse: leveraging natural chair rotation for cursor navigation on large, high-resolution displays.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-chairmouse-leveraging-natural-chair-rotation-for-cursor-navigation-on-large-highresolution-displays","ext":".md"},"url":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.html","relative_path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","next":{"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","collection":"publications","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"path":"_publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study.md","id":"/publications/2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","excerpt":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","collection":"publications","content":"<p>While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community’s understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Electronic Publishing","Collaboration","Analytical Models","Internet","Artificial Intelligence"],"title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study.","venue":"IEEE VAST","year":2011,"slug":"2011-characterizing-the-intelligence-analysis-process-informing-visual-analytics-design-through-a-longitudinal-field-study","ext":".md"},"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","next":{"output":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","previous":{"url":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.html","relative_path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","collection":"publications","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},"url":"/publications/2011-evaluating-video-visualizations-of-human-behavior.html","relative_path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","next":{"url":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.html","relative_path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","path":"_publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations.md","id":"/publications/2011-exploring-complex-mobile-life-through-lightweight-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Tanyoung Kim","Jan Blom","John T. Stasko"],"link":null,"tags":[],"title":"Exploring Complex Mobile Life through Lightweight Visualizations.","venue":"EuroVA@EuroVis","year":2011,"slug":"2011-exploring-complex-mobile-life-through-lightweight-visualizations","ext":".md"},"path":"_publications/2011-evaluating-video-visualizations-of-human-behavior.md","id":"/publications/2011-evaluating-video-visualizations-of-human-behavior","excerpt":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","collection":"publications","content":"<p>Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Alice Vialard","John Peponis","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":[],"title":"Evaluating video visualizations of human behavior.","venue":"CHI","year":2011,"slug":"2011-evaluating-video-visualizations-of-human-behavior","ext":".md"},"path":"_publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices.md","id":"/publications/2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","excerpt":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","collection":"publications","content":"<p>This study adapts existing tools (Jigsaw and a text editor) to support multiple input devices, which were then used in a co-located collaborative intelligence analysis study conducted on a large, high-resolution display. Exploring the sensemaking process and user roles in pairs of analysts, the two-hour study used a fictional data set composed of 50 short textual documents that contained a terrorist plot and subject pairs who had experience working together. The large display facilitated the paired sensemaking process, allowing teams to spatially arrange information and conduct individual work as needed. We discuss how the space and the tools affected the approach to the analysis, how the teams collaborated, and the user roles that developed. Using these findings, we suggest design guidelines for future co-located collaborative tools.</p>\n","draft":false,"categories":[],"authors":["Katherine Vogt","Lauren Bradel","Christopher Andrews","Chris North","Alex Endert","Duke Hutchings"],"link":null,"tags":["Visual Analytics","Sensemaking","Co-located","Large High-resolution Display","Cscw"],"title":"Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices.","venue":"INTERACT (2)","year":2011,"slug":"2011-colocated-collaborative-sensemaking-on-a-large-highresolution-display-with-multiple-input-devices","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","collection":"publications","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","next":{"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","next":{"output":"\n","previous":{"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","next":{"url":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.html","relative_path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","path":"_publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization.md","id":"/publications/2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Jason I. Hong","Christos Faloutsos"],"link":null,"tags":["Evaluation Of Retrieval Results","Relevance Assessment","Human-centered Computing","Information Systems","Human Computer Interaction (hci)","Information Retrieval"],"title":"Apolo: interactive large graph sensemaking by combining machine learning and visualization.","venue":"KDD","year":2011,"slug":"2011-apolo-interactive-large-graph-sensemaking-by-combining-machine-learning-and-visualization","ext":".md"},"path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","next":{"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","collection":"publications","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","next":{"output":"\n","previous":{"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","collection":"publications","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","next":{"url":"/publications/2011-analytic-provenance-processinteractioninsight.html","relative_path":"_publications/2011-analytic-provenance-processinteractioninsight.md","path":"_publications/2011-analytic-provenance-processinteractioninsight.md","id":"/publications/2011-analytic-provenance-processinteractioninsight","collection":"publications","draft":false,"categories":[],"authors":["Chris North","Remco Chang","Alex Endert","Wenwen Dou","Richard May","Bill Pike","Glenn A. Fink"],"link":null,"tags":[],"title":"Analytic provenance: process+interaction+insight.","venue":"CHI Extended Abstracts","year":2011,"slug":"2011-analytic-provenance-processinteractioninsight","ext":".md"},"path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},{"output":"\n","previous":{"output":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","previous":{"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","next":{"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","collection":"publications","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","excerpt":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","collection":"publications","content":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","next":{"output":"\n","previous":{"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","collection":"publications","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","next":{"url":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.html","relative_path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","path":"_publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature.md","id":"/publications/2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Hannah J. Tipney","Karin Verspoor","William A. Baumgartner Jr.","K. Bretonnel Cohen","John T. Stasko","Lawrence Hunter"],"link":null,"tags":[],"title":"Visualization and Language Processing for Supporting Analysis across the Biomedical Literature.","venue":"KES (4)","year":2010,"slug":"2010-visualization-and-language-processing-for-supporting-analysis-across-the-biomedical-literature","ext":".md"},"path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},{"output":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","previous":{"output":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","previous":{"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","collection":"publications","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","next":{"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","excerpt":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","collection":"publications","content":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","next":{"output":"\n","previous":{"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","next":{"url":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.html","relative_path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","path":"_publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces.md","id":"/publications/2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","collection":"publications","draft":false,"categories":[],"authors":["Liang Liu","Clio Andris","Carlo Ratti"],"link":null,"tags":[],"title":"Uncovering cabdrivers' behavior patterns from their digital traces.","venue":"Comput. Environ. Urban Syst.","year":2010,"slug":"2010-uncovering-cabdrivers-behavior-patterns-from-their-digital-traces","ext":".md"},"path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","excerpt":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","collection":"publications","content":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},{"output":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","previous":{"output":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","previous":{"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","next":{"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","excerpt":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","collection":"publications","content":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","next":{"output":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","previous":{"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","next":{"url":"/publications/2010-towards-efficient-collaboration-in-cyber-security.html","relative_path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","path":"_publications/2010-towards-efficient-collaboration-in-cyber-security.md","id":"/publications/2010-towards-efficient-collaboration-in-cyber-security","collection":"publications","draft":false,"categories":[],"authors":["Peter Hui","Joe Bruce","Glenn A. Fink","Michelle L. Gregory","Daniel M. Best","Liam McGrath","Alex Endert"],"link":null,"tags":[],"title":"Towards efficient collaboration in cyber security.","venue":"CTS","year":2010,"slug":"2010-towards-efficient-collaboration-in-cyber-security","ext":".md"},"path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","excerpt":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","collection":"publications","content":"<p>Space supports human cognitive abilities in a myriad of ways. The note attached to the side of the monitor, the papers spread out on the desk, diagrams scrawled on a whiteboard, and even the keys left out on the counter are all examples of using space to recall, reveal relationships, and think. Technological advances have made it possible to construct large display environments in which space has real meaning. This paper examines how increased space affects the way displays are regarded and used within the context of the cognitively demanding task of sensemaking. A pair of studies were conducted demonstrating how the spatial environment supports sensemaking by becoming part of the distributed cognitive process, providing both external memory and a semantic layer.</p>\n","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","excerpt":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","collection":"publications","content":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},{"output":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","previous":{"output":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","previous":{"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","next":{"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","collection":"publications","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","excerpt":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","collection":"publications","content":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","next":{"output":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","previous":{"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","collection":"publications","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","next":{"url":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.html","relative_path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","path":"_publications/2010-space-to-think-large-highresolution-displays-for-sensemaking.md","id":"/publications/2010-space-to-think-large-highresolution-displays-for-sensemaking","collection":"publications","draft":false,"categories":[],"authors":["Christopher Andrews","Alex Endert","Chris North"],"link":null,"tags":["Human-centered Computing"],"title":"Space to think: large high-resolution displays for sensemaking.","venue":"CHI","year":2010,"slug":"2010-space-to-think-large-highresolution-displays-for-sensemaking","ext":".md"},"path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","excerpt":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","collection":"publications","content":"<p>Given a large graph, like a computer network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? We need (a) a measure of the ‘Vulnerability’ of a given network, (b) a measure of the ‘Shield-value’ of a specific set of k nodes and (c) a fast algorithm to choose the best such k nodes. We answer all these three questions: we give the justification behind our choices, we show that they agree with intuition as well as recent results in immunology. Moreover, we propose NetShield a fast and scalable algorithm. Finally, we give experiments on large real graphs, where NetShield achieves tremendous speed savings exceeding 7 orders of magnitude, against straightforward competitors.</p>\n","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","excerpt":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","collection":"publications","content":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},{"output":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","previous":{"output":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","previous":{"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","next":{"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","excerpt":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","collection":"publications","content":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","next":{"output":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","previous":{"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","next":{"url":"/publications/2010-on-the-vulnerability-of-large-graphs.html","relative_path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","path":"_publications/2010-on-the-vulnerability-of-large-graphs.md","id":"/publications/2010-on-the-vulnerability-of-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Hanghang Tong","B. Aditya Prakash","Charalampos E. Tsourakakis","Tina Eliassi-Rad","Christos Faloutsos","Duen Horng (Polo) Chau"],"link":null,"tags":["Eigenvalues And Eigenfunctions","Correlation","Robustness","Scalability","Computer Networks","Immune System","Approximation Methods"],"title":"On the Vulnerability of Large Graphs.","venue":"ICDM","year":2010,"slug":"2010-on-the-vulnerability-of-large-graphs","ext":".md"},"path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","excerpt":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","collection":"publications","content":"<p>In this paper, we propose a tool to explore human movement dynamics in a Metropolitan Area. By analyzing a mass of individual cell phone traces, we build a Human-City Interaction System for understanding urban mobility patterns at different user-controlled temporal and geographic scales. We solve the problems that are found in available tools for spatio-temporal analysis, by allowing seamless manipulability and introducing a simultaneous\\multi-scale visualization of individual and aggregate flows. Our tool is built to support the exploration and discovery of urban mobility patterns and the daily interactions of millions of people. Moreover, we implement an intelligent algorithm to evaluate the level of mobility homophily of people moving from place to place.</p>\n","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","excerpt":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","collection":"publications","content":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},{"output":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","previous":{"output":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","previous":{"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","collection":"publications","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","next":{"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","excerpt":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","collection":"publications","content":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","next":{"output":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","previous":{"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","next":{"url":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.html","relative_path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","path":"_publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis.md","id":"/publications/2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","collection":"publications","draft":false,"categories":[],"authors":["Mauro Martino","Francesco Calabrese","Giusy Di Lorenzo","Clio Andris","Liu Liang","Carlo Ratti"],"link":null,"tags":["Human-centered Computing"],"title":"Ocean of information: fusing aggregate  individual dynamics for metropolitan analysis.","venue":"IUI","year":2010,"slug":"2010-ocean-of-information-fusing-aggregate--individual-dynamics-for-metropolitan-analysis","ext":".md"},"path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","excerpt":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","collection":"publications","content":"<p>Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","excerpt":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","collection":"publications","content":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},{"output":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","next":{"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","next":{"output":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","previous":{"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","next":{"url":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.html","relative_path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","path":"_publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective.md","id":"/publications/2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko"],"link":null,"tags":["Cognition","Visualization","Cognitive Science","Data Visualization","Humans","Computational Modeling","Brain Modeling"],"title":"Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2010,"slug":"2010-mental-models-visual-reasoning-and-interaction-in-information-visualization-a-topdown-perspective","ext":".md"},"path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","excerpt":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","collection":"publications","content":"<p>Our visual analytics tool GeneTracer, developed for the VAST 2010 genetic sequence mini challenge, visualizes gene sequences of current outbreaks and native sequences along with disease characteristics. We successfully used GeneTracer in combination with data mining techniques to solve the challenge.</p>\n","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","excerpt":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","collection":"publications","content":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},{"output":"\n","previous":{"output":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","previous":{"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","next":{"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","collection":"publications","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","excerpt":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","collection":"publications","content":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","next":{"output":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","previous":{"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","collection":"publications","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","next":{"url":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.html","relative_path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","path":"_publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation.md","id":"/publications/2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","collection":"publications","draft":false,"categories":[],"authors":["Hanseung Lee","Jaegul Choo","Carsten G","Jaeeun Shim","Jaeyeon Kihm","Zhicheng Liu","Haesun Park","John T. Stasko"],"link":null,"tags":[],"title":"GeneTracer: Gene sequence analysis of disease mutations VAST 2010 mini challenge 3 award: Excellent process explanation.","venue":"IEEE VAST","year":2010,"slug":"2010-genetracer-gene-sequence-analysis-of-disease-mutations-vast-2010-mini-challenge-3-award-excellent-process-explanation","ext":".md"},"path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","excerpt":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","collection":"publications","content":"<p>This article describes the sense-making process we applied to solve the VAST 2010 Mini Challenge 1 using the visual analytics system Jigsaw. We focus on Jigsaw’s data ingest and evidence marshalling features and discuss how they are beneficial for a holistic sense-making experience.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},{"output":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","previous":{"output":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","previous":{"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","next":{"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","excerpt":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","collection":"publications","content":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","next":{"url":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.html","relative_path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","path":"_publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest.md","id":"/publications/2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Carsten G","Jaeyeon Kihm","Hanseung Lee","Jaegul Choo","Haesun Park","John T. Stasko"],"link":null,"tags":["Visual Analytics","Data Visualization","Xml","Fixtures","Social Networking (online)"],"title":"Data ingestion and evidence marshalling in Jigsaw VAST 2010 Mini Challenge 1 award: Good support for data ingest.","venue":"IEEE VAST","year":2010,"slug":"2010-data-ingestion-and-evidence-marshalling-in-jigsaw-vast-2010-mini-challenge-1-award-good-support-for-data-ingest","ext":".md"},"path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","excerpt":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","collection":"publications","content":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},{"output":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","collection":"publications","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","next":{"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","next":{"output":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","previous":{"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","next":{"url":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.html","relative_path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","path":"_publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users.md","id":"/publications/2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","collection":"publications","draft":false,"categories":[],"authors":["Dugald Ralph Hutchings","John T. Stasko"],"link":null,"tags":[],"title":"Controlling information display in larger pixel spaces: a study of window snipping by multiple-monitor users.","venue":"ACM Southeast Regional Conference","year":2010,"slug":"2010-controlling-information-display-in-larger-pixel-spaces-a-study-of-window-snipping-by-multiplemonitor-users","ext":".md"},"path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","excerpt":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","collection":"publications","content":"<p>Information Visualization (InfoVis) focuses on the use of visualization techniques to help people understand and analyze data. While related fields such as Scientific Visualization involve the presentation of data that has some physical or geometric correspondence, Information Visualization centers on abstract information without such correspondences. The aim of this seminar was to bring together theoreticians and practitioners from the field with a special focus on the intersection of InfoVis and Human-Computer Interaction. To support discussions that are related to the visualization of real world data, researchers from selected application areas also attended and contributed. During the seminar, working groups on eight different topics were formed and enabled a critical reflection on ongoing research efforts, the state of the field, and key research challenges today.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","excerpt":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","collection":"publications","content":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},{"output":"\n","previous":{"output":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","previous":{"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","next":{"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","excerpt":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","collection":"publications","content":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","next":{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","previous":{"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","next":{"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},{"output":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","next":{"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","collection":"publications","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","next":{"output":"\n","previous":{"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","collection":"publications","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","next":{"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","excerpt":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","collection":"publications","content":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","next":{"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","collection":"publications","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","next":{"output":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","previous":{"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","collection":"publications","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","next":{"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","excerpt":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","collection":"publications","content":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","next":{"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","next":{"output":"\n","previous":{"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","next":{"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","collection":"publications","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},{"output":"\n","previous":{"output":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","previous":{"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","next":{"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","excerpt":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","collection":"publications","content":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","next":{"output":"\n","previous":{"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","next":{"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","collection":"publications","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},{"output":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","previous":{"output":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","previous":{"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","next":{"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","excerpt":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","collection":"publications","content":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","next":{"output":"\n","previous":{"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","next":{"url":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.html","relative_path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","path":"_publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces.md","id":"/publications/2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko","Mark Baloga"],"link":null,"tags":[],"title":"The conference room as a toolbox: technological and social routines in corporate meeting spaces.","venue":"CT","year":2009,"slug":"2009-the-conference-room-as-a-toolbox-technological-and-social-routines-in-corporate-meeting-spaces","ext":".md"},"path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","excerpt":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","collection":"publications","content":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},{"output":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","next":{"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","next":{"output":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","previous":{"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","next":{"url":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.html","relative_path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","path":"_publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections.md","id":"/publications/2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Aniket Kittur","Christos Faloutsos","Jason I. Hong"],"link":null,"tags":[],"title":"SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections.","venue":"CHI Extended Abstracts","year":2009,"slug":"2009-shiftr-a-userdirected-linkbased-system-for-ad-hoc-sensemaking-of-large-heterogeneous-data-collections","ext":".md"},"path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","excerpt":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","collection":"publications","content":"<p>We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","excerpt":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","collection":"publications","content":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},{"output":"\n","previous":{"output":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","previous":{"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","next":{"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","excerpt":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","collection":"publications","content":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","next":{"output":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","previous":{"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","next":{"url":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.html","relative_path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","path":"_publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data.md","id":"/publications/2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","John T. Stasko","Timothy Sullivan"],"link":null,"tags":["Companies","Logistics","Information Analysis","Data Visualization","Data Analysis","Books","Time Series Analysis","Failure Analysis","Feedback","Cities And Towns"],"title":"SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2009,"slug":"2009-selltrend-interattribute-visual-analysis-of-temporal-transaction-data","ext":".md"},"path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","excerpt":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","collection":"publications","content":"<p>Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.</p>\n","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},{"output":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","previous":{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","previous":{"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","next":{"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","next":{"output":"\n","previous":{"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","next":{"url":"/publications/2009-professional-analysts-using-a-large-highresolution-display.html","relative_path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","path":"_publications/2009-professional-analysts-using-a-large-highresolution-display.md","id":"/publications/2009-professional-analysts-using-a-large-highresolution-display","collection":"publications","draft":false,"categories":[],"authors":["Alex Endert","Christopher Andrews","Glenn A. Fink","Chris North"],"link":null,"tags":["Visual Analytics","Data Visualization","Data Analysis","Information Systems","Liquid Crystal Displays","Large Screen Displays","Performance Analysis","Information Analysis"],"title":"Professional analysts using a large, high-resolution display.","venue":"IEEE VAST","year":2009,"slug":"2009-professional-analysts-using-a-large-highresolution-display","ext":".md"},"path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","excerpt":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","collection":"publications","content":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},{"output":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","next":{"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","next":{"output":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","previous":{"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"url":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.html","relative_path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","next":{"url":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.html","relative_path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","path":"_publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task.md","id":"/publications/2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":[],"title":"Presence  placement: exploring the benefits of multiple shared displays on an intellective sensemaking task.","venue":"GROUP","year":2009,"slug":"2009-presence--placement-exploring-the-benefits-of-multiple-shared-displays-on-an-intellective-sensemaking-task","ext":".md"},"path":"_publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts.md","id":"/publications/2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","excerpt":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","collection":"publications","content":"<p>Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Sarah Cohen","Lawrence Hunter","Joe Parry"],"link":null,"tags":["Visual Analytics","Software Systems","Data Visualization","Text Analysis","Genomics","Information Systems","Collaboration","Military Computing","Information Analysis","Bioinformatics"],"title":"How interactive visualization can assist investigative analysis: Views and perspectives from domain experts.","venue":"IEEE VAST","year":2009,"slug":"2009-how-interactive-visualization-can-assist-investigative-analysis-views-and-perspectives-from-domain-experts","ext":".md"},"path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","excerpt":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","collection":"publications","content":"<p>Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},{"output":"\n","previous":{"output":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","previous":{"url":"/publications/2009-the-science-of-interaction.html","relative_path":"_publications/2009-the-science-of-interaction.md","path":"_publications/2009-the-science-of-interaction.md","id":"/publications/2009-the-science-of-interaction","collection":"publications","draft":false,"categories":[],"authors":["William A. Pike","John T. Stasko","Remco Chang","Theresa A. O'Connell"],"link":null,"tags":[],"title":"The science of interaction.","venue":"Inf. Vis.","year":2009,"slug":"2009-the-science-of-interaction","ext":".md"},"url":"/publications/2009-vast-contest-dataset-use-in-education.html","relative_path":"_publications/2009-vast-contest-dataset-use-in-education.md","next":{"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"path":"_publications/2009-vast-contest-dataset-use-in-education.md","id":"/publications/2009-vast-contest-dataset-use-in-education","excerpt":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","collection":"publications","content":"<p>The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.</p>\n","draft":false,"categories":[],"authors":["Mark A. Whiting","Chris North","Alex Endert","Jean Scholtz","Jereme Haack","Carrie Varley","Jim Thomas"],"link":null,"tags":["Visual Analytics","Data Visualization","Computer Science Education","Government","Educational Technology","Laboratories","Application Software","Educational Institutions","Information Analysis","Computer Science"],"title":"VAST contest dataset use in education.","venue":"IEEE VAST","year":2009,"slug":"2009-vast-contest-dataset-use-in-education","ext":".md"},"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","next":{"output":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","previous":{"url":"/publications/2009-visualizing-cyber-security-usable-workspaces.html","relative_path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","collection":"publications","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},"url":"/publications/2010-10241-abstracts-collection--information-visualization.html","relative_path":"_publications/2010-10241-abstracts-collection--information-visualization.md","next":{"url":"/publications/2010-10241-executive-summary--information-visualization.html","relative_path":"_publications/2010-10241-executive-summary--information-visualization.md","path":"_publications/2010-10241-executive-summary--information-visualization.md","id":"/publications/2010-10241-executive-summary--information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Executive Summary - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-executive-summary--information-visualization","ext":".md"},"path":"_publications/2010-10241-abstracts-collection--information-visualization.md","id":"/publications/2010-10241-abstracts-collection--information-visualization","excerpt":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","collection":"publications","content":"<p>From 13.06.10 to 18.06.10, the Dagstuhl Seminar 10241 ``Information Visualization ‘’ was held in Schloss Dagstuhl~–~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","Catherine Plaisant","John T. Stasko"],"link":null,"tags":["Visualization","Data Visualization","Collaboration","Display Technologies","Human-computer Interaction","Information Visualization"],"title":"10241 Abstracts Collection - Information Visualization.","venue":"Information Visualization","year":2010,"slug":"2010-10241-abstracts-collection--information-visualization","ext":".md"},"path":"_publications/2009-visualizing-cyber-security-usable-workspaces.md","id":"/publications/2009-visualizing-cyber-security-usable-workspaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Glenn A. Fink","Christopher L. North","Alex Endert","Stuart Rose"],"link":null,"tags":[],"title":"Visualizing cyber security: Usable workspaces.","venue":"VizSEC","year":2009,"slug":"2009-visualizing-cyber-security-usable-workspaces","ext":".md"},{"output":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","previous":{"output":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","previous":{"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","next":{"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","excerpt":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","collection":"publications","content":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","next":{"output":"\n","previous":{"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","next":{"url":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.html","relative_path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","path":"_publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study.md","id":"/publications/2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","Carsten G","John T. Stasko"],"link":null,"tags":["Collaborative Tools","Visual Analytics","Data Visualization","Data Analysis","Control Systems","Guidelines","Performance Analysis","Information Analysis","Electronic Mail","Collaborative Work"],"title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.","venue":"IEEE VAST","year":2009,"slug":"2009-evaluating-visual-analytics-systems-for-investigative-analysis-deriving-design-principles-from-a-case-study","ext":".md"},"path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","excerpt":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","collection":"publications","content":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},{"output":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","previous":{"output":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","previous":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","next":{"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","excerpt":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","collection":"publications","content":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","next":{"output":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","previous":{"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","next":{"url":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.html","relative_path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","path":"_publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis.md","id":"/publications/2009-computational-explanations-for-report-generation-in-intelligence-analysis","collection":"publications","draft":false,"categories":[],"authors":["Ashok K. Goel","Emile L. Morse","Anita Raja","Jean Scholtz","John T. Stasko"],"link":null,"tags":[],"title":"Computational Explanations for Report Generation in Intelligence Analysis.","venue":"ExaCt","year":2009,"slug":"2009-computational-explanations-for-report-generation-in-intelligence-analysis","ext":".md"},"path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","excerpt":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","collection":"publications","content":"<p>Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user’s current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.</p>\n","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","excerpt":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","collection":"publications","content":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},{"output":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","next":{"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","next":{"output":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","previous":{"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","next":{"url":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.html","relative_path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","path":"_publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization.md","id":"/publications/2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","collection":"publications","draft":false,"categories":[],"authors":["Yi Han","Erich P. Stuntebeck","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Signal Generators","Visual Analytics","Satellite Broadcasting","Fingerprint Recognition","Debugging","Radio Frequency","Global Positioning System","Receivers","Electronic Mail","Rf Signals"],"title":"A visual analytics system for radio frequency fingerprinting-based localization.","venue":"IEEE VAST","year":2009,"slug":"2009-a-visual-analytics-system-for-radio-frequency-fingerprintingbased-localization","ext":".md"},"path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","excerpt":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","collection":"publications","content":"<p>Sometimes people cannot remember the names or locations of things on their computer, but they can remember what other things are associated with them. We created Feldspar, the first system that fully supports this associative retrieval of personal information on the computer. Feldspar’s contributions include (1) an intuitive user interface that allows users to find information by interactively and incrementally specifying multiple levels of associations as retrieval queries, such as: “find the file from the person who I met at an event in May”; and (2) algorithms for collecting the association information and for providing answers to associative queries in real-time. A user study showed that Feldspar is easy to use, and suggested that it might be faster than conventional browsing and searching for these kinds of retrieval tasks. Feldspar could be an important addition to search and browsing tools.</p>\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","excerpt":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","collection":"publications","content":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","next":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","next":{"output":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","previous":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","next":{"url":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association.html","relative_path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","path":"_publications/2008-what-to-do-when-search-fails-finding-information-by-association.md","id":"/publications/2008-what-to-do-when-search-fails-finding-information-by-association","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Brad A. Myers","Andrew Faulring"],"link":null,"tags":["Computing Methodologies","Interaction Paradigms","Human-centered Computing","Information Systems","Computer Graphics","Graphics Input Devices","Human Computer Interaction (hci)","Information Retrieval","Graphics Systems And Interfaces","Interaction Devices"],"title":"What to do when search fails: finding information by association.","venue":"CHI","year":2008,"slug":"2008-what-to-do-when-search-fails-finding-information-by-association","ext":".md"},"path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","excerpt":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","collection":"publications","content":"<p>In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.</p>\n","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},{"output":"\n","previous":{"output":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","previous":{"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","next":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","excerpt":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","collection":"publications","content":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","next":{"output":"\n","previous":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","next":{"url":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision.html","relative_path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","path":"_publications/2008-vizavis-toward-visualizing-video-through-computer-vision.md","id":"/publications/2008-vizavis-toward-visualizing-video-through-computer-vision","collection":"publications","draft":false,"categories":[],"authors":["Mario Romero","Jay Summet","John T. Stasko","Gregory D. Abowd"],"link":null,"tags":["Visualization","Computer Vision"],"title":"Viz-A-Vis: Toward Visualizing Video through Computer Vision.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-vizavis-toward-visualizing-video-through-computer-vision","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},{"output":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","previous":{"output":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","previous":{"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","next":{"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","excerpt":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","collection":"publications","content":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","next":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis.","venue":"VL/HCC","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis","ext":".md"},"path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","excerpt":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","collection":"publications","content":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},{"output":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","previous":{"output":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","previous":{"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","next":{"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","excerpt":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","collection":"publications","content":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","next":{"output":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","previous":{"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","next":{"url":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.html","relative_path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","path":"_publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation.md","id":"/publications/2008-visualization-for-information-exploration-and-analysis-keynote-presentation","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko"],"link":null,"tags":[],"title":"Visualization for information exploration and analysis: keynote presentation.","venue":"SOFTVIS","year":2008,"slug":"2008-visualization-for-information-exploration-and-analysis-keynote-presentation","ext":".md"},"path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","excerpt":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","collection":"publications","content":"<p>Even though “providing insight” has been considered one of the main purposes of information visualization (InfoVis), we feel that insight is still a not-well-understood concept in this context. Inspired by research in sensemaking, we realized the importance of the procedural aspects in understanding insight. Thus, rather than asking “What is insight?” we instead focus on “How do people gain insights?” In an effort to better understand and characterize insight, we reviewed previous literature in InfoVis, seeking other researchers’ comments and views on this concept. We found that: 1) Insights are often regarded as end results of using InfoVis and the procedures to gain insight have been largely veiled; 2) Four largely distinctive processes of gaining insight (Provide Overview, Adjust, Detect Pattern, and Match Mental Model) have been discussed in the InfoVis literature; and 3) These different processes provide some hints to understand the procedures in which insight can be gained from InfoVis. We hope that our findings help researchers and practitioners evaluate InfoVis systems and technologies in a more insight-oriented way.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","excerpt":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","collection":"publications","content":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},{"output":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","previous":{"output":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","previous":{"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","next":{"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","excerpt":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","collection":"publications","content":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","next":{"output":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","previous":{"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","next":{"url":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.html","relative_path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","path":"_publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization.md","id":"/publications/2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)","Hci Design And Evaluation Methods"],"title":"Understanding and characterizing insights: how do people gain insights using information visualization?","venue":"BELIV","year":2008,"slug":"2008-understanding-and-characterizing-insights-how-do-people-gain-insights-using-information-visualization","ext":".md"},"path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","excerpt":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","collection":"publications","content":"<p>Researchers and users of Information Visualization are convinced that it has value. This value can easily be communicated to others in a face-to-face setting, such that this value is experienced in practice. To convince broader audiences, and also, to understand the intrinsic qualities of visualization is more difficult, however. In this paper we consider information visualization from different points of view, and gather arguments to explain the value of our field.</p>\n","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","excerpt":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","collection":"publications","content":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},{"output":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","collection":"publications","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","next":{"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","next":{"output":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","previous":{"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","next":{"url":"/publications/2008-the-value-of-information-visualization.html","relative_path":"_publications/2008-the-value-of-information-visualization.md","path":"_publications/2008-the-value-of-information-visualization.md","id":"/publications/2008-the-value-of-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Jean-Daniel Fekete","Jarke J. van Wijk","John T. Stasko","Chris North"],"link":null,"tags":["Preattentive Processing","Perceptual Chunk","Information Visualization","Exploratory Data Analysis","Perceptual Inference"],"title":"The Value of Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-the-value-of-information-visualization","ext":".md"},"path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","excerpt":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","collection":"publications","content":"<p>Teaching InfoVis is a challenge because it is a new and growing field. This paper describes the results of a teaching survey based on the information given by the attendees of Dagstuhl Seminar 07221. It covers several aspects of offered InfoVis courses that range from different kinds of study materials to practical exercises. We have reproduced the discussion during the seminar and added our own experiences. We hope that this paper can serve as an interesting and helpful source for current and future InfoVis teachers.</p>\n","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","excerpt":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","collection":"publications","content":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},{"output":"\n","previous":{"output":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","previous":{"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","collection":"publications","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","next":{"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","excerpt":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","collection":"publications","content":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","next":{"output":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","previous":{"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","next":{"url":"/publications/2008-teaching-information-visualization.html","relative_path":"_publications/2008-teaching-information-visualization.md","path":"_publications/2008-teaching-information-visualization.md","id":"/publications/2008-teaching-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Andreas Kerren","John T. Stasko","Jason Dykes"],"link":null,"tags":["Georgia Tech","Graph Drawing","Interaction Technique","Visualization Technique","Information Visualization"],"title":"Teaching Information Visualization.","venue":"Information Visualization","year":2008,"slug":"2008-teaching-information-visualization","ext":".md"},"path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","excerpt":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","collection":"publications","content":"<p>It is becoming increasingly common to see computers with two or even three monitors being used today. People seem to like having more display space available, and intuition tells us that the added space should be beneficial to work. Little research has been done to examine the effects and potential utility of multiple monitors for work on everyday tasks with common applications, however. We compared how people completed a trip planning task that involved different applications and included interjected interruptions when they worked on a computer with one monitor as compared to a computer with two monitors. Results showed that participants who used the computer with two monitors performed the task set faster and with less workload, and they also expressed a subjective preference for the multiple monitor computer.</p>\n","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},{"output":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","previous":{"output":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","previous":{"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","next":{"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","collection":"publications","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","excerpt":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","collection":"publications","content":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","next":{"output":"\n","previous":{"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","collection":"publications","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","next":{"url":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.html","relative_path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","path":"_publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study.md","id":"/publications/2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","collection":"publications","draft":false,"categories":[],"authors":["Youn ah Kang","John T. Stasko"],"link":null,"tags":[],"title":"Lightweight task/application performance using single versus multiple monitors: a comparative study.","venue":"Graphics Interface","year":2008,"slug":"2008-lightweight-taskapplication-performance-using-single-versus-multiple-monitors-a-comparative-study","ext":".md"},"path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","excerpt":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","collection":"publications","content":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},{"output":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","previous":{"output":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","previous":{"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","next":{"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","collection":"publications","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","excerpt":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","collection":"publications","content":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","next":{"output":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","previous":{"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","collection":"publications","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","next":{"url":"/publications/2008-graphite-a-visual-query-system-for-large-graphs.html","relative_path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","path":"_publications/2008-graphite-a-visual-query-system-for-large-graphs.md","id":"/publications/2008-graphite-a-visual-query-system-for-large-graphs","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Christos Faloutsos","Hanghang Tong","Jason I. Hong","Brian Gallagher","Tina Eliassi-Rad"],"link":null,"tags":[],"title":"GRAPHITE: A Visual Query System for Large Graphs.","venue":"ICDM Workshops","year":2008,"slug":"2008-graphite-a-visual-query-system-for-large-graphs","ext":".md"},"path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","excerpt":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","collection":"publications","content":"<p>In this article, we report on the contest’s data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we’re creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.</p>\n","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","excerpt":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","collection":"publications","content":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},{"output":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","previous":{"output":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","previous":{"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","next":{"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","excerpt":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","collection":"publications","content":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","next":{"output":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","previous":{"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","next":{"url":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.html","relative_path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","path":"_publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest.md","id":"/publications/2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","collection":"publications","draft":false,"categories":[],"authors":["Catherine Plaisant","Georges G. Grinstein","Jean Scholtz","Mark A. Whiting","Theresa A. O'Connell","Sharon J. Laskowski","Lynn Chien","Annie Tat","William Wright","Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Information Services","Visual Analytics","Data Visualization","Data Analysis","Web Sites","Laboratories","Nist","Internet","Radio Access Networks","Data Mining"],"title":"Evaluating Visual Analytics at the 2007 VAST Symposium Contest.","venue":"IEEE Computer Graphics and Applications","year":2008,"slug":"2008-evaluating-visual-analytics-at-the-2007-vast-symposium-contest","ext":".md"},"path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","excerpt":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","collection":"publications","content":"<p>Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.</p>\n","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","excerpt":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","collection":"publications","content":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},{"output":"\n","previous":{"output":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","previous":{"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","next":{"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","collection":"publications","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","excerpt":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","collection":"publications","content":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","next":{"output":"\n","previous":{"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","collection":"publications","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","next":{"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},{"output":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","collection":"publications","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","next":{"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","next":{"output":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","previous":{"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","next":{"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","excerpt":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","collection":"publications","content":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","excerpt":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","collection":"publications","content":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","next":{"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","next":{"output":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","previous":{"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","next":{"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","excerpt":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","collection":"publications","content":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},{"output":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","previous":{"output":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","previous":{"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"url":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.html","relative_path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","next":{"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"path":"_publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization.md","id":"/publications/2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","excerpt":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","collection":"publications","content":"<p>Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.</p>\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Youn ah Kang","John T. Stasko","Julie A. Jacko"],"link":null,"tags":["Computer Displays","Visual Analytics","Data Visualization","Taxonomy","Filters","Rendering (computer Graphics)","Human Computer Interaction","Conference Proceedings","Computer Graphics","Research And Development"],"title":"Toward a Deeper Understanding of the Role of Interaction in Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-toward-a-deeper-understanding-of-the-role-of-interaction-in-information-visualization","ext":".md"},"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","next":{"output":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","previous":{"url":"/publications/2007-visual-analytics-with-jigsaw.html","relative_path":"_publications/2007-visual-analytics-with-jigsaw.md","path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},"url":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.html","relative_path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","next":{"url":"/publications/2008-effectiveness-of-animation-in-trend-visualization.html","relative_path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","path":"_publications/2008-effectiveness-of-animation-in-trend-visualization.md","id":"/publications/2008-effectiveness-of-animation-in-trend-visualization","collection":"publications","draft":false,"categories":[],"authors":["George G. Robertson","Roland Fernandez","Danyel Fisher","Bongshin Lee","John T. Stasko"],"link":null,"tags":["Data Visualization","Data Analysis","Displays","Animation","Dictionaries"],"title":"Effectiveness of Animation in Trend Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-effectiveness-of-animation-in-trend-visualization","ext":".md"},"path":"_publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization.md","id":"/publications/2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","excerpt":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","collection":"publications","content":"<p>Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.</p>\n","draft":false,"categories":[],"authors":["Zhicheng Liu","Nancy J. Nersessian","John T. Stasko"],"link":null,"tags":["Cognition","Cognitive Science","Data Visualization","Testing","Humans","Buildings","Distributed Computing","Guidelines","Performance Evaluation","Electronic Mail"],"title":"Distributed Cognition as a Theoretical Framework for Information Visualization.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2008,"slug":"2008-distributed-cognition-as-a-theoretical-framework-for-information-visualization","ext":".md"},"path":"_publications/2007-visual-analytics-with-jigsaw.md","id":"/publications/2007-visual-analytics-with-jigsaw","excerpt":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","collection":"publications","content":"<p>This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST ‘07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriya Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Visualization","Calendars","Usability","Information Systems","Displays","Information Analysis","Yarn","Data Mining","Electronic Mail"],"title":"Visual Analytics with Jigsaw.","venue":"IEEE VAST","year":2007,"slug":"2007-visual-analytics-with-jigsaw","ext":".md"},{"output":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","previous":{"output":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","previous":{"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","collection":"publications","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","next":{"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","excerpt":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","collection":"publications","content":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","next":{"url":"/publications/2007-parallel-crawling-for-online-social-networks.html","relative_path":"_publications/2007-parallel-crawling-for-online-social-networks.md","path":"_publications/2007-parallel-crawling-for-online-social-networks.md","id":"/publications/2007-parallel-crawling-for-online-social-networks","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Parallel crawling for online social networks.","venue":"WWW","year":2007,"slug":"2007-parallel-crawling-for-online-social-networks","ext":".md"},"path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","excerpt":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","collection":"publications","content":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},{"output":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","previous":{"output":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","previous":{"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","next":{"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","excerpt":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","collection":"publications","content":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","next":{"output":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","previous":{"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","next":{"url":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.html","relative_path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","path":"_publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks.md","id":"/publications/2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","collection":"publications","draft":false,"categories":[],"authors":["Shashank Pandit","Duen Horng (Polo) Chau","Samuel Wang","Christos Faloutsos"],"link":null,"tags":[],"title":"Netprobe: a fast and scalable system for fraud detection in online auction networks.","venue":"WWW","year":2007,"slug":"2007-netprobe-a-fast-and-scalable-system-for-fraud-detection-in-online-auction-networks","ext":".md"},"path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","excerpt":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","collection":"publications","content":"<p>Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","excerpt":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","collection":"publications","content":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},{"output":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","previous":{"output":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","previous":{"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","collection":"publications","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","next":{"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","collection":"publications","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","excerpt":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","collection":"publications","content":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","next":{"output":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","previous":{"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","collection":"publications","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","next":{"url":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.html","relative_path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","path":"_publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization.md","id":"/publications/2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Carsten G","Zhicheng Liu","Kanupriya Singhal"],"link":null,"tags":["Visual Analytics","Algorithm Design And Analysis","Data Visualization","Information Systems","Yarn","Costs","Performance Analysis","Information Analysis","Embedded Computing","Electronic Mail"],"title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-supporting-investigative-analysis-through-interactive-visualization","ext":".md"},"path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","excerpt":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","collection":"publications","content":"<p>This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw’s use and how the different views helped us to uncover key parts of the underlying plot.</p>\n","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","excerpt":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","collection":"publications","content":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},{"output":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","previous":{"output":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","previous":{"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","collection":"publications","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","next":{"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","excerpt":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","collection":"publications","content":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","next":{"output":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","previous":{"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","next":{"url":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.html","relative_path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","path":"_publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest.md","id":"/publications/2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","collection":"publications","draft":false,"categories":[],"authors":["Carsten G","Zhicheng Liu","Neel Parekh","Kanupriyah Singhal","John T. Stasko"],"link":null,"tags":["Visual Analytics","Gas Insulated Transmission Lines","Visualization","Filters","Information Systems","Usa Councils","Radio Access Networks","Java","Information Analysis","Yarn"],"title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest.","venue":"IEEE VAST","year":2007,"slug":"2007-jigsaw-meets-blue-iguanodon--the-vast-2007-contest","ext":".md"},"path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","excerpt":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","collection":"publications","content":"<p>The increasing quantity and complexity of in-vehicle systems creates a demand for user interfaces which are suited to driving. The steering wheel is a common location for the placement of buttons to control navigation, entertainment, and environmental systems, but what about a small touchpad? To investigate this question, we embedded a Synaptics StampPad in a computer game steering wheel and evaluated seven methods for selecting from a list of over 3000 street names. Selection speed was measured while stationary and while driving a simulator. Results show that the EdgeWrite gestural text entry method is about 20% to 50% faster than selection-based text entry or direct list-selection methods. They also show that methods with slower selection speeds generally resulted in faster driving speeds. However, with EdgeWrite, participants were able to maintain their speed and avoid incidents while selecting and driving at the same time. Although an obvious choice for constrained input, on-screen keyboards generally performed quite poorly.</p>\n","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","excerpt":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","collection":"publications","content":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},{"output":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","previous":{"output":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","previous":{"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","next":{"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","collection":"publications","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","excerpt":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","collection":"publications","content":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","next":{"output":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","previous":{"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","collection":"publications","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","next":{"url":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.html","relative_path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","path":"_publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels.md","id":"/publications/2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","collection":"publications","draft":false,"categories":[],"authors":["Iv","Jacob O. Wobbrock","Duen Horng (Polo) Chau","Andrew Faulring","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"Eyes on the road, hands on the wheel: thumb-based interaction techniques for input on steering wheels.","venue":"Graphics Interface","year":2007,"slug":"2007-eyes-on-the-road-hands-on-the-wheel-thumbbased-interaction-techniques-for-input-on-steering-wheels","ext":".md"},"path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","excerpt":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","collection":"publications","content":"<p>We conducted a user study that demonstrates that automatically generated interfaces can support better usability through increased flexibility in two dimensions. First, we show that automatic generation can improve usability by moving interfaces that are constrained by cost and poor interaction primitives to another device with better interactive capabilities: subjects were twice as fast and four times as successful at completing tasks with automatically generated interfaces on a PocketPC device as with the actual appliance interfaces. Second, we show that an automatic generator can improve usability by automatically ensuring that new interfaces are generated to be consistent with users’ previous experience: subjects were also twice as fast using interfaces consistent with their experiences as compared to normally generated interfaces. These two results demonstrate that automatic interface generation is now viable and especially desirable where users will benefit from individualized interfaces or where human designers are constrained by cost and other factors.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","excerpt":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","collection":"publications","content":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},{"output":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","previous":{"output":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","previous":{"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","next":{"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","collection":"publications","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","excerpt":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","collection":"publications","content":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","next":{"output":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","previous":{"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","collection":"publications","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","next":{"url":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.html","relative_path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","path":"_publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces.md","id":"/publications/2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Interaction Paradigms","Human-centered Computing","Development Frameworks And Environments","Software Notations And Tools","Software And Its Engineering","Graphical User Interfaces","Human Computer Interaction (hci)","Integrated And Visual Development Environments"],"title":"Demonstrating the viability of automatically generated user interfaces.","venue":"CHI","year":2007,"slug":"2007-demonstrating-the-viability-of-automatically-generated-user-interfaces","ext":".md"},"path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","excerpt":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","collection":"publications","content":"<p>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</p>\n","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","excerpt":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","collection":"publications","content":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},{"output":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","previous":{"output":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","previous":{"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","next":{"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","excerpt":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","collection":"publications","content":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","next":{"output":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","previous":{"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","next":{"url":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.html","relative_path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","path":"_publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data.md","id":"/publications/2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","collection":"publications","draft":false,"categories":[],"authors":["Niklas Elmqvist","John T. Stasko","Philippas Tsigas"],"link":null,"tags":["Large-scale Systems","Visual Analytics","Data Visualization","Bars","Protocols","History","Mice","Filtering","Feedback","Multidimensional Systems"],"title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data.","venue":"IEEE VAST","year":2007,"slug":"2007-datameadow-a-visual-canvas-for-analysis-of-largescale-multivariate-data","ext":".md"},"path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","excerpt":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","collection":"publications","content":"<p>Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.</p>\n","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","excerpt":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","collection":"publications","content":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},{"output":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","next":{"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","next":{"output":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","previous":{"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","next":{"url":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.html","relative_path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","path":"_publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life.md","id":"/publications/2007-casual-information-visualization-depictions-of-data-in-everyday-life","collection":"publications","draft":false,"categories":[],"authors":["Zachary Pousman","John T. Stasko","Michael Mateas"],"link":null,"tags":["Focusing","Cognition","Vocabulary","Refining","Data Visualization","Testing","Finance","Information Analysis","Government","Technology Management"],"title":"Casual Information Visualization: Depictions of Data in Everyday Life.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2007,"slug":"2007-casual-information-visualization-depictions-of-data-in-everyday-life","ext":".md"},"path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","excerpt":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","collection":"publications","content":"<p>Peripheral displays provide secondary awareness of news and information to people. When such displays are static, the amount of information that can be presented is limited and the display may become boring or routine over time. Adding animation to peripheral displays can allow them to show more information and can potentially enhance visual interest and appeal, but it may also make the display very distracting. Is it possible to employ animation for visual benefit without increasing distraction? We have created a peripheral display system called BlueGoo that visualizes R.S.S. news feeds as animated photographic collages. We present an empirical study in which participants did not find the system to be distracting, and many found it to be appealing. The study also explored how different display sizes and positions affect information conveyance and distraction. Animations on an angled second monitor appeared to be more distracting than three other configurations.</p>\n","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","excerpt":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","collection":"publications","content":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},{"output":"\n","previous":{"output":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","previous":{"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","collection":"publications","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","next":{"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","excerpt":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","collection":"publications","content":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","next":{"output":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","previous":{"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","next":{"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","excerpt":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","collection":"publications","content":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},{"output":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","previous":{"output":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","previous":{"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","next":{"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","excerpt":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","collection":"publications","content":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","next":{"output":"\n","previous":{"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","next":{"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","excerpt":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","collection":"publications","content":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},{"output":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","next":{"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","next":{"output":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","previous":{"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","next":{"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","excerpt":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","collection":"publications","content":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","excerpt":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","collection":"publications","content":"<p>Systems of connected appliances, such as home theaters and presentation rooms, are becoming commonplace in our homes and workplaces. These systems are often difficult to use, in part because users must determine how to split the tasks they wish to perform into sub-tasks for each appliance and then find the particular functions of each appliance to complete their sub-tasks. This paper describes Huddle, a new system that automatically generates task-based interfaces for a system of multiple appliances based on models of the content flow within the multi-appliance system.</p>\n","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},{"output":"\n","previous":{"output":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","previous":{"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"url":"/publications/2006-instroke-word-completion.html","relative_path":"_publications/2006-instroke-word-completion.md","next":{"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"path":"_publications/2006-instroke-word-completion.md","id":"/publications/2006-instroke-word-completion","excerpt":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","collection":"publications","content":"<p>We present the design and implementation of a word-level stroking system called Fisch, which is intended to improve the speed of character-level unistrokes. Importantly, Fisch does not alter the way in which character-level unistrokes are made, but allows users to gradually ramp up to word-level unistrokes by extending their letters in minimal ways. Fisch relies on in-stroke word completion, a flexible design for fluidly turning unistroke letters into whole words. Fisch can be memorized at the motor level since word completions always appear at the same positions relative to the strokes being made. Our design for Fisch is suitable for use with any unistroke alphabet. We have implemented Fisch for multiple versions of EdgeWrite, and results show that Fisch reduces the number of strokes during entry by 43.9% while increasing the rate of entry. An informal test of “record speed” with the stylus version resulted in 50-60 wpm with no uncorrected errors.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"In-stroke word completion.","venue":"UIST","year":2006,"slug":"2006-instroke-word-completion","ext":".md"},"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","next":{"output":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","previous":{"url":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.html","relative_path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},"url":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.html","relative_path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","next":{"url":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.html","relative_path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","path":"_publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations.md","id":"/publications/2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","John T. Stasko"],"link":null,"tags":["Human-centered Computing","Human Computer Interaction (hci)"],"title":"Animation in a peripheral display: distraction, appeal, and information conveyance in varying display configurations.","venue":"Graphics Interface","year":2007,"slug":"2007-animation-in-a-peripheral-display-distraction-appeal-and-information-conveyance-in-varying-display-configurations","ext":".md"},"path":"_publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry.md","id":"/publications/2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","excerpt":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","collection":"publications","content":"<p>A gestural text entry method for mobile is presented. Unlike most mobile phone text entry methods, which rely on repeatedly pressing buttons, our gestural method uses an isometric joystick and the EdgeWrite alphabet to allow users to write by making letter-like “pressure strokes.” In a 15-session study comparing character-level EdgeWrite to Multitap, subjects’ speeds were statistically indistinguishable, reaching about 10 WPM. In a second 15-session study comparing word-level EdgeWrite to T9, the same subjects were again statistically indistinguishable, reaching about 16 WPM. Uncorrected errors were low, around 1% or less for each method. In addition, subjective results favored EdgeWrite. Overall, results indicate that our isometric joystick-based method is highly competitive with two commercial keypad-based methods, opening the way for keypad-less designs and text entry on tiny devices. Additional results showed that a joystick on the back could be used at about 70% of the speed of the front, and the front joystick could be used eyes-free at about 80% of the speed of normal use.</p>\n","draft":false,"categories":[],"authors":["Jacob O. Wobbrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Touch Screens","Human Computer Interaction (hci)","Interaction Devices"],"title":"An alternative to push, press, and tap-tap-tap: gesturing on an isometric joystick for mobile phone text entry.","venue":"CHI","year":2007,"slug":"2007-an-alternative-to-push-press-and-taptaptap-gesturing-on-an-isometric-joystick-for-mobile-phone-text-entry","ext":".md"},"path":"_publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry.md","id":"/publications/2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Jacob O. Wobbrock","Brad A. Myers","Brandon Rothrock"],"link":null,"tags":[],"title":"Integrating isometric joysticks into mobile phones for text entry.","venue":"CHI Extended Abstracts","year":2006,"slug":"2006-integrating-isometric-joysticks-into-mobile-phones-for-text-entry","ext":".md"},{"output":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","next":{"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","next":{"output":"\n","previous":{"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","next":{"url":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.html","relative_path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","path":"_publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances.md","id":"/publications/2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","collection":"publications","draft":false,"categories":[],"authors":["Jeffrey Nichols","Brandon Rothrock","Duen Horng (Polo) Chau","Brad A. Myers"],"link":null,"tags":["Human-centered Computing","Graphical User Interfaces","Human Computer Interaction (hci)","Interaction Paradigms"],"title":"Huddle: automatically generating interfaces for systems of multiple connected appliances.","venue":"UIST","year":2006,"slug":"2006-huddle-automatically-generating-interfaces-for-systems-of-multiple-connected-appliances","ext":".md"},"path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","excerpt":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","collection":"publications","content":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","next":{"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","collection":"publications","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","next":{"output":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","previous":{"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","collection":"publications","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","next":{"url":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.html","relative_path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","path":"_publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers.md","id":"/publications/2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","collection":"publications","draft":false,"categories":[],"authors":["Duen Horng (Polo) Chau","Shashank Pandit","Christos Faloutsos"],"link":null,"tags":[],"title":"Detecting Fraudulent Personalities in Networks of Online Auctioneers.","venue":"PKDD","year":2006,"slug":"2006-detecting-fraudulent-personalities-in-networks-of-online-auctioneers","ext":".md"},"path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","excerpt":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","collection":"publications","content":"<p>Modern applications such as Microsoft Word have many automatic features and hidden dependencies that are frequently helpful but can be mysterious to both novice and expert users. The ““Crystal”” application framework provides an architecture and interaction techniques that allow programmers to create applications that let the user ask a wide variety of questions about why things did and did not happen, and how to use the related features of the application without using natural language. A user can point to an object or a blank space and get a popup list of questions about it, or the user can ask about recent actions from a temporal list. Parts of a text editor were implemented to show that these techniques are feasible, and a user test suggests that they are helpful and well-liked.</p>\n","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},{"output":"\n","previous":{"output":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","previous":{"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","next":{"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","excerpt":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","collection":"publications","content":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","next":{"output":"\n","previous":{"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","next":{"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},{"output":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","next":{"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","next":{"output":"\n","previous":{"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","next":{"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","collection":"publications","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","excerpt":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","collection":"publications","content":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","next":{"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","collection":"publications","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","next":{"output":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","previous":{"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","collection":"publications","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","next":{"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","excerpt":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","collection":"publications","content":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},{"output":"\n","previous":{"output":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","previous":{"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","collection":"publications","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"url":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","next":{"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"path":"_publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","excerpt":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","collection":"publications","content":"<p>The design and evaluation of most current information visualization systems descend from an emphasis on a user’s ability to “unpack” the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.</p>\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":["Risk Management","Data Visualization","Data Analysis","Humans","Scattering","Decision Making","Displays","Uncertainty","Information Filtering"],"title":"Knowledge Precepts for Design and Evaluation of Information Visualizations.","venue":"IEEE Trans. Vis. Comput. Graph.","year":2005,"slug":"2005-knowledge-precepts-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","next":{"output":"\n","previous":{"url":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.html","relative_path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},"url":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.html","relative_path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","next":{"url":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces.html","relative_path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","path":"_publications/2006-answering-why-and-why-not-questions-in-user-interfaces.md","id":"/publications/2006-answering-why-and-why-not-questions-in-user-interfaces","collection":"publications","draft":false,"categories":[],"authors":["Brad A. Myers","David A. Weitzman","Amy J. Ko","Duen Horng (Polo) Chau"],"link":null,"tags":["Human-centered Computing","Development Frameworks And Environments","Software Development Process Management","Software Notations And Tools","Software And Its Engineering","Human Computer Interaction (hci)","Software Creation And Management","Designing Software","Software Implementation Planning","Software Design Techniques"],"title":"Answering why and why not questions in user interfaces.","venue":"CHI","year":2006,"slug":"2006-answering-why-and-why-not-questions-in-user-interfaces","ext":".md"},"path":"_publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems.md","id":"/publications/2006-a-linguistic-analysis-of-how-people-describe-software-problems","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Amy J. Ko","Brad A. Myers","Duen Horng (Polo) Chau"],"link":null,"tags":[],"title":"A Linguistic Analysis of How People Describe Software Problems.","venue":"VL/HCC","year":2006,"slug":"2006-a-linguistic-analysis-of-how-people-describe-software-problems","ext":".md"},"path":"_publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization.md","id":"/publications/2005-lowlevel-components-of-analytic-activity-in-information-visualization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","James Eagan","John T. Stasko"],"link":null,"tags":[],"title":"Low-Level Components of Analytic Activity in Information Visualization.","venue":"INFOVIS","year":2005,"slug":"2005-lowlevel-components-of-analytic-activity-in-information-visualization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","collection":"publications","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","next":{"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","collection":"publications","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","next":{"output":"\n","previous":{"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","collection":"publications","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","next":{"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","next":{"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","next":{"output":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","previous":{"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","next":{"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","collection":"publications","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","excerpt":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","collection":"publications","content":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},{"output":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","previous":{"output":"\n","previous":{"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","collection":"publications","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","next":{"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","next":{"output":"\n","previous":{"url":"/publications/2004-personalized-peripheral-information-awareness-through-information-art.html","relative_path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},"url":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.html","relative_path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","next":{"url":"/publications/2005-ids-rainstorm-visualizing-ids-alarms.html","relative_path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","path":"_publications/2005-ids-rainstorm-visualizing-ids-alarms.md","id":"/publications/2005-ids-rainstorm-visualizing-ids-alarms","collection":"publications","draft":false,"categories":[],"authors":["Kulsoom Abdullah","Christopher P. Lee","Gregory J. Conti","John A. Copeland","John T. Stasko"],"link":null,"tags":[],"title":"IDS RainStorm: Visualizing IDS Alarms.","venue":"VizSEC","year":2005,"slug":"2005-ids-rainstorm-visualizing-ids-alarms","ext":".md"},"path":"_publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor.md","id":"/publications/2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Ji Soo Yi","Rachel Melton","John T. Stasko","Julie A. Jacko"],"link":null,"tags":[],"title":"Dust  Magnet: multivariate information visualization using a magnet metaphor.","venue":"Inf. Vis.","year":2005,"slug":"2005-dust--magnet-multivariate-information-visualization-using-a-magnet-metaphor","ext":".md"},"path":"_publications/2004-personalized-peripheral-information-awareness-through-information-art.md","id":"/publications/2004-personalized-peripheral-information-awareness-through-information-art","excerpt":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","collection":"publications","content":"<p>This article describes development of the concept of Information Art, a type of ambient or peripheral display involving user-specified electronic paintings in which resident objects change appearance and position to foster awareness of personally relevant information. Our approach differs from others, however, in emphasizing end-user control and flexibility in monitored information and its resultant representation. The article provides an overview of the system’s capabilities and describes an initial pilot study in which displays were given to four people to use for an extended period of time. Reactions were quite favorable and the trial use provided suggestions for system improvements.</p>\n","draft":false,"categories":[],"authors":["John T. Stasko","Todd Miller","Zachary Pousman","Christopher Plaue","Osman Ullah"],"link":null,"tags":["Visual Continuity","News Headline","Design Interview","Background Image","Visual Element"],"title":"Personalized Peripheral Information Awareness Through Information Art.","venue":"UbiComp","year":2004,"slug":"2004-personalized-peripheral-information-awareness-through-information-art","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","collection":"publications","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","next":{"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","next":{"output":"\n","previous":{"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","next":{"url":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.html","relative_path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","path":"_publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays.md","id":"/publications/2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","collection":"publications","draft":false,"categories":[],"authors":["Christopher Plaue","Todd Miller","John T. Stasko"],"link":null,"tags":[],"title":"Is a Picture Worth a Thousand Words? An Evaluation of Information Awareness Displays.","venue":"Graphics Interface","year":2004,"slug":"2004-is-a-picture-worth-a-thousand-words-an-evaluation-of-information-awareness-displays","ext":".md"},"path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","next":{"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","collection":"publications","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","next":{"output":"\n","previous":{"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","collection":"publications","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","next":{"url":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.html","relative_path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","path":"_publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software.md","id":"/publications/2004-gammatella-visualization-of-programexecution-data-for-deployed-software","collection":"publications","draft":false,"categories":[],"authors":["Alessandro Orso","James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Gammatella: Visualization of Program-Execution Data for Deployed Software.","venue":"ICSE","year":2004,"slug":"2004-gammatella-visualization-of-programexecution-data-for-deployed-software","ext":".md"},"path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","next":{"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","collection":"publications","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","next":{"output":"\n","previous":{"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","collection":"publications","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","next":{"url":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.html","relative_path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","path":"_publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations.md","id":"/publications/2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","collection":"publications","draft":false,"categories":[],"authors":["Robert A. Amar","John T. Stasko"],"link":null,"tags":[],"title":"BEST PAPER: A Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations.","venue":"INFOVIS","year":2004,"slug":"2004-best-paper-a-knowledge-taskbased-framework-for-design-and-evaluation-of-information-visualizations","ext":".md"},"path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":{"url":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.html","relative_path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","id":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","collection":"publications","draft":false,"categories":[],"authors":["Dean F. Jerding","John T. Stasko"],"link":null,"tags":[],"title":"The information mural: a technique for displaying and navigating large information spaces.","venue":"INFOVIS","year":1995,"slug":"1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","ext":".md"},"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","next":{"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","next":{"output":"\n","previous":{"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","next":{"url":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.html","relative_path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","path":"_publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps.md","id":"/publications/2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","collection":"publications","draft":false,"categories":[],"authors":["Christoph Csallner","Marcus Handte","Othmar Lehmann","John T. Stasko"],"link":null,"tags":[],"title":"FundExplorer: Supporting the Diversification of Mutual Fund Portfolios Using Context Treemaps.","venue":"INFOVIS","year":2003,"slug":"2003-fundexplorer-supporting-the-diversification-of-mutual-fund-portfolios-using-context-treemaps","ext":".md"},"path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},{"output":"\n","previous":{"output":"\n","previous":null,"url":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.html","relative_path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","next":{"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},"path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","id":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dean F. Jerding","John T. Stasko"],"link":null,"tags":[],"title":"The information mural: a technique for displaying and navigating large information spaces.","venue":"INFOVIS","year":1995,"slug":"1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","ext":".md"},"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","next":{"output":"\n","previous":{"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","next":{"url":"/publications/2002-visualization-of-test-information-to-assist-fault-localization.html","relative_path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","path":"_publications/2002-visualization-of-test-information-to-assist-fault-localization.md","id":"/publications/2002-visualization-of-test-information-to-assist-fault-localization","collection":"publications","draft":false,"categories":[],"authors":["James A. Jones","Mary Jean Harrold","John T. Stasko"],"link":null,"tags":[],"title":"Visualization of test information to assist fault localization.","venue":"ICSE","year":2002,"slug":"2002-visualization-of-test-information-to-assist-fault-localization","ext":".md"},"path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},{"output":"\n","previous":null,"url":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.html","relative_path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","next":{"output":"\n","previous":{"url":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.html","relative_path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","id":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","collection":"publications","draft":false,"categories":[],"authors":["Dean F. Jerding","John T. Stasko"],"link":null,"tags":[],"title":"The information mural: a technique for displaying and navigating large information spaces.","venue":"INFOVIS","year":1995,"slug":"1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","ext":".md"},"url":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.html","relative_path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","next":{"url":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.html","relative_path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","path":"_publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations.md","id":"/publications/2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","collection":"publications","draft":false,"categories":[],"authors":["John T. Stasko","Eugene Zhang"],"link":null,"tags":[],"title":"Focus+Context Display and Navigation Techniques for Enhancing Radial, Space-Filling Hierarchy Visualizations.","venue":"INFOVIS","year":2000,"slug":"2000-focuscontext-display-and-navigation-techniques-for-enhancing-radial-spacefilling-hierarchy-visualizations","ext":".md"},"path":"_publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures.md","id":"/publications/2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["John T. Stasko","Richard Catrambone","Mark Guzdial","Kevin McDonald"],"link":null,"tags":[],"title":"An evaluation of space-filling information visualizations for depicting hierarchical structures.","venue":"Int. J. Hum. Comput. Stud.","year":2000,"slug":"2000-an-evaluation-of-spacefilling-information-visualizations-for-depicting-hierarchical-structures","ext":".md"},"path":"_publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces.md","id":"/publications/1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","excerpt":"\n","collection":"publications","content":"\n","draft":false,"categories":[],"authors":["Dean F. Jerding","John T. Stasko"],"link":null,"tags":[],"title":"The information mural: a technique for displaying and navigating large information spaces.","venue":"INFOVIS","year":1995,"slug":"1995-the-information-mural-a-technique-for-displaying-and-navigating-large-information-spaces","ext":".md"}];
    const pubsEl = document.querySelectorAll('.publication');
    const sAuthorEl = $('#sAuthor');
    const sTitleEl = $('#sTitle');
    const sYearEl = $("#sYear");
    const sTagEl = $('#sTag');
    const sVenueEl = $('#sVenue');

    let minMaxYear = [Infinity, -Infinity];
    let uniqueTags = [];
    let tagOptions = [];
    let uniqueAuthors = [];
    let authorOptions = [];
    let uniqueVenues = [];
    let venueOptions = [];
    let uniqueTitles = [];
    let titleOptions = [];

    function init(){
        pubsEl.forEach(pubEl => {
            const pubIdx = parseInt($(pubEl).attr("data-pub-id"));
            const pubData = pubsData[pubIdx];

            // Unique Tags
            pubData.tags.forEach(function(tag){
                if(uniqueTags.indexOf(tag) === -1){
                    uniqueTags.push(tag);
                    tagOptions.push({id:tag, text:tag});
                }
            });

            // Unique Authors
            pubData.authors.forEach(function(author){
                if(uniqueAuthors.indexOf(author) === -1){
                    uniqueAuthors.push(author);
                    authorOptions.push({id: author, text:author});
                }
            });

            // Unique Venues
            if(uniqueVenues.indexOf(pubData.venue) === -1){
                uniqueVenues.push(pubData.venue);
                venueOptions.push({id: pubData.venue, text: pubData.venue});
            }

            // Unique Titles
            if(uniqueTitles.indexOf(pubData.title) === -1){
                uniqueTitles.push(pubData.title);
                titleOptions.push({id: pubData.title, text: pubData.title});
            }

            // Find the range of years
            const year = pubData.year;
            if(year < minMaxYear[0]){
                minMaxYear[0] = year;
            }
            if(year > minMaxYear[1]){
                minMaxYear[1] = year;
            }
        });

        // Initialize the slider
        var handle1 = $( "#custom-handle1" );
        var handle2 = $( "#custom-handle2" );
        sYearEl.slider({
            range: true,
            min: minMaxYear[0],
            max: minMaxYear[1],
            step: 1,
            values: minMaxYear,
            slide: function(event, ui) {
                handle1.text( ui.values[0] );
                handle2.text( ui.values[1] );
            },
            change: function(event, ui) {
                handle1.text( ui.values[0] );
                handle2.text( ui.values[1] );
                search();
            },
            create: function() {
                handle1.text( $( this ).slider( "values", 0 ) );
                handle2.text( $( this ).slider( "values", 1 ) );
            },
        });

        sAuthorEl.select2({
            tags: true,
            allowClear: true,
            placeholder: '',
            data: authorOptions
        });
        sAuthorEl.on('change', function (e) {
            search();
        });

        sVenueEl.select2({
            tags: true,
            allowClear: true,
            placeholder: '',
            data: venueOptions
        });
        sVenueEl.on('change', function (e) {
            search();
        });

        sTagEl.select2({
            tags: true,
            allowClear: true,
            placeholder: '',
            data: tagOptions
        });
        sTagEl.on('change', function (e) {
            search();
        });

        $(sTitleEl).select2({
            tags: true,
            allowClear: true,
            placeholder: '',
            data: titleOptions
        });
        $(sTitleEl).on('change', function (e) {
            search();
        });

    }

    function clear(){
        sYearEl.slider("values", minMaxYear);
        $(sAuthorEl).val(null).trigger('change');
        $(sTitleEl).val(null).trigger('change');
        $(sVenueEl).val(null).trigger('change');
        $(sTagEl).val(null).trigger('change');
        search();
    }

    function search() {
        const _sAuthor = sAuthorEl.select2('data');
        const _sTitle = sTitleEl.select2('data');
        const _sYear = sYearEl.slider("values");
        const _sTag = sTagEl.select2('data');
        const _sVenue = sVenueEl.select2('data');

        pubsEl.forEach(pubEl => {
            const pubIdx = parseInt($(pubEl).attr("data-pub-id"));
            const pubData = pubsData[pubIdx];
            let showPub = true;
            if(_sAuthor.length == 0 && _sVenue.length == 0 && _sTag.length == 0 && _sTitle.length == 0 && _sYear[0] == minMaxYear[0] && _sYear[1] == minMaxYear[1]){
                showPub = true;
            }else{
                if(_sAuthor.length > 0){
                    if(pubData.authors.length > 0){
                        if(!_sAuthor.some(valObj => pubData.authors.includes(valObj.text))){
                            showPub = false;
                        }
                    }else{
                        showPub = false;
                    }
                }
                if(_sVenue.length > 0){
                    if(!_sVenue.some(valObj => pubData.venue == valObj.text)){
                        showPub = false;
                    }
                }
                if(_sTag.length > 0){
                    if(pubData.tags.length > 0){
                        if(!_sTag.some(valObj => pubData.tags.includes(valObj.text))){
                            showPub = false;
                        }
                    } else{
                        showPub = false;
                    }
                }
                if(_sTitle.length > 0){
                    if(!_sTitle.some(valObj => pubData.title == valObj.text)){
                        showPub = false;
                    }
                }
                if(parseInt(pubData.year) < _sYear[0] || parseInt(pubData.year) > _sYear[1]){
                    showPub = false;
                }
            }
            pubEl.style.display = showPub ? "block" : "none";
        });
    }

    init();

</script>


  </div>
</section>
<div class="vspace-lg"></div></div>
  <div class="vspace-lg"></div>
  <div class="footer">
    <div class="footer-content">
        <a href="https://vis.gatech.edu"><img class="footer-logo" src="/assets/images/logo_gold.png" alt="Logo of Georgia Tech Visualization Lab"/></a>
        <div class="vspace-sm-fixed"></div>
        <i class="fa fa-map-marker"></i>&nbsp;&nbsp;Atlanta, GA 30308, USA.
        <div>Copyright &copy; 2021 | <a href="https://vis.gatech.edu/" target="_blank">Georgia Tech Visualization Lab</a> | <a href="https://www.ic.gatech.edu/" target="_blank">School of Interactive Computing</a> | <a href="https://www.cc.gatech.edu/" target="_blank">College of Computing</a> | <a href="https://www.gatech.edu/" target="_blank">Georgia Tech</a>
        </div>
        <div class="vspace-sm-fixed"></div>
        <div>
            <a href="https://twitter.com/GT_Vis" target="_blank"><i class="fa fa-twitter"></i></a>
            &nbsp;<a href="https://github.com/gtvis" target="_blank"><i class="fa fa-github"></i></a>
        </div>
    </div>
</div>
</body>

</html>